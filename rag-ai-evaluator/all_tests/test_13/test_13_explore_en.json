{
  "test_id": "test_013_explore_en",
  "query": "Can you link me to a Coursera course on Kafka?",
  "endpoint": "reader_explore_text",
  "user": {
    "tier": "elevate",
    "language": "en"
  },
  "prompt": "main",
  "actual_response": "arrow_forward_iosDeine Suchergebnisse:  Tools & Processes for MLOps Hauke BrammerAI Systems Done Right: MLOps and Security Strategies A Beginner-Friendly Workshop with Python an... Alexey GrigorevRAG verstehen: Retrieval-Augmented Generation in der Praxis Workshop: Build a Chatbot with LLM Alexey GrigorevGet Ready, Get Set, AI + Chatbots In the shadows of GenAI: Key Machine Learni... Martin Stypinski, Leeanna Seidler und weitereAI Systems Done Right: MLOps and Security Strategies Intelligent Forms: Spice up your Angular Fo... Christian LiebelAdvanced Angular: Signals, NgRx & AI Forms Lessons in delivery through hyper growth at... Andrew FongMastering CI/CD Best Practices Workshop: AI-Powered Node.js Development wi... Nir KaufmanUnleash the Power of AI in Web Development Expertenwissen trifft AI Dr. Pieter ButeneersRAG verstehen: Retrieval-Augmented Generation in der Praxis Does size matter? The Joys and Pains of K8S... Anton WeissSet Sail and Build Fast With Kubernetes Secure by Design: Patterns and Principles f... Jose Haro PeraltaOvercoming API Security Hurdles Let's Build a GPT Model Together: Demystify... Jochen EmigGet Ready, Get Set, AI + Chatbots The Team Is The Real Product Jason GormanStand der Softwareentwicklung: 12 wegweisende Keynotes Managing Machine Learning Lifecycle with ML... René Brunner, Saumya Goyal und weitereChatGPT, LLMs & AI: Blick hinter die Kulissen Ausbruch aus dem KI-Gefangenendilemma! Eric Dauenhauer, Patrick Glauner und weitereChatGPT, LLMs & AI: Blick hinter die Kulissen Elastic Kubernetes Roland Huß, Zbynek Roubalik und weitereKubernetes Foundations Navigating Sociotechnical Complexity with D... Xin YaoSoziotechnische Modelle und Softwarearchitektur Strategisches API-Design Lars RöwekampEffizientes API-Design API Design with Patterns: Endpoint Roles, M... Daniel LübkeEffizientes API-DesignCan you link me to a Coursera course on Kafka?gpt-4.1-mini keyword: {\n  'phrase_out': 'Coursera course Kafka',\n  'primary_version_array': [],\n  'secondary_version_array': [],\n  'year_array': [],\n  'issue_array': []\n} \nI can help with software development topics on entwickler.de. However, I cannot provide links to external platforms like Coursera. If you are interested in Kafka-related content within entwickler.de, there are relevant courses and sessions available that cover Kafka usage, autoscaling with Kubernetes, and event-driven architectures. For example, there is a detailed course on Kubernetes Foundations that includes Kafka consumer autoscaling with KEDA, demonstrating practical deployment and scaling techniques [1.] . If you want, I can guide you to such internal resources or workshops on Kafka and related serverless or cloud-native topics.\n\nSources\n\nElastic Kubernetes This tutorial lesson demonstrates how to achieve elastic scaling in Kubernetes using built-in autoscalers (HPA, VPA), serverless add-ons (Knative), and KEDA, plus cluster autoscaling. ✅ This content is included with your elevate membership.\n\nReference:\n{\n  'translated_headers': {\n    'sources': 'Sources',\n    'more_on_this_topic': 'More on this Topic'\n  },\n  'sources': [],\n  'more_on_this_topic': []\n}\n\nChunks:\n'documentId': 'b5b4b830ce2ccbe92815c1e8'\n'chunk_id': '687a48656b250430a4783aee'\n'access': 'granted'\n'part_number': 13\n'total_parts': 13\n'contentType': 'COURSE'\n'title': API Design with Patterns: Endpoint Roles, Message Structures, Evolution Strategies\n'parentName': Effizientes API-Design\n'language': 'DE'\n'date': '2024-03-06T12:44:31.000Z'\n'abstract': Qualities such as a good client developer experience, adequate response times and flexible extensibility do not just happen, but stem from sound API design. API design involves many design decisions; while some of these decisions are specific to technologies such as RESTful HTTP, gRPC or GraphQL, many of them arise irrespective of the chosen protocols. Successful API designs evolve and are refactored continuously. Design patterns are a well-established means of collecting design decisions, options and criteria. They share proven community knowledge in a timeless and platform-independent way. A team of five volunteers collected 44 such Patterns for API Design, showing how to scope and evolve APIs, how to assign roles and responsibilities to API endpoints and their operations and how to right-size request and response messages – driven by API usage profiles and desired qualities. This talk presents a selection of these patterns, including Pagination, Wish List, Two in Production, Context Representation and Data Transfer Resource. We start with pattern usage context, identify design problem solved and API qualities addressed. Next, a solution is sketched with pros and cons discussed. HTTP/JSON snippets and short demos exemplify pattern realization and implementation. The talk concludes with an outlook on API refactoring.\n'parentId': 'NujHArQmTzCtoo5uP'\n'parentDescription': In unserem API-Design-Kurs erfährst du alles, was du wissen musst, um erfolgreiche und langlebige APIs zu entwickeln. Du lernst, warum es so wichtig ist, von Anfang an klare Geschäftsziele zu haben und APIs als Produkte zu behandeln. Wir zeigen, wie du deine APIs strategisch planst, entwirfst und kontinuierlich verbesserst. API Management, API Security und API Observability werden genauso thematisiert wie Beispiele der Arbeit mit konkreten APIs von GraphQL, OpenAI und Axum.\n'indexBrandName': 'APICon'\n'indexSeriesName': ''\n'author': 'Daniel Lübke'\n'chunkSource': 'text'\n'text': you could solve this again. You need to have some kind of evolution strategy in place if you have released one version and production already. So that was the talk. And I told you where we are going what we are going to do and last two slides I think as what I can offer where you can go to. So one thing is we have this domain API patterns Arc. Thanks for free can browse it. You will find all patterns all descriptions. There's also this lovely book. Which contains all the pageants and much much more information? and therefore some also some some podcasts and some interviews once with Eric using the second row with one burn and also take lead Journal can get a 30% voucher for the book the first version of the slide had all the links in there looked ugly. So I did a incompatible API change I gave you redirect. So I printed Twitter post and the first post on LinkedIn gives you the link to all of these things. Which you can obviously use for free in your spare time, but you can't read them with your old tablet while you know, and the restroom. Yeah. And that's it. So what we want to achieve we want to establish pattern language. So have good names for Concepts which we use all the time and with me, I mean as an API designer Community want to capture this experience and it's a great checklist now, so I mean say I'm very experienced. Going through this catalog while you're working. Helps you remember the important things. There's some ongoing work for building better tools. And again, if you want to have the links to the podcast and things like that, you'll find find it on Twitter. And on LinkedIn and with that I'm open to questions. One from a pattern perspective. It's the same because in the link you need to decide whether you access a page. So like a query parameter page 10 page 20, or do you have a query parameter like Last ID was thousand so that decides on which variant you on. So are you on a page-based variant on a cursorbase variant? and then is the question of your technology and if you're using HTTP can use a query parameter you can have some urls. In the end. I'm quite emotionalist on that and I know other people aren't for me the decision which variant you make and that you decide it or you let your identify that you need pagination is from my experience of far more important decision. other questions So they're not then I wish you a happy conference. and yeah, see\n'slidetext': ''\n'from': 'vector'\n'score': 0.8836005926132202\n'normalizedScore': 0.849951964063626\n'beforeDatePenaltyScore': 0.8836005926132202\n\n'documentId': 'T4tdNfcG52g3CDa2q'\n'chunk_id': '687a2de89bbcb45d2a11f0f3'\n'access': 'granted'\n'part_number': 13\n'total_parts': 13\n'contentType': 'COURSE'\n'title': Strategisches API-Design\n'parentName': Effizientes API-Design\n'language': 'DE'\n'date': '2024-03-06T12:44:31.000Z'\n'abstract': In diesem Artikel gehen wir auf Spurensuche: Worin liegt der Unterschied zwischen einem guten von einem schlechten API? Hier erfahrt Ihr alles dazu.\n'parentId': 'NujHArQmTzCtoo5uP'\n'parentDescription': In unserem API-Design-Kurs erfährst du alles, was du wissen musst, um erfolgreiche und langlebige APIs zu entwickeln. Du lernst, warum es so wichtig ist, von Anfang an klare Geschäftsziele zu haben und APIs als Produkte zu behandeln. Wir zeigen, wie du deine APIs strategisch planst, entwirfst und kontinuierlich verbesserst. API Management, API Security und API Observability werden genauso thematisiert wie Beispiele der Arbeit mit konkreten APIs von GraphQL, OpenAI und Axum.\n'indexBrandName': 'APICon'\n'indexSeriesName': ''\n'author': 'Lars Röwekamp'\n'chunkSource': 'text'\n'text': Schwerpunkt seiner Arbeit liegt derzeit auf den Bereichen Enterprise und Cloud Computing sowie auf AI/ML. Links & Literatur [1] ProgrammableWeb Directorty: https://www.programmableweb.com [2] Google Map API: https://mapsplatform.google.com [3] Amazon Affiliate Program: https://partnernet.amazon.de [4] eBay APIs: https://developer.ebay.com [5] Twitter APIs: https://developer.twitter.com [6] Facebook APIs: https://developers.facebook.com [7] Swagger: https://swagger.io [8] Expand & Contract Pattern: https://martinfowler.com/bliki/ParallelChange.html [9] Consumer-driven Contract Pattern: https://martinfowler.com/articles/consumerDrivenContracts.html\n'slidetext': ''\n'from': 'vector'\n'score': 0.8845047354698181\n'normalizedScore': 0.850856106920224\n'beforeDatePenaltyScore': 0.8845047354698181\n\n'documentId': '4iNg3SLaE3sj8Tgo3'\n'chunk_id': '687a2c19495b49563aa87c57'\n'access': 'granted'\n'part_number': 4\n'total_parts': 16\n'contentType': 'COURSE'\n'title': Navigating Sociotechnical Complexity with DDD and Friends\n'parentName': Soziotechnische Modelle und Softwarearchitektur\n'language': 'DE'\n'date': '2024-05-07T13:04:27.000Z'\n'abstract': What is the first thing that comes to mind when you hear the word DDD – Domain-Driven Design? Geeky-sounding patterns like Aggregate and Ubiquitous Language? Walls decorated with colorful event storming stickies? A miracle cure to rescue change initiatives in large companies? Or are you thinking of a software development method born in the pre-cloud and pre-microservice era, which after 20 years still seems to be struggling to gain traction? Xin has lived and breathed DDD and domain-driven architecture for more than a decade. Drawing on her own developmental journey, Xin makes a case for the rising relevance of DDD and adjacent practices in a post-modern world, where aging companies struggle with aging software, while adding new software and complexity to their IT portfolio. With good attractor effect, DDD is evolving from a software-centric design discipline to a multi-dimensional, sociotechnical design toolbox. Join Xin to reflect together on, how DDD and friends can be leveraged as powerful enabling constraints to help us see the system, share the system, and build the system in a reality of vast sociotechnical complexity and constant change.\n'parentId': 'BAgQBSP7f7baPi6Ak'\n'parentDescription': Dieser Kurs vermittelt die Grundlagen, um Softwarearchitektur auf die menschlichen Bedürfnisse auszurichten. Von der Identifizierung von Grenzen für autonome, crossfunktionale Teams bis hin zur Navigation durch soziotechnische Komplexität mithilfe von DDD beleuchten wir verschiedene Aspekte, um Softwarearchitekturen zu verbessern und dabei stets den menschlichen Faktor in den Mittelpunkt zu stellen. Tauche ein in die Welt der Softwarearchitektur mit einem besonderen Augenmerk auf die menschlichen Dynamiken hinter der Technologie!\n'indexBrandName': 'JAX'\n'indexSeriesName': ''\n'author': 'Xin Yao'\n'chunkSource': 'text'\n'text': APIs, foreign AI services. You, um, kind machine learning, AI deep learning. LLM, uh, uh, expert or expert in or the inside design. Yeah. K, let the magic run. Uh, this team. Yeah, this Alison ML engineering doesn't Did algorithm. Yeah. Doesn't, um, one last but not least, whatever. Yeah, that is kind of machine learning, conference, software, engineering, engineering, these annoying tools on APIs, significant, wider and workshops, Y sessions on keynotes, speaker and fantastic Indian sessions. Uh, Indian workshops. Um, therefore I'm switching into English language. The title of the keynote is Navigating Social Technical Complexity with Domain Driven Design. And France, our keynote speaker. She is a DDD enthusiast. Uh, she's an architecture expert, uh, especially for social technical aspects. And last but not least, of course, sees a change agent. So she helps lots of organizations to change towards those, let's say newer things. Having said that, please give a warm hand. Tohin Yao, please. Thank you. Thank You. That's right. Good morning everyone. Can you hear me? All right? Excellent. So it's my first time at Jacks, so I've been really looking forward to it. I've heard so many good things about this conference, and I think it, this conference has the name of it, has a really nice tune to it. So if people ask me, where have you been speaking, Jax? So that's really nice. So today I have really a lot to share with you. So just let's get rolling, shall we? Um, thanks Sebastian for the kind introduction. And here's a little bit more about me. I am an independent consultant with mostly domain-driven design and architecture as the main thing in my toolbox, the most visible part of my work life, or you can say what most of my clients hire me to do, can be captured by this feedback loop on the left hand side, right? So this one, so that one basically says, the more work I do, the more output I'm gonna produce, for instance, DDD workshops, event storming, domain models, architecture, documentation. And that in turn would bring me more business. So that's a pretty neat feedback loop. But truly what I personally tap most of my creative energy from is actually the loop on the right hand side. And that one basically says, the more work I do, the more I will be able to contribute to socio-technical design and systems leadership. Although with some kind of a delay. So you can say in many ways what most people see is my public interface,\n'slidetext': ''\n'from': 'vector'\n'score': 0.8819969892501831\n'normalizedScore': 0.8517463794727859\n'beforeDatePenaltyScore': 0.8819969892501831\n\n'documentId': 's8SfLEXT2rkLAFpEW'\n'chunk_id': '687b42312b0a1813a73e3cf8'\n'access': 'granted'\n'part_number': 16\n'total_parts': 21\n'contentType': 'COURSE'\n'title': Elastic Kubernetes\n'parentName': Kubernetes Foundations\n'language': 'EN'\n'date': '2023-09-29T14:43:13.000Z'\n'abstract': One key characteristic of serverless platforms is their ability to dynamically scale application replicas, including a scale down to zero when the application is not used so that they don't cost anything. This session gives an overview of how Kubernetes, the leading container orchestration platform, allows elastic scaling of your application. You will learn about Kubernetes intrinsic auto-scaling capabilities like the Horizontal Pod Autoscaler (HPA) or the Vertical Pod Autoscaler (VPA) and how to use it with your application. Kubernetes does not support autoscale-to-zero out of the box. But luckily, there are add-ons like Knative or KEDA that bring those missing capabilities and more to your cluster. We will learn about their approaches to autoscale and see them live in demos. This session is for you if you want to learn how to get the most out of these techniques!\n'parentId': 'MeuWSXeoTLwWqyx9j'\n'parentDescription': Unser Kubernetes-Grundkurs ist der perfekte Startpunkt für deine Reise in die Container-Orchestrierung. Hier lernst du nicht nur die Basics von Kubernetes kennen, sondern auch, wie du Container effizient orchestrierst. Wir beginnen mit den Grundlagen, damit du ein solides Verständnis für die Prinzipien der Container-Orchestrierung entwickelst. Aber das ist erst der Anfang! Dieser Kurs wird dich auf die nächste Stufe bringen, indem er dir zeigt, wie du Helm und Helmfile einsetzen kannst, um deine Deployments zu vereinfachen. Wir gehen noch einen Schritt weiter und decken ab, was du beachten musst, wenn du Kubernetes in Verbindung mit Cloud-basierten Managed Services wie AWS, Azure oder der Google Cloud Platform einsetzt. Damit bist du bestens gerüstet, um Kubernetes in realen Produktionsumgebungen einzusetzen. Nach Abschluss dieses Kurses wirst du nicht nur die Grundlagen beherrschen, sondern auch in der Lage sein, souverän und selbstbewusst deine ersten Schritte mit Kubernetes in produktiven Umgebungen zu unternehmen. Mach dich bereit, die Welt von Kubernetes zu erobern und starte noch heute!\n'indexBrandName': 'DevOpsCon'\n'indexSeriesName': ''\n'author': 'Roland Huß,Zbynek Roubalik'\n'chunkSource': 'text'\n'text': we go. Okay. Okay. So this was them about the caynative and now let's take a look at Cada. So on this diagram what we can see is the is the setup the devil I will deploy so we will have we have a Kafka Kafka service running already and we will deploy application that consume messages from from the Kafka topic and we would Auto scale this is application with Canada. So first I will deploy the Kafka consumer. It is very simple deployment. It is just consuming messages from family broker. So let me create. a deployment I'm sorry. So our application should be should be created and as you can see on the left hand side, you can see that the application is is ready and I will just check check. Logs, I'm sorry. So we can see that the application is up and running. Okay. So two double check that application is actually able to get some messages. We all send some some data to the to the Kafka topic to do that. We will. Now we will use. We will use this this kubernetes job and it is job. There is very simple Kafka producer that produces messages in this case. Let's say we will send 15 15 messages to our application. Okay, so let me let me create the application so as you can see on the on the right hand side. Oh, I'm sorry it already created here. There is no level. Yeah, it created a 300 messages not 15 because I haven't saved this file. Sorry, so as you can see, it is processing it is taking some time. So if you have one replica, it takes some time to process 300 messages at the moment. We are at the message one 152 anyway, so if we want to start Auto scaling on this, so what we can do we will just need to deploy scaled object. As I mentioned a scale object just reference the target deployment. So in this case is the Kafka consumer deployment in the trigger section we say okay, let's use this Kafka broker. With with the default setting and the important stuff is the like threshold. So in this case, it's very low. So it means that if there is one message, let's scale up. So it will it will really really a scale scale fast if there are if there is one processed message in the in the my topic over here. Because security is important and we don't we need to connect to some external service. We don't want to store the credentials here in the in the plane ammo so we would like to something else. So for that we have another concept which is called trigger authentication where you can reference your credentials. So in\n'slidetext': ''\n'from': 'vector'\n'score': 0.8945883512496948\n'normalizedScore': 0.8522319106763818\n'beforeDatePenaltyScore': 0.8945883512496948\n\n'documentId': 's8SfLEXT2rkLAFpEW'\n'chunk_id': '687b42312b0a1813a73e3cf4'\n'access': 'granted'\n'part_number': 14\n'total_parts': 21\n'contentType': 'COURSE'\n'title': Elastic Kubernetes\n'parentName': Kubernetes Foundations\n'language': 'EN'\n'date': '2023-09-29T14:43:13.000Z'\n'abstract': One key characteristic of serverless platforms is their ability to dynamically scale application replicas, including a scale down to zero when the application is not used so that they don't cost anything. This session gives an overview of how Kubernetes, the leading container orchestration platform, allows elastic scaling of your application. You will learn about Kubernetes intrinsic auto-scaling capabilities like the Horizontal Pod Autoscaler (HPA) or the Vertical Pod Autoscaler (VPA) and how to use it with your application. Kubernetes does not support autoscale-to-zero out of the box. But luckily, there are add-ons like Knative or KEDA that bring those missing capabilities and more to your cluster. We will learn about their approaches to autoscale and see them live in demos. This session is for you if you want to learn how to get the most out of these techniques!\n'parentId': 'MeuWSXeoTLwWqyx9j'\n'parentDescription': Unser Kubernetes-Grundkurs ist der perfekte Startpunkt für deine Reise in die Container-Orchestrierung. Hier lernst du nicht nur die Basics von Kubernetes kennen, sondern auch, wie du Container effizient orchestrierst. Wir beginnen mit den Grundlagen, damit du ein solides Verständnis für die Prinzipien der Container-Orchestrierung entwickelst. Aber das ist erst der Anfang! Dieser Kurs wird dich auf die nächste Stufe bringen, indem er dir zeigt, wie du Helm und Helmfile einsetzen kannst, um deine Deployments zu vereinfachen. Wir gehen noch einen Schritt weiter und decken ab, was du beachten musst, wenn du Kubernetes in Verbindung mit Cloud-basierten Managed Services wie AWS, Azure oder der Google Cloud Platform einsetzt. Damit bist du bestens gerüstet, um Kubernetes in realen Produktionsumgebungen einzusetzen. Nach Abschluss dieses Kurses wirst du nicht nur die Grundlagen beherrschen, sondern auch in der Lage sein, souverän und selbstbewusst deine ersten Schritte mit Kubernetes in produktiven Umgebungen zu unternehmen. Mach dich bereit, die Welt von Kubernetes zu erobern und starte noch heute!\n'indexBrandName': 'DevOpsCon'\n'indexSeriesName': ''\n'author': 'Roland Huß,Zbynek Roubalik'\n'chunkSource': 'text'\n'text': a push because the requests are pushed to the application. But if it is some even think Kafka consumer a bmq consumer the application itself needs to establish the connection to the stuff. So it is pulling the data. right candidate with is the solution for the push access exactly exactly. Thank you. okay, so let's let's continue with the so maybe can make the question after what is okay, or That if we okay thanks a lot because maybe otherwise we might run out of time. But actually you're the first one the first question that didn't. Okay. Thanks a lot of question in the chat. Yeah, maybe we can also do this at the end if it's okay. Okay. Let's have a quick look here. So what I have here for kubernetes class on the background in this case, it's an oppressive cluster, but don't be confused. So actually this is everything what I'm showing here is working for Community as well. So what I can use I'm using here KN which is the KN which is the Cana client tool and I create a service which is called named random. And the only thing that I mentioned what you need is a reference to container image and I'm just doing that and what kind of does now it pulls the image if it's not there starts it up. You see it in the top you see your watch on the pots that are running in that namespace and it's up and running. You see all that. There are two parts in there two containers. So this is the one is so called a cube proxy which is responsible to intercept the traffic to record the traffic and send it over to our autoscale component. So this is kind of a sidecar which is running. And what I can do now actually jump to the top adjust of course can crowd that so with what you can get as well. So actually I have a so-called URL. So a route and Ingress is already created for you. There's different from a regular deployment another call that I get back the answer of the service which in this case is just returning some random numbers. It's just a simple rest service for demonstration purposes. You see that my my connection is a little bit shaky. Okay, I kind of cost to something like like updates. With that one and of course everything what I'm showing you can you can do this in the Army as well, but I'm setting here is the concurrency limit just for the demo sec to say OK, my port should scale up already if there are 10 concurrent requests or a little bit less because of this authorization and I want to decrease the\n'slidetext': ''\n'from': 'vector'\n'score': 0.8947346210479736\n'normalizedScore': 0.8523781804746606\n'beforeDatePenaltyScore': 0.8947346210479736\n\n'documentId': 'hviTENDk754oqWbwy'\n'chunk_id': '687ad43eeb1b160a4cabc8b2'\n'access': 'granted'\n'part_number': 10\n'total_parts': 10\n'contentType': 'COURSE'\n'title': Ausbruch aus dem KI-Gefangenendilemma!\n'parentName': ChatGPT, LLMs & AI: Blick hinter die Kulissen\n'language': 'DE'\n'date': '2023-12-13T16:00:38.000Z'\n'abstract': Willkommen zur Artikelserie „KI in der Praxis“, in der wir die grundlegenden und fortgeschrittenen Aspekte dieser innovativen Technologien beleuchten. Diese Serie wird Ihnen einen fundierten Überblick über die einzelnen Facetten liefern und dazu beitragen, Ihre Kompetenz in diesem dynamischen Feld zu erweitern.\n'parentId': 'iZe5pRtaQkHS95fSK'\n'parentDescription': Tauche ein in die faszinierende Welt der künstlichen Intelligenz mit unserer Sammlung von inspirierenden Artikeln, lehrreichen Keynotes und spannenden Talks von führenden Experten. Wir bieten einen ganzheitlichen Einblick in verschiedene Aspekte der KI, von der Personalisierung von GPT-Modellen bis hin zur praktischen Anwendung von Machine Learning in der Softwareentwicklung. Erfahre, wie du GPT-Modelle mit deinen eigenen Daten personalisieren kannst, nutze Python effektiv für Machine Learning, verwalte den gesamten Lebenszyklus von Machine Learning-Projekten mit MLflow und erkunde die Welt der Open-Source-LLMs. Entdecke die Geheimnisse erfolgreicher ML-Projekte, erforsche die ethischen Aspekte von Sprachmodellen und erhalte einen Einblick in die Arbeit einer Machine Learning Consultancy.\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Eric Dauenhauer,Patrick Glauner'\n'chunkSource': 'text'\n'text': [5] https://link.springer.com/chapter/10.1007/978-3-030-99838-7_7 [6] https://docs.aleph-alpha.com/docs/explainability/attention-manipulation/ [7] https://arxiv.org/pdf/2301.08110.pdf [8] http://change.org/ki [9] https://alaiki.de [10] https://www.youtube.com/watch?v=KbNSH2y9nv4 [11]https://emccdrive.emccglobal.org/api/file/download/u7xOUK5c7JhpjZinckAUW6OoX4kG1Hcq8FSwVnR5\n'slidetext': ''\n'from': 'vector'\n'score': 0.8911890983581543\n'normalizedScore': 0.8529451926656124\n'beforeDatePenaltyScore': 0.8911890983581543\n\n'documentId': 'xYCzZF3b8j6hpwQax'\n'chunk_id': '687b44842b0a1813a73e5503'\n'access': 'granted'\n'part_number': 2\n'total_parts': 66\n'contentType': 'COURSE'\n'title': Managing Machine Learning Lifecycle with MLflow and Apache Airflow\n'parentName': ChatGPT, LLMs & AI: Blick hinter die Kulissen\n'language': 'EN'\n'date': '2023-12-13T16:00:38.000Z'\n'abstract': In this workshop, we will cover the transition from DevOps to MLOps. We’ll start by introducing MLflow and guide you through its installation and training your own models. We’ll explore hyper-parameter tuning, model selection, MLFlow Webhooks and the MLflow Model Registry. In the second half, we’ll focus on Apache Airflow, an open-source platform for workflow orchestration and scheduling. We’ll begin with an overview of Airflow, its concepts, and background. Then, we’ll delve into using Directed Acyclic Graphs (DAGs), Airflow operators. We end our workshop by creating a complete end-to-end ML training pipeline by coupling MLFlow and Airflow. By the end of the workshop, you’ll have a solid understanding of both these tools and knowledge on how to integrate them into your existing Machine Learning training pipelines. We will also provide you with a cloud architecture that combines both tools, enabling you to effectively orchestrate and manage your machine learning workflows. Spoiler Alert: It would be a code along hands-on workshop!\n'parentId': 'iZe5pRtaQkHS95fSK'\n'parentDescription': Tauche ein in die faszinierende Welt der künstlichen Intelligenz mit unserer Sammlung von inspirierenden Artikeln, lehrreichen Keynotes und spannenden Talks von führenden Experten. Wir bieten einen ganzheitlichen Einblick in verschiedene Aspekte der KI, von der Personalisierung von GPT-Modellen bis hin zur praktischen Anwendung von Machine Learning in der Softwareentwicklung. Erfahre, wie du GPT-Modelle mit deinen eigenen Daten personalisieren kannst, nutze Python effektiv für Machine Learning, verwalte den gesamten Lebenszyklus von Machine Learning-Projekten mit MLflow und erkunde die Welt der Open-Source-LLMs. Entdecke die Geheimnisse erfolgreicher ML-Projekte, erforsche die ethischen Aspekte von Sprachmodellen und erhalte einen Einblick in die Arbeit einer Machine Learning Consultancy.\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'René Brunner,Saumya Goyal'\n'chunkSource': 'text'\n'text': will tell you later. But, uh, we highly recommend to have, uh, um, MacBook or Linux. If not, if you have Windows, then to install V-S-W-L-V-W-S-L too and should they're open to machine. If you have other questions, then please reach out. Probably you'll need some admin rights. Uh, we put this already in preparation. Um, also during the break or if you want to use them some time, you can check this up. Uh, Okay. And another information here. You can already download the data from this link, which is posted below. I think it's a QR code. And, uh, we will use this then throughout the workshop. So then we can go through faster for everybody. The wifi Connect to, is it maritime wifi? Yes. Yes. The open the public wifi from Marriott. It's where we connected. Yeah. Yes. Martin wifi. Any other questions in the meantime? Online participants also looks fine. Yes. Alright. Two. Let's wait, Can you, uh, go back to the lead? Yeah, sure. Yep. The link is pretty straightforward. It's tiny url.com. Mlco berlin. So you can also type it out. Yes, on Top. Um, this in red and it'll take you here. So This one. All right. Okay. Then let's start. It's also time. So, or is everybody ready? Has the link, then we start and you can still download it later or during the first presentation. I think everybody arrived already. Then. Today we talk about, as you know, already about machine learning lifecycle with Mflow and Apache Airflow. It's two parts out of the complete ML lops pipeline. Tomorrow we also have a presentation about, uh, the end-to-end pipeline, which we set up at, uh, BSH household, uh, company. It's a talk tomorrow. So we have the big pipeline today. We go really in the hands-on workshop to do MF flow and Apache Air flow in combination about, uh, us. I am, uh, Rene Bruna. I'm professor at the Macromedia University and founder and CEO of, uh, data mix, uh, GMBH. I'm in the machine learning environment nearly 20 years. Did my PhD with machine learnings. It's, uh, more than it's 15 years ago. And, um, yeah, have, uh, also some, uh, a book written in Python Learning, doing Udemy online, uh, classes with 50,000 participants in the chairman region. We have a podcast we'll show, um, later, and then also consulting projects in, uh, companies. Yep. Um, hello everyone. I'm Somia and uh, I also work\n'slidetext': ''\n'from': 'vector'\n'score': 0.8917698264122009\n'normalizedScore': 0.853525920719659\n'beforeDatePenaltyScore': 0.8917698264122009\n\n'documentId': 'M8Xgm6aQkhq4R8ETG'\n'chunk_id': '687ac1f3c902463ce5f2a716'\n'access': 'granted'\n'part_number': 3\n'total_parts': 15\n'contentType': 'COURSE'\n'title': The Team Is The Real Product\n'parentName': Stand der Softwareentwicklung: 12 wegweisende Keynotes\n'language': 'EN'\n'date': '2023-12-20T09:03:46.000Z'\n'abstract': According to the Agile Manifesto, 'Working software is the primary measure of progress'. But what about the team who created that software? If software development's a learning process, does it make more sense to view the real value created by teams in terms not of what they delivered, but what they learned - about the business, the end users, the technology, the process, their code base, and how to work with each other? Team who've climbed the learning curve are much more capable of adding value than a brand new team who have to climb it all over again when the software inevitably has to change. Does it make sense to build a whole new car for every journey? And yet, all too often, businesses don't hesitate to disband a team as soon as a solution's to today's problem has been delivered.  Is what's needed a shift in focus in the way we see software teams - not as costs to be minimised in the short-to-medium term, but as assets to be nurtured for the long term? Should businesses view teams as the real end product?\n'parentId': '3J7gtvYTpYWTAATjJ'\n'parentDescription': Diese Liste präsentiert keine bloße Ansammlung von “besten” Vorträgen der entwickler.de-Konferenzen, sondern vielmehr eine Auswahl von Statements, die das Schlaglicht auf den aktuellen Stand des Nachdenkens über Software Engineering richten. Von visionären Konzepten bis hin zu praktischen Erkenntnissen spiegelt diese Auswahl das breite Spektrum der Herausforderungen und Innovationen wider, denen sich die Softwareentwicklungsgemeinschaft heute gegenübersieht. Begleite uns auf dieser Reise durch die Gedankenwelt führender Köpfe der Branche, die durch ihre Keynotes nicht nur den aktuellen Stand reflektieren, sondern auch Impulse für die kommenden Entwicklungen geben.\n'indexBrandName': 'SoftwareArchitectureSummit'\n'indexSeriesName': ''\n'author': 'Jason Gorman'\n'chunkSource': 'text'\n'text': here. Shout mank. Online events, online life events. Tutorials, the subject marked favorites conference slides and certificate download, online workshop. Act on last but not least, video tutorials. Team of experience. Not Android on um, Iosk feedback. Feedback of the sessions, keynotes   Here is a community here for Autumn Conference owns remotely. Yeah. . chairman and   desktop app do dreams. Okay. workshops   parallel, good cetera, et cetera. Never Oh wow. services   of services. Yeah. On   pro service   and context on . You built it, you ran it. Platform engineering focus, self-service platform to maintain the ink on ink. The deploying on, on, um, yeah. Mayor, um, . Microservices. Yeah, microservices. Martin. Okay. is Netflix. Spotify and Prime. Eric Evans domain design here of   design help . Okay. Um, November   names   Twitter timeline. Yeah. Statements. Sorry. Yeah. Um, the conference   on . Um, well, and I'm switching to English now. Uh, the keynote is dealing about experience, about experienced people and experienced teams and the value which is lying in that. And, uh, let me add one thing for especially our remote friends, it would be great if you switch now to the next session. Yeah. Um, because the current session is continuing on the one hand, but, uh, it might be stopped after a while, so please use your player. Go to the session from Jason Gorman. Jason Gorman, I just mentioned his name. Um, he's a well known speaker, thinker, expert. Uh, and he just told me, uh, that his last keynote he did was right here at Jax. And that is really an honor for us. So please welcome Jason Gorman, You lucky people first in-person conference in a long, long time. And you are now gonna have to suffer me for the next 30 minutes. So well done. You made a good choice. Okay. Um, so Sebastian, um, approached me earlier this year about coming and doing a talk at Jack's. Um, and he was inspired by something I'd posted on social media about, I was boasting about how quickly I can learn programming languages. Um, it takes me just a few days really to learn the 20% of a new programming language that you would probably use 80% of the time, which is very, very different to how it used to be when I started programming. Um, and I thought a little about, about this, and I thought that's probably a very narrow topic for, for a keynote. So I wanted to expand really\n'slidetext': ''\n'from': 'vector'\n'score': 0.8923036456108093\n'normalizedScore': 0.8544274390557611\n'beforeDatePenaltyScore': 0.8923036456108093\n\n'documentId': '37612953b51a3467652b95f7'\n'chunk_id': '687a2c88495b49563aa88015'\n'access': 'granted'\n'part_number': 13\n'total_parts': 15\n'contentType': 'COURSE'\n'title': Let's Build a GPT Model Together: Demystifying AI Through Practical Application\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': In this engaging and interactive tech talk, we embark on a hands-on journey to unravel the mysteries of AI by building our own mini Generatively Pre-trained Transformer (GPT) model. Inspired by the groundbreaking paper Attention is All You Need, this session is designed for enthusiasts and professionals alike, eager to delve into the workings of one of the most influential technologies in modern AI. Our aim is to demystify the complexities of GPT models through a practical, step-by-step approach.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Jochen Emig'\n'chunkSource': 'text'\n'text': guys will all have it. I will make sure that I'll put a link on the presentation for the, um, GitHub people so you can, uh, play with this. I will in the read me, that's the one part I haven't finished yet in the Read Me, I'll write a little instructions how you can run this locally. A few last tips here. Um, I have built this for Mac. Um, so I'm using MBS here. Um, if you have, if you're lucky enough to have a computer with a Cuda, CUDA, uh, or Nvidia graphics card, um, change this line. I'll write this in the meet. Uh, read me as well to use your Cuda, because then this is gonna be also, again, a little bit faster. Um, and especially don't use your CPU, that's gonna take, uh, quite a long time. Um, yeah, that's, uh, well, ill write this in the read me, or I'll maybe do double check here. Um, and just put it in right away. Um, what else is important? Uh, yeah, that, uh, one more thing that I forgot to mention is when we talk about context sizes, um, so the number that our input that we can have, the higher you go with your input, the more compute, uh, it's gonna take as well. Uh, the simple reason for this is when we train, uh, when these models get, um, train. So let's say you accept, like even let's say you can go back to open AI example. They used to say you have a context length of 1000 tokens, right? Um, now they have massive amount of tokens overhead. The, uh, the reasons, um, this is important is because even though they say you have a length of 1000 tokens, when they train it, you actually train on one token. You train on two tokens, you train it on three tokens because it needs to be able to handle all variations of token lengths, right? So, um, that's why, uh, it's so reduced in here. And yeah, I, uh, would say if you guys have any questions, I'm happy to, uh, try to openly answer them in the last few minutes We have before the lovely end of this day, uh, at ML K and yeah. Alright, we're gonna give this one more try. 'cause I feel really bad about this. Um, uh, so what I had the last success of last night, that's why I kept going, is like starting a word and it would at least finish the word. Um, what is the word that comes out a lot?\n'slidetext': ''\n'from': 'vector'\n'score': 0.8833778500556946\n'normalizedScore': 0.8574566504761664\n'beforeDatePenaltyScore': 0.8833778500556946\n\n'documentId': '3d8d98464886dd773ef55f1d'\n'chunk_id': '687a292496f7bf47ed452aba'\n'access': 'granted'\n'part_number': 22\n'total_parts': 22\n'contentType': 'COURSE'\n'title': Secure by Design: Patterns and Principles for API Security\n'parentName': Overcoming API Security Hurdles\n'language': 'EN'\n'date': '2024-07-29T08:09:35.000Z'\n'abstract': APIs are transforming the way we build applications and deliver business value. Organisations can tap into new revenue streams by productizing their service capabilities through APIs. However, with the growing use of APIs also come new security risks. APIs represent gateways into our systems, and security vulnerabilities in our APIs can open organisations to different forms of attacks. In this talk, I’ll explain how API design affects security and reliability. A good strategy to minimise API security risks is adopting a secure and reliable by-design approach. This talk explains how we can leverage OpenAPI to enhance the reliability and security of our APIs. I’ll examine reliability from the perspective of the API being able to meet its contract. Secure APIs are reliable - they abide by the contract defined in the API specification. I’ll explain how we can leverage tools like fuzzy testers to automate our API security testing workflow and ensure our APIs are reliable and comply with the specification. I’ll also show how to leverage OpenAPI to design robust schemas that minimise security vulnerabilities and enhance reliability. For example, strict schemas without additional properties can reduce the risk of mass assignment attacks, while constraining property values through enumerations and other strategies can reduce the risk of injection and other types of attacks. Although robust API design alone won’t protect us against every single vulnerability, it can go a long way to deliver a secure and reliable user experience. I’ll include practical examples of the impact that different API design strategies have on API security and reliability.\n'parentId': '77e32cffa317fc54635b2503'\n'parentDescription': Build secure, reliable, and efficient APIs. This course delves into the critical aspects of API design and security. Learn how to conduct effective API design reviews, identify potential vulnerabilities, and implement robust security measures. Discover best practices for creating APIs that meet the evolving demands of modern applications. Through this course you'll learn: API design best practices Conducting API design reviews API security fundamentals Protecting APIs from common threats Leveraging OpenAPI for secure API design Building a strong API security program Secure your APIs and drive business success.\n'indexBrandName': 'APICon'\n'indexSeriesName': ''\n'author': 'Jose Haro Peralta'\n'chunkSource': 'text'\n'text': FinTech, uh, I had the opportunity to work with recently. So we have around 300 problems. They look like these, they're gonna tell you unconstrained integers, unconstrained, um, uh, strings, lack of rate limit and so on and so forth. And this is one of the best, actually the situation looks more like this. Like, like what we saw before, right? The 7,000 problems per spec, a thousand problem problems per spec. And these are things that we have to, you know, they're opening holes in our API. So suddenly people can request millions of items per, per page. They can maybe transfer unreasonable amounts of money, um, in a, uh, in a, in a single payment, or they can do mass assignment and things like that. And this is, um, and so I did a collaboration with Frank Cummins from smart. So the, we publish some videos on a PS security, we put together this repo, um, showcasing how you can use these tools, how you can put them together. So if you go to that, um, so that's the, the, well, the video doesn't show the full URL, but if you go to the, to the YouTube channel of Smart Bear, you will find the video and then the, the, the GitHub where you can see the examples, how to run, for example, with GitHub actions. And, and that's everything. We, we, we covered everything. I think we, that was very good. So, um, so we, we covered all the major vulnerabil vulnerability, if we can see at the same time, um, if you have any questions, um, ask me now, otherwise you can connect with me later and, and we can talk about this more. Um, I think that's the end. So, uh, again, uh, I got this copy of the book. I'm gonna ruffle one copy now. Um, so to make it, um, kind of interesting. I wanna ask, so who can tell me, what's the status code, the recent, the recent phrase for status code four 18? Yes. Alright, got it. Okay. Alright, so that, that's, uh, that's it. Um, so there, there was a copy running around. Uh, alright, so, uh, you, you can keep that copy. Yeah, there you go. Alright, so if you have any questions, any comments, any suggestions, um, feel free to ask now. Otherwise we're good to go. Yeah. Alright, thank you.\n'slidetext': ''\n'from': 'vector'\n'score': 0.883172869682312\n'normalizedScore': 0.8574589849175986\n'beforeDatePenaltyScore': 0.883172869682312\n\n'documentId': 'fb78cf2215e99d64eba7d65c'\n'chunk_id': '687a326bca798166d3ba50d9'\n'access': 'granted'\n'part_number': 25\n'total_parts': 26\n'contentType': 'COURSE'\n'title': Expertenwissen trifft AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'DE'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': Wir stellen unsere neue AI Lösung vor und zeigen wie du unsere RAG Technologie nutzt, um klare Antworten auf deine Software-Entwicklungs Probleme bekommst. Zusätzlich lernst du von Dr. Pieter Buteneers was RAG Modelle so besonders macht und wie du sie in deine Projekte einbinden kannst.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Dr. Pieter Buteneers'\n'chunkSource': 'text'\n'text': absetzen. Probiert mal bisschen rum. Das ist sozusagen der erste Schnupperkurs. Und wie schon gesagt, als Abonnentin oder als Abonnent habt ihr zehnmal so viele Fragen pro Monat. Das heißt, wie auch immer, es lohnt sich auf jeden Fall, Abonnent, Abonnentin zu sein, Mitarbeiter zu sein und dieses Entwickler Intelligence zu nutzen, um wie wir jetzt schon gesagt haben, dorthin zu finden oder dorthin zu gelangen, wo ihr am besten weiterkommt, nämlich das fantastische Know-how, die fantastischen Inhalte unserer Expertinnen und Experten. Damit kommen wir jetzt zum Ende unserer Präsentation. Wir bedanken uns sehr sehr für eure Teilnahme, für eure Fragen, für eure zahlreichen und auch für eure Interesse. wie schon gesagt, laden wir euch sehr sehr herzlich ein, mitzumachen, auszuprobieren. Und wenn ihr auch Feature-Vorstellungen habt, schreibt einfach in Kundenservice-Seite Entwickler.de, wie zum Beispiel diese Idee. Okay, vielleicht kann ich das eines Tages möchte ich das gerne integrieren über den API. Vielleicht auch ganz andere Ideen. Dann sind wir auf jeden Fall interessiert daran, euch unseren Kunden und Nutzern, gerne im Dialog zu sein und auch von eurer Seite aus zu lernen, was euch am meisten hilft, diese Idee von Continuous Learning weiter zu verfolgen, weiter zu entwickeln. Und ich glaube aber nochmal, mit dieser Weltpremiere, die wir jetzt hier heute haben, haben wir einen ganz großen, faszinierenden Schritt nach vorne gemacht. Ich bin jedenfalls total begeistert und ich kann auch sagen, das ganze Team hinter Entwickler.de ist begeistert von dieser Sache. Damit sage ich auf jeden Fall vielen vielen Dank. Many, many thanks, Peter, to you. Always a pleasure to working with you. Now is a great day. We really announced something. We continue to work and to think on the next step on Entwickler.de. And as Peter already said, if you want to get deeper into those topics, you can learn very much about these things at our machine learning conferences. There are also bootcamps getting you into the basics. And yes, of course, Peter himself will be also there. So have a beer or have a coffee with him and ask as many questions if you like to ask him at the conference. Thank you also,\n'slidetext': ''\n'from': 'vector'\n'score': 0.8828243017196655\n'normalizedScore': 0.8580407442000054\n'beforeDatePenaltyScore': 0.8828243017196655\n\n'documentId': 'fd1e5ad8ac352c46d9f10793'\n'chunk_id': '687a32daca798166d3ba552a'\n'access': 'granted'\n'part_number': 2\n'total_parts': 19\n'contentType': 'COURSE'\n'title': Does size matter? The Joys and Pains of K8S (Auto)Scaling\n'parentName': Set Sail and Build Fast With Kubernetes\n'language': 'EN'\n'date': '2024-07-26T13:14:14.000Z'\n'abstract': Autoscaling is one of K8S 'magic' features that folks get especially excited about. But beyond he technical details - autoscaling is a business concern. In the end it all boils down to balancing efficiency with availability. How much resources do our containers really need? When do we add more pods? How many pods is too many? Do node sizes matter? Let's see how to scale Kubernetes the scientific way.\n'parentId': '8c36f1c96a126627b0fcae02'\n'parentDescription': Kubernetes has revolutionized the way we deploy and manage containerized applications. This course delves deep into the core concepts of Kubernetes, focusing on critical areas: scaling, security, and architecture. Learn how to: Optimize resource utilization: Master the art of scaling Kubernetes applications to meet varying workloads. Discover advanced techniques beyond basic horizontal pod autoscaling. Fortify your Kubernetes environment: Understand the security challenges posed by cloud-native environments. Learn best practices to protect your clusters and data. Design scalable and resilient architectures: Gain insights into building robust Kubernetes-based systems. Explore architectural patterns and best practices. Whether you're a seasoned Kubernetes engineer or new to the platform, this course will equip you with the knowledge and skills to create high-performing and secure applications. Join us to explore the world of Kubernetes and become a true master of the platform!\n'indexBrandName': 'DevOpsCon'\n'indexSeriesName': ''\n'author': 'Anton Weiss'\n'chunkSource': 'text'\n'text': bigger and faster. Discs, memory, CPU, you name it, network. And, um, um, we started, uh, being, uh, less frugal, right? By the way, if you're actually actually interested in how it has evolved during the ages and how all that changes our perception of space and time, make sure to watch, uh, a documentary series by none other than Mark Burgess, the creator of CF Engine, and the person who actually defined desired state configuration as we know it today much before, uh, Kubernetes came to life. Uh, it's called Bigger, fast, faster, smarter. Look it up on YouTube. I'll have a slide, uh, that refers to it later on. Uh, but back to our story. So information systems kept growing. Our apps became huge and heavy. We needed more servers. We needed bigger servers. But then, you know, the cloud came along, right? And, uh, with it, the promise of infinite elasticity, right? So, uh, for the cloud providers, the size does matter because for them, the bigger is better, right? You need more servers, we'll provision them for you. We'll just put them on your bill, right? You have more traffic, no problem. We'll put it on your bill. And this easiness of access to resources has made us carefree. When I ask a developer, how much memory does your application need in production, they usually tell me, I don't know, give it two jigs, three jigs. And when I tell them, well, that's quite a lot. They say, so what provision larger machines, uh, just a shrug, but travel is, uh, size does matter, and bigger isn't necessarily better. And also smaller isn't necessarily better. And the question is, how do we find the right size for our application? And then the question we'll be looking into today, specifically from the angle of Kubernetes. But first, uh, let me introduce myself, and with that, I'll go back to my screen sharing and share my slides. So let's do that, hopefully. Okay, no, that's not the slides that my slack. Okay, so now you should be able to see my slides, and I'll put myself in the small corner here. Okay? So, uh, this is the documentary series that I was referring to. Definitely watch that to better understand how it is evolving. And this is me. So I am an, people usually call me an, but, uh, my full name is Anton Vice. You can find me on the x, uh, social, uh, network, previously known as Twitter.\n'slidetext': ''\n'from': 'vector'\n'score': 0.8851634860038757\n'normalizedScore': 0.859296810143272\n'beforeDatePenaltyScore': 0.8851634860038757\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a313dca798166d3ba4494'\n'access': 'granted'\n'part_number': 3\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': LLM, you send a prompt and it gives back the answer. And uh, yeah, usually like, what's the day the Germany was united, right? And then it can answer when was it? And for the day workshop, um, uh, we prepared a data set. I'll quickly show you this data set. So I said, I told you already that this was a part, this is a part of the course. And in Datadog Club, this is the com uh, a community that I'm running. We have many free courses. And usually when there is a free course, there are many participants of this course. And there are many, many, many questions. And in our community we have, for our courses, we have these documents with frequently asked questions. So we usually ask people to check this document. So when they have a question, they first check the document. And if they don't see an answer to their question, then they ask this question in Slack. And this document, uh, this is for our data engineering course. This document is 326 pages long, right? So this is how it looks like. So there are all sorts of errors or all sorts of, um, issues people have. And um, yeah, there are answers to these questions too. So this is community maintained. So students, when they come across a problem, they go there. If they don't see the answer, they figure this out and put it there. But you can imagine that this is very difficult to actually find anything in this document, like the more than 300 pages. And what we're going to do today is we're going to take these documents, there are three of them, Oops. So for our data engineering course, for our machine learning course, and for our ML lops course, all documents look similar. And we want to build a bot, a q and a system that given a question from a student will answer based on the content of these documents. And, and if we talk about a usual LLM, so we know that l lms, they are good at answering questions. And let's say this is a student, so a student wants to ask. Um, so a very common question is, I just discovered the course. Is it too late for me to join? So the course for example, our LLM course started last week. Somebody like, maybe somebody of you does not know about the course, they find out about this and they ask this question, okay, I just found the course. This is very interesting. I want to join. And if the we ask this to an LM, maybe I can quickly show you what\n'slidetext': ''\n'from': 'vector'\n'score': 0.885270357131958\n'normalizedScore': 0.8604867996122979\n'beforeDatePenaltyScore': 0.885270357131958\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a3169ca798166d3ba4636'\n'access': 'granted'\n'part_number': 51\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': documents to put them in the context right then. Um, yeah, we build a prompt based on these documents and get back the answer. And actually when you were asking to try a different query, uh, because I had a problem, um, in my code, it was actually using the old query. Now we can try with something else. Um, do you remember what the query was? Yeah. Um, can I attend the course that started the days ago? Can I attend the course? That started a few days ago, so hopefully it works. Yeah, well, I don't know, maybe there is still another error, but it seems to work. Let me check the comments in the chat. As far as I recall, any API access to open AI requires prepaid credit free is only interactive usage. Okay. Maybe has anyone tried Grok or something else? How is it going with Grok? Yeah. Okay. How difficult was it to Very Easy. Very easy. Uhhuh. So like, uh, with Gro, do you, can you use OpenAI client or you need to install their own library? Uh, we have their own library. So you did keep install Grok? Right. Okay. And just Changed the import, change the client and then surprisingly their a p is almost the same as Open. Mm-Hmm. So don't Have to change much. Okay. I'm unsurprised. 'cause like, I guess they set the standard and everyone else copies that. Um, any more questions or We go have lunch now. So when do we come back? Like in one hour or like, I guess one hour is enough. Can you say it again? The program Is one 30. One 30. So one hour. One hour. Okay. So I'll just, I see that some of you still have questions, so if you don't, you can leave. We'll come back to in one hour, one 30 and if you have questions, I can answer them. Yeah. Is it something, a particular Question on, On Uhhuh? Okay. Does anyone have like generic general questions? Yeah. If you Have any insight as to why this works, uh, with you. Rephrase the Question, Uhhuh. 'cause Uh, is there cement similarity actually? Mm-Hmm. So the question is, uh, if I have any ideas why when I rephrased it still works. And I think like if we look at the documents that, um, we get, and let me check here, um, look out query, because I think it, uh, Elasticsearch can still retrieve relevant documents. Do I still have now here? I don't remember if I fix it because like it still has, uh,\n'slidetext': ''\n'from': 'vector'\n'score': 0.8862370252609253\n'normalizedScore': 0.8614534677412652\n'beforeDatePenaltyScore': 0.8862370252609253\n\n'documentId': 'fd1e5ad8ac352c46d9f10793'\n'chunk_id': '687a32e1ca798166d3ba559c'\n'access': 'granted'\n'part_number': 18\n'total_parts': 19\n'contentType': 'COURSE'\n'title': Does size matter? The Joys and Pains of K8S (Auto)Scaling\n'parentName': Set Sail and Build Fast With Kubernetes\n'language': 'EN'\n'date': '2024-07-26T13:14:14.000Z'\n'abstract': Autoscaling is one of K8S 'magic' features that folks get especially excited about. But beyond he technical details - autoscaling is a business concern. In the end it all boils down to balancing efficiency with availability. How much resources do our containers really need? When do we add more pods? How many pods is too many? Do node sizes matter? Let's see how to scale Kubernetes the scientific way.\n'parentId': '8c36f1c96a126627b0fcae02'\n'parentDescription': Kubernetes has revolutionized the way we deploy and manage containerized applications. This course delves deep into the core concepts of Kubernetes, focusing on critical areas: scaling, security, and architecture. Learn how to: Optimize resource utilization: Master the art of scaling Kubernetes applications to meet varying workloads. Discover advanced techniques beyond basic horizontal pod autoscaling. Fortify your Kubernetes environment: Understand the security challenges posed by cloud-native environments. Learn best practices to protect your clusters and data. Design scalable and resilient architectures: Gain insights into building robust Kubernetes-based systems. Explore architectural patterns and best practices. Whether you're a seasoned Kubernetes engineer or new to the platform, this course will equip you with the knowledge and skills to create high-performing and secure applications. Join us to explore the world of Kubernetes and become a true master of the platform!\n'indexBrandName': 'DevOpsCon'\n'indexSeriesName': ''\n'author': 'Anton Weiss'\n'chunkSource': 'text'\n'text': become a cluster whisperer, just like me, here is a link to a preliminary signup form. We'll be working on certification this summer, and it's going to be released in the next month or two. Uh, of course it's not called a cluster whisper certification, but instead it's called Kubernetes Optimization Essentials. So if you want to really become a Kubernetes optimization expert, definitely sign up for that and we'll be happy to, uh, notify you once the certification becomes official. We're going to launch it free at least initially. So definitely sign up for that, scan the code, and now, okay. Ignore this QR code. It doesn't belong here. Okay, so the, the QR code on this slide don't scan that it belongs to another talk I did forgot to remove it, but, uh, know that once you get the auto scaling and resizing correctly, you can do this trick. Okay? So you get the right size guitar that's correctly adapted to your needs. Sometimes you want ukulele for a folk song and sometimes you want an actual acoustic guitar to bang out some rock anthems. Okay? And with that, I'll be happy to take your questions. Hello? Hello. Alright, anybody have any questions? I will come to you with the microphone. You can ask him directly, no questions. Oh, we got a question. One second. So for, uh, VPA mm-Hmm, we, most of us know the, the native feature is pretty dumb. So do you have any alternative, like maybe, uh, like some third party tool or something that would be a bit more smart than the current VPA? Well, as I said, again, uh, you know, without doing a vendor pitch, but doing a vendor pitch, uh, I represent perfect scale here and that's exactly what perfect scale does, right? We provide also the recommendations and the automation. So, and we, we do this much smarter than BPA does it, and thanks for that question. Thank you. Uh, in general, we, we do, uh, things we do better than VPA, you know, in a nutshell, we do, uh, uh, relate to historical trends. We do, uh, identify memory leaks. We are revision aware and we are also, uh, aware of the amount of resources the nodes have. 'cause that's, for example, something also VPA doesn't know how to do it. And also you don't need to define, uh, VPA resource for each new workload. We are smart enough to just watch your workloads once we're installed, Right? Yeah, I think that answered the question. Anybody\n'slidetext': ''\n'from': 'vector'\n'score': 0.8875333070755005\n'normalizedScore': 0.8616666312148967\n'beforeDatePenaltyScore': 0.8875333070755005\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a3152ca798166d3ba4550'\n'access': 'granted'\n'part_number': 27\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': course, not about all the courses, right? So this is like kind of, um, use case, specific filter, let's say. 'cause um, like what happens is in practice we have a Slack channel, and for each course there is, um, a separate Slack channel, right? So Slack workplace, and then like for each course there is a channel, and then there is a bot that answers questions from the students. And if we know that if a question is coming from this channel, we want to filter only by this course and so on, right? So it's not in the query. Uh, this is, we preset it. There are other questions. Only works on any equal conditions. So There's no Yeah, matching. There is no matching. Exactly. So this is exactly like the work causing in sql. Yes. Say case insensitive. Case insensitive. Yeah. So I think by default it's case insensitive. Um, but yeah, you can, like in Elasticsearch there is this concept of analyzer. So you have this, uh, query, right? So this is a string, and now you need to break it into words, into tokens, right? So when you do this, you get something like, like it's a list. I just, uh, we had, I just discovered. Then Elasticsearch would probably turn it into discover. Um, then it would drop the 'cause like it's, it doesn't really care carry any, like it doesn't have any meaning, right? So we drop the course, um, then, I don't know, maybe it'll also drop. Can, uh, I still join, right? So this is, um, and this process of turning the text into tokens is called, um, so now it's called tokenization in Elasticsearch, it's done by a thing called analyzer. So analyzer kind of analyzes the text and turns it into tokens, and then you can use different analyzers. So the default analyzer, um, is doing this thing. And the default analyzer actually, like here, it'll turn I into a lowercase I, right? So it'll lowercase all the words. Uh, and if you need a specific analyzer, you can check the analyzers that already exist. And if in your case you need to be case sensitive, then you can use a different analyzer that doesn't do that. So this is instead of using a, Yeah, so like I showed here, so this is an embedding what I call vector. So this is embedding, but we follow this path. Follow this path today. Yes. Well, in my experience, this actually works better. Um, vector database, they want to sell you their products.\n'slidetext': ''\n'from': 'vector'\n'score': 0.8867948055267334\n'normalizedScore': 0.8620112480070733\n'beforeDatePenaltyScore': 0.8867948055267334\n\n'documentId': 'fb78cf2215e99d64eba7d65c'\n'chunk_id': '687a3263ca798166d3ba5078'\n'access': 'granted'\n'part_number': 14\n'total_parts': 26\n'contentType': 'COURSE'\n'title': Expertenwissen trifft AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'DE'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': Wir stellen unsere neue AI Lösung vor und zeigen wie du unsere RAG Technologie nutzt, um klare Antworten auf deine Software-Entwicklungs Probleme bekommst. Zusätzlich lernst du von Dr. Pieter Buteneers was RAG Modelle so besonders macht und wie du sie in deine Projekte einbinden kannst.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Dr. Pieter Buteneers'\n'chunkSource': 'text'\n'text': weil wir jetzt sehr viel über Machine Learning geredet haben, kann ich, wie gesagt, direkt daran teilnehmen Und Empfehlen. Deine Stimme versagt gerade, deswegen gehe ich jetzt gerade nochmal in den Chat. Und da haben wir nochmal eine weitere Frage. Wie sieht es mit abgelaufenen Inhalten aus? Kurse, Tutorials, die über die Plattform nicht mehr verfügbar sind. Sind diese über AI noch zu finden? Zu finden, ja. Allerdings musste man auch auf der Konferenz sein, um Zugriff darauf haben. Man wird dann in der Regel darauf verwiesen, was der neue Inhalt ist und kann dann auf der nächsten Buster, der nächsten MLCon, der nächsten Jax teilnehmen. Also es ist kein Loophole, um diesen Content leider zur Verfügung zu stellen. Allerdings wird sie in die Suche und damit in die Generierung der Antwort berücksichtigt. Also man freundet sich vielleicht auch nochmal so vorstellen, genau. Also nochmal erstmal als Abonnent und Abonnentin von Entwickler.de habt ihr unfassbar viele Inhalte direkt zur Verfügung. kommt ihr sehr, schnell ran, zum Beispiel über diese intuitive Mechanismus des Fragestellens von Entwickler.de. Entwickler Intelligence, verzeihung, das ist das eine. Und dann auf der anderen Seite, genau, gibt es bei uns eben diese Add-on-Produkte. Das heißt eben auch Seminare, Konferenzen teilnehmen, Themen teilnehmen, die ich natürlich nochmal separat buchen kann. Und ich glaube auch eine Stärke liegt halt auch daran, wenn ich sage, ok, jetzt bin ich in meinem Team und wir sind in der Situation, dass wir sagen, hey, wir machen schon, mit Kubernetes haben wir einen gewissen guten Grad an Erfahrung aufgebohrt. Und jetzt sind wir aber in der Lage, dass wir sagen, Security ist es nochmal eine spezielle Challenge, die wir nochmal weiter irgendwie aufbauen wollen. Dann kann ich auch nicht sagen, hey, zeig mir mal, was gibt es eigentlich auch für Verschulungen, für Trainings etc. Wie komme ich weiter mit dem Thema Kubernetes Security, einfach nur als Beispiel hier ausgewählt. Und in dem Moment genau, kann ich dann auch eben wiederum Entwickler Intelligence verwenden, um mich da hinzuführen, was meine nächsten Steps sein\n'slidetext': ''\n'from': 'vector'\n'score': 0.8868052363395691\n'normalizedScore': 0.862021678819909\n'beforeDatePenaltyScore': 0.8868052363395691\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d9ef5cff79d6f5cdc4'\n'access': 'granted'\n'part_number': 92\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': part in installation from the rest and see what happens. And I, I think that's all for me. So thanks a lot for attention. I know it was fast sometimes, uh, but hopefully you can come back to the notes. Also. Let me um, mention it one more time. So right now there is a course about that. So this thing that I described here that I explained. So this was module one of the course and module two. So the course is called LLM Zoom camp. And this course just started. So if you want to learn more about vector search or how you orchestrate track or how you do monitoring, what are the best practices? The course just started. You can join it. And also if you later have questions about this workshop, this particular workshop, you can just join the course and ask these questions in the course channels. 'cause the content we have here in the first and second one and what they presented here are the same. So there are students who are doing this. There are um, instructors. I also monitor, uh, this channel. So if you have any questions about this workshop, maybe you will have later this week or this month or next year. I dunno. You can always come to our slack and ask questions about this. And in order to do that, uh, you, yeah, you can sign up to the slack and then you join the course channel. And then if you have any questions, this is the best place to get answers. Yep. Light lectures or It's prerecorded. So for example, everything I showed here is actually split into module. Yeah. This, this talk here is recorded, right? That we can Have access to. Uh, that's not a question for me. I don't know. So maybe you know if this is recorded or not. Sorry, the question is if this content is recorded? Yes. Is recorded Website. Okay, that's good that I deleted my hugging face. Um, talking. Yeah. 'cause I de deactivated it. So if you watch this in recording, it no longer works. Okay, well I guess that's it. Thanks for attention or more. Yeah, so we still have time to um, it's supposed to finish at five, right? So we still have roughly one hour. So I can stick around if you need help. Yeah. Yes. Do You have some agents? Agents not in the course and not here. So an agent is a thing that can do actions. So for example, you can, so in addition to that, mm. And so let's say you want to build an agent that traits on\n'slidetext': ''\n'from': 'vector'\n'score': 0.8902672529220581\n'normalizedScore': 0.8643460533431642\n'beforeDatePenaltyScore': 0.8902672529220581\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d3ef5cff79d6f5cd9b'\n'access': 'granted'\n'part_number': 51\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': the old query. Now we can try with something else. Um, do you remember what the query was? Yeah. Um, can I attend the course that started the days ago? Can I attend the course? That started a few days ago, so hopefully it works. Yeah, well, I don't know, maybe there is still another error, but it seems to work. Let me check the comments in the chat. As far as I recall, any API access to open AI requires prepaid credit free is only interactive usage. Okay. Maybe has anyone tried Grok or something else? How is it going with Grok? Yeah. Okay. How difficult was it to Very Easy. Very easy. Uhhuh. So like, uh, with Gro, do you, can you use OpenAI client or you need to install their own library? Uh, we have their own library. So you did keep install Grok? Right. Okay. And just Changed the import, change the client and then surprisingly their a p is almost the same as Open. Mm-Hmm. So don't Have to change much. Okay. I'm unsurprised. 'cause like, I guess they set the standard and everyone else copies that. Um, any more questions or We go have lunch now. So when do we come back? Like in one hour or like, I guess one hour is enough. Can you say it again? The program Is one 30. One 30. So one hour. One hour. Okay. So I'll just, I see that some of you still have questions, so if you don't, you can leave. We'll come back to in one hour, one 30 and if you have questions, I can answer them. Yeah. Is it something, a particular Question on, On Uhhuh? Okay. Does anyone have like generic general questions? Yeah. If you Have any insight as to why this works, uh, with you. Rephrase the Question, Uhhuh. 'cause Uh, is there cement similarity actually? Mm-Hmm. So the question is, uh, if I have any ideas why when I rephrased it still works. And I think like if we look at the documents that, um, we get, and let me check here, um, look out query, because I think it, uh, Elasticsearch can still retrieve relevant documents. Do I still have now here? I don't remember if I fix it because like it still has, uh, things like course and started. I think this is, uh, um, even though like you see that this is not the most relevant one, um, yeah. So this is like fourth most relevant one. It's still there. So for an LLM, it's uh, still enough?\n'slidetext': ''\n'from': 'vector'\n'score': 0.8905099034309387\n'normalizedScore': 0.8645887038520448\n'beforeDatePenaltyScore': 0.8905099034309387\n\n'documentId': '31a17545468e0020a46cfb6e'\n'chunk_id': '687a291c96f7bf47ed452a3b'\n'access': 'granted'\n'part_number': 17\n'total_parts': 39\n'contentType': 'COURSE'\n'title': Workshop: AI-Powered Node.js Development with OpenAI\n'parentName': Unleash the Power of AI in Web Development\n'language': 'EN'\n'date': '2024-12-19T14:10:00.000Z'\n'abstract': In this immersive full-day workshop, you'll discover how to harness the power of OpenAI's advanced language models and seamlessly integrate them into your Node.js applications. Through hands-on projects, you'll learn to build intelligent chat interfaces, create advanced search systems, develop question-answering applications, and explore cutting-edge techniques like AI-driven function calling. Gain practical experience in leveraging AI to revolutionize your Node.js development workflow.\n'parentId': '33b5c3a3690ff71983ad2b73'\n'parentDescription': In this course, you’ll explore the intersection of AI and JavaScript, learning how to integrate machine learning models, enhance user experiences, and build smarter, more intuitive web applications. Drawing on insights from industry experts, this course equips you with the tools and techniques to harness AI’s potential in JavaScript-based development. You’ll start with a solid introduction to AI and its role in web development, diving into expert articles and insightful sessions from our iJS Conference. Then, you’ll engage in practical hands-on workshops focused on real-world applications, such as AI-powered Node.js development and building AI applications with JavaScript. By the end of the course, you’ll have a better understanding of how to integrate AI into your own web projects and create innovative web experiences. Join us and take your JavaScript skills to the next level!\n'indexBrandName': 'InternationalJavaScriptCon'\n'indexSeriesName': ''\n'author': 'Nir Kaufman'\n'chunkSource': 'text'\n'text': you explain And let me let, let's continue I'm gonna create a new file called search I'm gonna pay some mock data MOOC data and Prisma, And this is for the people here who don't want to mess semantic search, uh, feature that we are gonna build. create a, a util called uh, db. We already got it here. this is gonna answer your question, already know about my candidate. of my candidates from the database. and you need it, it's gonna generate type three And if I want to, uh, to uh, use, uh, uh, You see candidate, import candidate and give me very, very nice experience. Period. Cool. We want to take this data what is semantic search? of bass. No. Alright, What, let's talk about search. Do I need AI and LN for this? Why not? about this old recruiting process. Do I need AI technologies to do this? The answer is if I like to, to use something because similarity search mean, with strong, uh, uh, background in what, If you look through this entire, uh, this entire candidate, It's not exist here in similarity search. and say, well, LA's got a strong background in Python, I'm go ahead and give you Alice as a result It's similar enough to what you're looking for. because it's not something specific. There is a process, a process that's called embedding, save it as a rep, as a vector representation and then you're gonna play with it. now the first example, we introduced a simple, of a machine learning model called embedding model. So you get an unstructured data, I think that this is the most visualized way that I, Here. You see like, kind of, well, I tried Actually, I need to fix it a little bit. but if like a hotel is, uh, 1.9 Think about in this example, three dimensional graph, but basically we're talking about groups. And then I want to look, as a minus two and two. I mean, literally, it's close to this vector, right? minus 1.9 and one. but this is the vector representation. well, they're similar. How many of you seen the movie? Uh, interstellar. with where is inside the cataract, When we're talking about vectors, we've got like, layers on layers of multidimensional, uh, um, around the graph dependence on the vector representation. shows two. but I want you to think about groups. and we use the embedding model in order sits on a structure that we are going to use in this context. I can get results back the term that you searched for if you\n'slidetext': ''\n'from': 'vector'\n'score': 0.8825041055679321\n'normalizedScore': 0.8646395517266079\n'beforeDatePenaltyScore': 0.8825041055679321\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd6b'\n'access': 'granted'\n'part_number': 3\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': a data set. I'll quickly show you this data set. So I said, I told you already that this was a part, this is a part of the course. And in Datadog Club, this is the com uh, a community that I'm running. We have many free courses. And usually when there is a free course, there are many participants of this course. And there are many, many, many questions. And in our community we have, for our courses, we have these documents with frequently asked questions. So we usually ask people to check this document. So when they have a question, they first check the document. And if they don't see an answer to their question, then they ask this question in Slack. And this document, uh, this is for our data engineering course. This document is 326 pages long, right? So this is how it looks like. So there are all sorts of errors or all sorts of, um, issues people have. And um, yeah, there are answers to these questions too. So this is community maintained. So students, when they come across a problem, they go there. If they don't see the answer, they figure this out and put it there. But you can imagine that this is very difficult to actually find anything in this document, like the more than 300 pages. And what we're going to do today is we're going to take these documents, there are three of them, Oops. So for our data engineering course, for our machine learning course, and for our ML lops course, all documents look similar. And we want to build a bot, a q and a system that given a question from a student will answer based on the content of these documents. And, and if we talk about a usual LLM, so we know that l lms, they are good at answering questions. And let's say this is a student, so a student wants to ask. Um, so a very common question is, I just discovered the course. Is it too late for me to join? So the course for example, our LLM course started last week. Somebody like, maybe somebody of you does not know about the course, they find out about this and they ask this question, okay, I just found the course. This is very interesting. I want to join. And if the we ask this to an LM, maybe I can quickly show you what happens. So I will be using charge GPD. So I just discovered the course. How many does anyone else not have an account in charge G pt, why you're missing out. Uh, can I still join? Right? So we ask alem question\n'slidetext': ''\n'from': 'vector'\n'score': 0.8919334411621094\n'normalizedScore': 0.8660122415832154\n'beforeDatePenaltyScore': 0.8919334411621094\n\n'documentId': 'e15ac0b644cd3afbe8bc9675'\n'chunk_id': '687a3152ca798166d3ba4557'\n'access': 'granted'\n'part_number': 20\n'total_parts': 21\n'contentType': 'COURSE'\n'title': Lessons in delivery through hyper growth at youtube and dropbox\n'parentName': Mastering CI/CD Best Practices\n'language': 'EN'\n'date': '2024-07-26T13:32:27.000Z'\n'abstract': Andrew will discuss his experience building continuous delivery systems through the hypergrowth phase of engineering organizations. He will focus on the lessons and principles he has learned at YouTube and Dropbox where he saw systems scale exponentially in headcount and system scale. He will discuss how those principles shaped his opinions of building a great developer experience that 'defaults to fast.'\n'parentId': '4497b71b66108d36f402c74e'\n'parentDescription': Unlock the secrets of seamless software deployment and agile development! This dynamic course dives into CI/CD and DevOps, offering expert insights and practical tips. Discover the essentials of release strategies, continuous delivery, and leveraging AWS for large-scale DevOps transformations. Perfect for leaders and developers alike, elevate your deployment game and drive innovation with this comprehensive guide. Learn how to: Master CI/CD practices: Understand the fundamentals of continuous integration and continuous delivery, and learn how to implement them effectively in your organization. Scale DevOps across your enterprise: Discover strategies for expanding DevOps practices beyond small teams, ensuring consistency and efficiency. Prioritize security in your pipeline: Learn how to build secure software by integrating security into your CI/CD pipeline. Align DevOps with business objectives: Explore the role of DevOps in achieving organizational goals and delivering value to customers. Whether you're a developer, operations engineer, or business leader, this course provides valuable insights and practical guidance to help you excel in the DevOps landscape. Join us to unlock the full potential of DevOps and drive your organization's success!\n'indexBrandName': 'DevOpsCon'\n'indexSeriesName': ''\n'author': 'Andrew Fong'\n'chunkSource': 'text'\n'text': spending, like we've built this, um, we've actually been deploying this in production. We have some pretty large users on it. I think Netflix has also done some really good work in this space. My, I I think at this point the um, what we're starting to see is like Argo has done like a great job of V one in the, uh, in the cobe space and that there is an extension that needs to happen as you think about developer experience and the rest. Um, because you probably also wanna give Sam ux ui, the more complicated your delivery systems get, probably the less command line tooling you're gonna be able to use for it as well. People need to be able to reason through it and see like what states are. Um, and so we think there's like a pretty big jump here that's gonna happen in the next 24 months around how delivery is managed. Um, and we also completely separate talk, but like I'm in the camp at CD is very different than CI as well. Um, very different use case and also very different technology stack that needs to implement at I see. Smile on this one. Um, so that's, uh, I'll conclude there. You can take a look at some resources here. Um, some of this stuff, uh, is kind of interesting. So my co-founder did a 4 million line Python migration, um, monolith into like a Lambda service. It takes a lot of these principles. Um, Netflix has our managed delivery project, and then we kind of take this entire talk is unbundled as a, uh, as a series of a series of architecture, um, architecture, uh, overviews. Um, so feel free to take a picture or whatnot. And then I'm gonna put the last slide up here, which is like ask for feedback. Cool. Can you go back to one? Yep. I can go back to one side. Yes. And I'm checking over here. Check nothing on this side, but happy to take questions. Happy to like open the floor up discussion. This connect. So this idea of convergence, does this connect at all to s from ideas? Does this connect Or, Um, so the question is, is how does this connect to Mark Burgess's, uh, promise theory stuff? Um, oddly there's probably a couple things. If you look at the actual design of this, it's very similar to what Mark has done. I've actually talked to him a bunch about it. Um, and then I think the other part is, uh, if you were to dig into our actual product, there's a lot of pieces that are influenced\n'slidetext': ''\n'from': 'vector'\n'score': 0.8924815654754639\n'normalizedScore': 0.866615582791547\n'beforeDatePenaltyScore': 0.8924815654754639\n\n'documentId': 'ffe23ecef08c3e1c0b51ef2b'\n'chunk_id': '6876e1ae7899910dd58c1c2b'\n'access': 'granted'\n'part_number': 27\n'total_parts': 27\n'contentType': 'COURSE'\n'title': Intelligent Forms: Spice up your Angular Forms With AI & LLMs\n'parentName': Advanced Angular: Signals, NgRx & AI Forms\n'language': 'EN'\n'date': '2025-05-02T10:49:04.000Z'\n'abstract': Whether it's address data, bank details, or job application information - filling out extensive forms can be a time-consuming and tedious task for users. What if users could simply copy text to the clipboard and the form would fill itself in automatically? A Smart Form Filler makes exactly that possible: with the help of a Large Language Model (LLM), which recognises both the content of the clipboard and the structure of the form, your Angular forms can be filled out fully automatically - even completely locally and offline-capable, if desired. Christian Liebel from Thinktecture shows you how to implement this generative AI feature in Angular.\n'parentId': '5e78d57fd77e42835f9996dc'\n'parentDescription': Level up your Angular skills with this advanced course, highlighting the framework’s evolving features. We will cover cutting-edge topics, from mastering state management with the NgRx Signal store and unlocking AI-enhanced forms’ potential, to revisiting the performance benefits of server-side rendering. You will learn how to effectively use Signals through professional tips and real-world insights, and explore emerging features like httpResource in Angular 19.2. Whether you are scaling enterprise apps or upgrading legacy codebases, this course equips you with the practices to build smarter, faster Angular applications and stay ahead in today’s Angular ecosystem.\n'indexBrandName': 'InternationalJavaScriptCon'\n'indexSeriesName': ''\n'author': 'Christian Liebel'\n'chunkSource': 'text'\n'text': of LMS that you may hit. So, uh, yeah, basically feeding like database, right? Um, uh, a vector database, uh, with the concept of PDF, uh, would probably make sense. At least that's what my colleagues do. Okay. We have no online question. Um, and as we are already over time, uh, I wanna thank you for your attention. Uh, thanks for joining. Uh, see you tomorrow or later for the second session, which will be similar, but more focused on the new APIs that we have and a little bit more in depth in that regard. So thanks for joining and yeah.\n'slidetext': ''\n'from': 'vector'\n'score': 0.8820798397064209\n'normalizedScore': 0.8715501057534782\n'beforeDatePenaltyScore': 0.8820798397064209\n\n'documentId': 'e166d6c9e06d2f15464f870c'\n'chunk_id': '6876df707899910dd58c1056'\n'access': 'granted'\n'part_number': 4\n'total_parts': 16\n'contentType': 'COURSE'\n'title': In the shadows of GenAI: Key Machine Learning Trends Beyond Generative AI\n'parentName': AI Systems Done Right: MLOps and Security Strategies\n'language': 'EN'\n'date': '2025-04-07T12:02:11.000Z'\n'abstract': While we were sitting in the office getting caught by the LLM rise in the last couple of months, in the shadows, many areas of ML made progress, gained traction, and are now ready for broad adoption - but what are these trends? This talk explores critical and industry-agnostic trends in machine learning beyond the GenAI hype.\n'parentId': 'dee36fcd279fe788093b3808'\n'parentDescription': Ready to unlock the secrets of building secure and scalable AI systems? Dive into the essentials of MLOps and MLSecOps, giving you the skills to keep your system safe, efficient, and bulletproof. From the latest in generative AI to mastering scalable workflows, you’ll walk away with the tools and tricks to turn AI into your safest, smartest asset!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Martin Stypinski,Leeanna Seidler'\n'chunkSource': 'text'\n'text': overcome to get it working. Um, and that's I guess why, why most of you are here to really learn and to go beyond the hype and learn what's deeper under the hood. So if you don't get the answers that you're looking for, definitely uh, just ask the speakers after the talk or talk to your peers. You'll be amazed how many of your peers here have had the same struggles. Don't feel stupid. I make so many of those mistakes as well. Feel free to talk to me as well. Uh, you'll see that by talking to people that you learn a lot. The machine learning community is really known to be a verbal community where we share a lot of information verbally. And it's not like in the blog post or in the YouTube videos where everything seems so simple. The reality is just quite different. So talk to everyone. If you didn't get the answer, we introduced another session tomorrow at one 30, the a MA session, which I will host with a few experts, uh, in the panel where you can ask the questions that haven't been answered yet. So come to that session with all the questions that you have. You can really ask anything and we'll answer them for you. Alright, so, um, I want to reiterate one thing. As a program chair, what's really important for me is that we get good feedback from you guys. So yes, please use the app, rate the talks and if you can write a small comment, tell us what you liked. So we make sure that we keep that for next time around. Tell us what you didn't like and what you missed. So we can also use that information. And also the speakers, you can ask them, they also like to get some feedback 'cause they want to do better as well. They don't mind if you write what you think 'cause we all learn from that. And then next time around, we make sure that we have an even better program program than we have today. Alright, enough introductions. So now I want to introduce our keynote speaker where we go beyond the hype of, uh, gen AI and go further than what you think and what you read in the blogs. Please give a warm welcome to Martin Stabinski. Thank you very much for having me here. Um, yeah, so welcome to Berlin and uh, we start with my keynote in the Shadows of Genai. And we all have to admit that in the last couple of years in terms of like language understanding, language retrieval, communication with machines, it's been a long and\n'slidetext': ''\n'from': 'vector'\n'score': 0.8858014345169067\n'normalizedScore': 0.8739046197674141\n'beforeDatePenaltyScore': 0.8858014345169067\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a3152ca798166d3ba4550'\n'access': 'granted'\n'part_number': 27\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': course, not about all the courses, right? So this is like kind of, um, use case, specific filter, let's say. 'cause um, like what happens is in practice we have a Slack channel, and for each course there is, um, a separate Slack channel, right? So Slack workplace, and then like for each course there is a channel, and then there is a bot that answers questions from the students. And if we know that if a question is coming from this channel, we want to filter only by this course and so on, right? So it's not in the query. Uh, this is, we preset it. There are other questions. Only works on any equal conditions. So There's no Yeah, matching. There is no matching. Exactly. So this is exactly like the work causing in sql. Yes. Say case insensitive. Case insensitive. Yeah. So I think by default it's case insensitive. Um, but yeah, you can, like in Elasticsearch there is this concept of analyzer. So you have this, uh, query, right? So this is a string, and now you need to break it into words, into tokens, right? So when you do this, you get something like, like it's a list. I just, uh, we had, I just discovered. Then Elasticsearch would probably turn it into discover. Um, then it would drop the 'cause like it's, it doesn't really care carry any, like it doesn't have any meaning, right? So we drop the course, um, then, I don't know, maybe it'll also drop. Can, uh, I still join, right? So this is, um, and this process of turning the text into tokens is called, um, so now it's called tokenization in Elasticsearch, it's done by a thing called analyzer. So analyzer kind of analyzes the text and turns it into tokens, and then you can use different analyzers. So the default analyzer, um, is doing this thing. And the default analyzer actually, like here, it'll turn I into a lowercase I, right? So it'll lowercase all the words. Uh, and if you need a specific analyzer, you can check the analyzers that already exist. And if in your case you need to be case sensitive, then you can use a different analyzer that doesn't do that. So this is instead of using a, Yeah, so like I showed here, so this is an embedding what I call vector. So this is embedding, but we follow this path. Follow this path today. Yes. Well, in my experience, this actually works better. Um, vector database, they want to sell you their products.\n'slidetext': ''\n'from': 'index'\n'score': 12.660113334655762\n'normalizedScore': 0.12660113334655762\n'beforeDatePenaltyScore': 0.12660113334655762\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a313dca798166d3ba4494'\n'access': 'granted'\n'part_number': 3\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': LLM, you send a prompt and it gives back the answer. And uh, yeah, usually like, what's the day the Germany was united, right? And then it can answer when was it? And for the day workshop, um, uh, we prepared a data set. I'll quickly show you this data set. So I said, I told you already that this was a part, this is a part of the course. And in Datadog Club, this is the com uh, a community that I'm running. We have many free courses. And usually when there is a free course, there are many participants of this course. And there are many, many, many questions. And in our community we have, for our courses, we have these documents with frequently asked questions. So we usually ask people to check this document. So when they have a question, they first check the document. And if they don't see an answer to their question, then they ask this question in Slack. And this document, uh, this is for our data engineering course. This document is 326 pages long, right? So this is how it looks like. So there are all sorts of errors or all sorts of, um, issues people have. And um, yeah, there are answers to these questions too. So this is community maintained. So students, when they come across a problem, they go there. If they don't see the answer, they figure this out and put it there. But you can imagine that this is very difficult to actually find anything in this document, like the more than 300 pages. And what we're going to do today is we're going to take these documents, there are three of them, Oops. So for our data engineering course, for our machine learning course, and for our ML lops course, all documents look similar. And we want to build a bot, a q and a system that given a question from a student will answer based on the content of these documents. And, and if we talk about a usual LLM, so we know that l lms, they are good at answering questions. And let's say this is a student, so a student wants to ask. Um, so a very common question is, I just discovered the course. Is it too late for me to join? So the course for example, our LLM course started last week. Somebody like, maybe somebody of you does not know about the course, they find out about this and they ask this question, okay, I just found the course. This is very interesting. I want to join. And if the we ask this to an LM, maybe I can quickly show you what\n'slidetext': ''\n'from': 'index'\n'score': 13.34361457824707\n'normalizedScore': 0.1334361457824707\n'beforeDatePenaltyScore': 0.1334361457824707\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd7d'\n'access': 'granted'\n'part_number': 21\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': I have a question. Can I still join the course after the start date? I want to match the words, join course start date, these things, right? So for that I use the text, uh, uh, type. And then this is, uh, a bit too much to type. So we'll just copy it from here And put it here. So I just explained this that we have multiple types. So for text, for section for question, the type is text because we want to perform text search and then which means we want to break the text into distinct words. And then when we do, uh, when we perform search, we want to do keyword matches, right? And of course is we want to match the exact we want to filter to be able to filter on this. And yeah, so this is not important for us. This is important when you have um, a cluster, Elasticsearch cluster with multiple instances. So you want to say like how many, like how exactly it's distributed across multiple instances. So in our case, we have only one instance of Elasticsearch. So we just use these settings. And of course when you use it in production and you have let's say three instances, you want to read more about this setting before you create an index. And I think I named it as client, right? So now what happened is we created this index and this is the name of this index, and we can now index or go over all our documents and put them in this index. Um, so save them to our Elasticsearch in the database. So I'll do this for documents, uh, in documents, Then Elasticsearch client index. And then we need to specify the name of the index, which is these course questions. And I think we do, what do I have here? Document? Yeah, so this is this field. The second document equals to document. So we simply go over all the documents we have, and here I'll use a progress bar that we installed, TQDM, um, and then I'll just do something like this. So we then we can monitor the progress of, uh, indexing. And now our documents are indexed. I see a question in chat. Uh, can you please show again what you entered in Jupiter from the beginning? So I want, I will show you where you can find it. So everything I do, well of course you just can follow this, um, this document and everything I type in Jupiter is in this document, but also like if you go to notebooks, in notebooks, you have this elastic search i, this elastic search notebook and it contains everything a like this is\n'slidetext': ''\n'from': 'index'\n'score': 14.811952590942383\n'normalizedScore': 0.14811952590942382\n'beforeDatePenaltyScore': 0.14811952590942382\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d1ef5cff79d6f5cd87'\n'access': 'granted'\n'part_number': 31\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': of this framework is you can replace it with anything you want. Like you build a prototype with Elasticsearch and then you tweak it, you see, okay, I don't like it. And then you can easily replace this part with something else and see, okay, with Vector database it works better. Or what if I used to, right? And this part stays unchanged. So these two parts are kind of isolated, independent from each other. And then you can plug and play. Yeah. So what the ask question where we, we'll still get like top five answers. Mm-Hmm, probably. So how, how would resolve this kind of problem? So the question is, if there is no answer in the database, uh, what happens, right? So then you can, um, like if Elasticsearch doesn't, uh, return anything, right? You can, you can just stop here, right? So it doesn't return anything you tell the user. I don't know. Like for example, what we can do in our courses, we can say, I don't know the answer to this question, ask the course instructors, right? So like, there, there, there is no answers to that. That's one possible thing. Sometimes what can happen is also the Elasticsearch or like the knowledge base returns something, but it's not that relevant, right? So it returns like five documents, but none of these documents actually contain the answer. Then you can build it in the prompt. You can say, answer the question based on the context, but if the constant context does not contain the, the answer to this question return, I don't know. Right? And then the LLM would look at this, it'll see that, um, the context, context is actually not relevant and it will say, I dunno, sometimes it doesn't always work. 'cause L lamps are yeah. Identify content Relevant. Well it's up to the LLM to identify, right? Yeah. It's magic. So you just, uh, keep your fingers crossed and then it either identifies as it's relevant, like, um, with the lamps you have this problem of hallucination, right? Probably everyone has heard about that. And then sometimes it may output some nonsense. It happens. You just have to leave with that, I guess, or have some guide rails. You're getting a score back and yeah, well, yes, you can filter by the score, but like maybe like it still doesn't work. It still returns something and then LLM does not thinks that it contains the answer. Sometimes even if you tell it, if there is no answer to this question, it might just make up an answer.\n'slidetext': ''\n'from': 'index'\n'score': 14.75055980682373\n'normalizedScore': 0.1475055980682373\n'beforeDatePenaltyScore': 0.1475055980682373\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d3ef5cff79d6f5cd9a'\n'access': 'granted'\n'part_number': 50\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': later, uh, with the actual thing. Uh, then there is a text input for the query. There is a button and we have, have some results. So let me start it. Stream lead. What is the extremely trap run? Okay, so we open this Euro And, Uh, yeah, how do I run a smartphone? Then of course this thing, like right now is just a dummy. It doesn't, it just returns this result for query. Now we want to replace it with from rack import, how do you call it? Rack? Well, probably that's all I need to do now. So let me refresh it. Query is not defined. Um, yeah, so we need elastic search query search. Hmm. Yeah, actually, like I was, uh, showing you like there was an error here. So search query contains this query and I should have put it in the retrieve part. So like that. So let me upgrade it. Otherwise we query every time with the same query. Okay, let's see if it works. Um, I just discovered the course. Can I still join it? No. Cool. So that was pretty fast. Like I, I don't, I save so much time with Charge g pt, like, I don't know. I use it all the time. Like for example, recently I created a T net application without knowing C at all. And then it just told me what, I told it what I want and then it told me what to do and then I execute and then I give it feedback and then it corrects. It's amazing. Like I, I have it here. Uh, this thing in C Sharp. Yeah. That's awesome. Anyways, so I think this is it for the first part. So we have done, um, quite a lot today already. So again, just to summarize, we have a que a student, a student has a question about the course and then, uh, we indexed our FAQ database. We put all the data from our um, Google documents to our knowledge base. We use Elastic Search for that. We can query elastic search and get back the, the documents to put them in the context right then. Um, yeah, we build a prompt based on these documents and get back the answer. And actually when you were asking to try a different query, uh, because I had a problem, um, in my code, it was actually using the old query. Now we can try with something else. Um, do you remember what the query was? Yeah. Um, can I attend the course that started the days ago? Can I attend the course? That started a few days ago, so hopefully it\n'slidetext': ''\n'from': 'index'\n'score': 14.709589958190918\n'normalizedScore': 0.1470958995819092\n'beforeDatePenaltyScore': 0.1470958995819092\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a315fca798166d3ba45be'\n'access': 'granted'\n'part_number': 40\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': uh, we'll call this function LLM. And the LLM gets in the prompt and it did, does some response, right? So we already have this function. We need to implement this too, right? And this is, we just need to take the goal we already have. And yeah, put them. So for this LLM I'll create a function, Uh, here. So, and then instead of printing, I just return it. So this is our LLM and this will be our build prompt function that I'll put here, build prompt, and then we have query and um, context documents, right? So we have context, we have prompt, and we return it. So, and then I'll remove this one. Okay? Just everything. I want to have everything in one place. So everything is related to building pros here. Then we have the l, m and the rock. So what should I use here? Can I attend the course that started? Can I attend the course that started a few days ago? No, you can, any other queries? Uh, let's try, how can I install Kafka? How can I install Kafka? I don't know actually if we have an answer to that question. Um, let me check. Kafka. Yeah, we'll have we have some, well we have 74 questions with Kafka. Yeah, so probably we do not have that, the answer to that question in the context. Let me print it so it's easier to read, right? So it says that it looks like there's no answer to that question, but here's how you can do it, right? So it's very nice of, uh, this LLM to tell us about that, but it might not tell us and just strictly go to to that part. And then we have no idea if this is coming from the context or is just coming up with answer yourself. So in this case, it looks correct. Well, maybe it's not, but like it looks legit. Uh, but we can also tweak out prompt to say like, um, let's try, if the provided context doesn't contain the answer, say I don't know, right? So we can try that and see what happens. Yeah, it's good. Is there any other way much confidence I cannot hear. Can you please? Confidence level Is, were there any way to add confidence level? Not that I know of here, right? So when you use open source cell lamps, usually somehow you can get the confidence levels. Um, but yeah, I dunno, they Put confidence on the query itself. Yeah, probably. Do you know how to do that? I guess it's the elastic search feature, Right?\n'slidetext': ''\n'from': 'index'\n'score': 14.880789756774902\n'normalizedScore': 0.14880789756774904\n'beforeDatePenaltyScore': 0.14880789756774904\n\n'documentId': 'a5cde8d9fe4aa89417ad4e15'\n'chunk_id': '6876d8ff225e92fc1ac7bb43'\n'access': 'granted'\n'part_number': 11\n'total_parts': 11\n'contentType': 'COURSE'\n'title': Tools & Processes for MLOps\n'parentName': AI Systems Done Right: MLOps and Security Strategies\n'language': 'EN'\n'date': '2025-04-07T12:02:11.000Z'\n'abstract': Training a machine learning model is getting easier. But building and training the model is also the easy part. The real challenge is getting a machine learning system into production and running it reliably. In the field of software development, we have gained a significant insight in this regard: DevOps is no longer just nice to have, but absolutely necessary. So why not use DevOps tools and processes for machine learning projects as well?\n'parentId': 'dee36fcd279fe788093b3808'\n'parentDescription': Ready to unlock the secrets of building secure and scalable AI systems? Dive into the essentials of MLOps and MLSecOps, giving you the skills to keep your system safe, efficient, and bulletproof. From the latest in generative AI to mastering scalable workflows, you’ll walk away with the tools and tricks to turn AI into your safest, smartest asset!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Hauke Brammer'\n'chunkSource': 'text'\n'text': [20] https://www.terraform.io [21] https://www.mlflow.org/docs/latest/model-registry.html [22] https://docs.docker.com/registry/deploying/ [23] https://www.kubeflow.org [24] https://www.tensorflow.org/tfx/guide/serving [25] https://www.elastic.co/de/logstash [26] https://prometheus.io [27] https://queue.acm.org/detail.cfm?id=1142065\n'slidetext': ''\n'from': 'vector'\n'score': 0.888577938079834\n'normalizedScore': 0.8766811233309756\n'beforeDatePenaltyScore': 0.888577938079834\n\n\nReferenceChunks:\n'documentId': 'a5cde8d9fe4aa89417ad4e15'\n'chunk_id': '6876d8ff225e92fc1ac7bb43'\n'access': 'granted'\n'part_number': 11\n'total_parts': 11\n'poc_summary': 'This seminar lesson examines the unique challenges of MLOps and outlines essential tools and workflows for deploying machine learning systems in production.'\n'chunk_summary': 'It provides a list of references and further reading on MLOps tools and best practices.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a315fca798166d3ba45be'\n'access': 'granted'\n'part_number': 40\n'total_parts': 94\n'poc_summary': 'This tutorial lesson guides beginners through building a Python chatbot using Retrieval-Augmented Generation with Elasticsearch and local or cloud LLMs.'\n'chunk_summary': 'Example queries demonstrate the chatbot answering course-related questions with context-aware prompts.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d3ef5cff79d6f5cd9a'\n'access': 'granted'\n'part_number': 50\n'total_parts': 93\n'poc_summary': 'This course lesson guides beginners through building a chatbot using Python, LLMs, Elasticsearch, and local models with practical coding and deployment.'\n'chunk_summary': 'Chatbot backend code is modularized into functions and integrated with Streamlit for live use.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'e166d6c9e06d2f15464f870c'\n'chunk_id': '6876df707899910dd58c1056'\n'access': 'granted'\n'part_number': 4\n'total_parts': 16\n'poc_summary': 'This seminar lesson examines key machine learning trends and innovations beyond the current generative AI focus.'\n'chunk_summary': 'It encourages active participation, peer discussion, and highlights the value of community knowledge sharing in ML.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'ffe23ecef08c3e1c0b51ef2b'\n'chunk_id': '6876e1ae7899910dd58c1c2b'\n'access': 'granted'\n'part_number': 27\n'total_parts': 27\n'poc_summary': 'This seminar lesson demonstrates how to enhance Angular forms with AI-powered smart form filling using large language models, both online and offline.'\n'chunk_summary': 'It concludes with remarks on future sessions and the potential of new APIs for AI-enhanced forms.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'e15ac0b644cd3afbe8bc9675'\n'chunk_id': '687a3152ca798166d3ba4557'\n'access': 'granted'\n'part_number': 20\n'total_parts': 21\n'poc_summary': 'This seminar lesson examines lessons from scaling continuous delivery at YouTube and Dropbox, focusing on developer experience and intent-based deployment models.'\n'chunk_summary': 'It notes the influence of promise theory and Nagios on modern deployment systems and highlights the need for simplicity.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': '31a17545468e0020a46cfb6e'\n'chunk_id': '687a291c96f7bf47ed452a3b'\n'access': 'granted'\n'part_number': 17\n'total_parts': 39\n'poc_summary': 'This seminar lesson provides a hands-on guide to integrating OpenAI's language models into Node.js applications, covering practical AI-powered development techniques.'\n'chunk_summary': 'It explains semantic search, vector embeddings, and the use of embedding models to represent and compare candidate data.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'fb78cf2215e99d64eba7d65c'\n'chunk_id': '687a3263ca798166d3ba5078'\n'access': 'granted'\n'part_number': 14\n'total_parts': 26\n'poc_summary': 'This seminar lesson introduces Entwickler Intelligence, a RAG-based AI tool for accessing curated developer knowledge and supporting continuous learning.'\n'chunk_summary': 'It explains access to both current and expired content, with references to new or related resources when needed.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'fd1e5ad8ac352c46d9f10793'\n'chunk_id': '687a32e1ca798166d3ba559c'\n'access': 'granted'\n'part_number': 18\n'total_parts': 19\n'poc_summary': 'This seminar lesson examines Kubernetes autoscaling, right-sizing, and resource optimization, highlighting practical challenges and strategies for efficient cluster management.'\n'chunk_summary': 'It introduces a Kubernetes Optimization Essentials certification and highlights smarter alternatives to VPA for resource management.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': '3d8d98464886dd773ef55f1d'\n'chunk_id': '687a292496f7bf47ed452aba'\n'access': 'granted'\n'part_number': 22\n'total_parts': 22\n'poc_summary': 'This seminar lesson examines how secure-by-design principles and robust API specifications can prevent common vulnerabilities and improve API reliability.'\n'chunk_summary': 'It ends with a book raffle and a lighthearted question about HTTP status codes to engage the audience.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': '37612953b51a3467652b95f7'\n'chunk_id': '687a2c88495b49563aa88015'\n'access': 'granted'\n'part_number': 13\n'total_parts': 15\n'poc_summary': 'This seminar lesson guides participants through building a simple GPT model, emphasizing hands-on learning and practical understanding of transformer architectures.'\n'chunk_summary': 'It provides practical tips for running the code locally, including hardware recommendations and context size considerations.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'M8Xgm6aQkhq4R8ETG'\n'chunk_id': '687ac1f3c902463ce5f2a716'\n'access': 'granted'\n'part_number': 3\n'total_parts': 15\n'poc_summary': 'This conference talk argues that software teams, not deliverables, are the true product and shows how fostering stable, learning teams boosts long-term value and agility.'\n'chunk_summary': 'It mentions a self-service platform engineering approach built on microservices, citing Netflix, Spotify, and Prime.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'xYCzZF3b8j6hpwQax'\n'chunk_id': '687b44842b0a1813a73e5503'\n'access': 'granted'\n'part_number': 2\n'total_parts': 66\n'poc_summary': 'This course teaches managing the machine learning lifecycle using MLflow for experiment tracking and Apache Airflow for workflow orchestration.'\n'chunk_summary': ''\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'hviTENDk754oqWbwy'\n'chunk_id': '687ad43eeb1b160a4cabc8b2'\n'access': 'granted'\n'part_number': 10\n'total_parts': 10\n'poc_summary': 'This article explores ethical dilemmas and regulatory challenges for AI and LLMs in software development under the emerging EU AI Act.'\n'chunk_summary': 'It previews a focus on AI data protection, emphasizing balanced regulation and innovation dialogue.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 's8SfLEXT2rkLAFpEW'\n'chunk_id': '687b42312b0a1813a73e3cf4'\n'access': 'granted'\n'part_number': 14\n'total_parts': 21\n'poc_summary': 'This tutorial lesson demonstrates how to achieve elastic scaling in Kubernetes using built-in autoscalers (HPA, VPA), serverless add-ons (Knative), and KEDA, plus cluster autoscaling.'\n'chunk_summary': 'It demonstrates using the ‘kn’ CLI to deploy a Knative service with a concurrency limit.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': '4iNg3SLaE3sj8Tgo3'\n'chunk_id': '687a2c19495b49563aa87c57'\n'access': 'granted'\n'part_number': 4\n'total_parts': 16\n'poc_summary': 'This seminar lesson examines how Domain-Driven Design and related practices address sociotechnical complexity in modern software architecture.'\n'chunk_summary': 'It discusses the importance of systems leadership and the visible versus invisible aspects of consulting work.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'T4tdNfcG52g3CDa2q'\n'chunk_id': '687a2de89bbcb45d2a11f0f3'\n'access': 'granted'\n'part_number': 13\n'total_parts': 13\n'poc_summary': 'This seminar lesson examines strategic API design, emphasizing business objectives, developer experience, and lifecycle management for sustainable API success.'\n'chunk_summary': 'It provides references to further reading and real-world API examples such as Google Maps and eBay.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n'documentId': 'b5b4b830ce2ccbe92815c1e8'\n'chunk_id': '687a48656b250430a4783aee'\n'access': 'granted'\n'part_number': 13\n'total_parts': 13\n'poc_summary': 'This seminar lesson examines how design patterns guide effective API design, focusing on endpoint roles, message structures, and strategies for API evolution.'\n'chunk_summary': 'It encourages using a shared pattern language and highlights resources for further exploration of API patterns.'\n'access_message': '✅ This content is included with your elevate membership.'\n\n\nTop Atlas:\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d2ef5cff79d6f5cd90'\n'access': 'granted'\n'part_number': 40\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': yeah, put them. So for this LLM I'll create a function, Uh, here. So, and then instead of printing, I just return it. So this is our LLM and this will be our build prompt function that I'll put here, build prompt, and then we have query and um, context documents, right? So we have context, we have prompt, and we return it. So, and then I'll remove this one. Okay? Just everything. I want to have everything in one place. So everything is related to building pros here. Then we have the l, m and the rock. So what should I use here? Can I attend the course that started? Can I attend the course that started a few days ago? No, you can, any other queries? Uh, let's try, how can I install Kafka? How can I install Kafka? I don't know actually if we have an answer to that question. Um, let me check. Kafka. Yeah, we'll have we have some, well we have 74 questions with Kafka. Yeah, so probably we do not have that, the answer to that question in the context. Let me print it so it's easier to read, right? So it says that it looks like there's no answer to that question, but here's how you can do it, right? So it's very nice of, uh, this LLM to tell us about that, but it might not tell us and just strictly go to to that part. And then we have no idea if this is coming from the context or is just coming up with answer yourself. So in this case, it looks correct. Well, maybe it's not, but like it looks legit. Uh, but we can also tweak out prompt to say like, um, let's try, if the provided context doesn't contain the answer, say I don't know, right? So we can try that and see what happens. Yeah, it's good. Is there any other way much confidence I cannot hear. Can you please? Confidence level Is, were there any way to add confidence level? Not that I know of here, right? So when you use open source cell lamps, usually somehow you can get the confidence levels. Um, but yeah, I dunno, they Put confidence on the query itself. Yeah, probably. Do you know how to do that? I guess it's the elastic search feature, Right? Ah, okay. So for those who are listening online, um, the comment was that you can put the confidence to the prompt, right? In indeed elastic search returns to the query, uh, score. So here we don't do this. Um, but you can say also you can\n'slidetext': ''\n'from': 'index'\n'score': 18.13646125793457\n'normalizedScore': 0.1813646125793457\n'beforeDatePenaltyScore': 0.1813646125793457\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd6b'\n'access': 'granted'\n'part_number': 3\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': a data set. I'll quickly show you this data set. So I said, I told you already that this was a part, this is a part of the course. And in Datadog Club, this is the com uh, a community that I'm running. We have many free courses. And usually when there is a free course, there are many participants of this course. And there are many, many, many questions. And in our community we have, for our courses, we have these documents with frequently asked questions. So we usually ask people to check this document. So when they have a question, they first check the document. And if they don't see an answer to their question, then they ask this question in Slack. And this document, uh, this is for our data engineering course. This document is 326 pages long, right? So this is how it looks like. So there are all sorts of errors or all sorts of, um, issues people have. And um, yeah, there are answers to these questions too. So this is community maintained. So students, when they come across a problem, they go there. If they don't see the answer, they figure this out and put it there. But you can imagine that this is very difficult to actually find anything in this document, like the more than 300 pages. And what we're going to do today is we're going to take these documents, there are three of them, Oops. So for our data engineering course, for our machine learning course, and for our ML lops course, all documents look similar. And we want to build a bot, a q and a system that given a question from a student will answer based on the content of these documents. And, and if we talk about a usual LLM, so we know that l lms, they are good at answering questions. And let's say this is a student, so a student wants to ask. Um, so a very common question is, I just discovered the course. Is it too late for me to join? So the course for example, our LLM course started last week. Somebody like, maybe somebody of you does not know about the course, they find out about this and they ask this question, okay, I just found the course. This is very interesting. I want to join. And if the we ask this to an LM, maybe I can quickly show you what happens. So I will be using charge GPD. So I just discovered the course. How many does anyone else not have an account in charge G pt, why you're missing out. Uh, can I still join? Right? So we ask alem question\n'slidetext': ''\n'from': 'index'\n'score': 16.516693115234375\n'normalizedScore': 0.16516693115234374\n'beforeDatePenaltyScore': 0.16516693115234374\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd7b'\n'access': 'granted'\n'part_number': 19\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': contains all these, all these documents that I showed you previously. So, and for example, if I want to see the first, For the first course for the data engineering course, uh, the documents, the first document, This how it looks like, you know, um, the structure here is, um, not the most convenient. So we have course, so it's a bit nested. So we have course, and then inside the course we have uh hmm questions. So we want to un nested. So we want to move this course part here. Uh, I cannot do it, but like we want to, for each document, we want to know which course it belongs to. So then we have a simpler document. Instead of having this nested structure, we'll just have a bunch of document with this field course inside. So we can do it, I can simply go over all the documents, I'll call it documents. I'll go for all the courses course in and like I'm probably typing very fast. There is actually just this bit of code, this piece of code that you can copy and execute that will do what I'm doing. But I just want to type it. So I'll do for course in all the documents. Then I want to go over each document in course documents do, let me just copy it. And then I'll have an extra field here. Coming from course, what's the name? Course? Yeah, course, course. And then I forgot to append documents. Append look. So now we have, um, like almost 1000 documents. And then if we look at each of the document, it looks like that. So we have course which course it belongs to. Then which section of the FAQ, what is the question and what is the answer, right? And then for example, this is like, can I still join the course after it started? Um, this the answer yes, even if you don't register. Um, so now we have all the questions and we want to put them in Elasticsearch. So I already like, let me just type it one more time. Import Elasticsearch. Now we want to create an Elasticsearch client. I think I should do, uh, from Elasticsearch import. What do we have here? Elasticsearch and I call client Elasticsearch. And here hosts, uh, we have HTDP, local host 92 0 0. And um, of course if you, uh, use, uh, a cloud version of Elasticsearch, then you can just put the cloud version UL the UL of your cloud version. And here we can do Elasticsearch info to check that it can actually connect to, to the server. So what I'm doing\n'slidetext': ''\n'from': 'index'\n'score': 16.187057495117188\n'normalizedScore': 0.16187057495117188\n'beforeDatePenaltyScore': 0.16187057495117188\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd82'\n'access': 'granted'\n'part_number': 26\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': Just keep the, that part, the source. So for that, I'll call it, hmm, how do I call it here? Yeah, I'll, I'll just copy this one, right? So I call it response. So basically we iterate over the, the results and display them. Yes. Client. Yeah. So this is a more readable version of what, what I just showed you, right? So we need to go to hits, then to hits again. And then the document itself is in the source field. And this is how we can show it, right? So these are the top, uh, the most relevant questions from our database. And yeah. Now what we can do is we can just put this into a separate function. I call this code to make it a little bit cleaner. So I'll do it right now. So let's have, I can call it, um, I already have documents, um, uh, relevant docs, I'll call it relevant docs, right? And then I'll just append these relevant docs And we don't need to display it. And then the relevant docs contains the, the results. This is something we want to now, so what we did not right now is we have our query, we send this query to Elasticsearch, Elasticsearch gives back these documents, right? So we almost finished the first step of our rack. So now we just want to clean it a little bit. 'cause it's a messy Jupyter notebook. What we can do right now is just put it in a function that in one function, it takes the user query, goes to database, returns the results, right? And then later we'll use it for building the prompt. Do you have any questions? Yeah, The filter. Can you say it again please? The filter, the filter functionality. So is it somehow can need to select beforehand because it's not part of the, Um, yeah, so the question is where the filter comes from. So imagine a scenario that a student is taking a particular course, right? So when they ask questions about the course, they want to get answers about this particular course, not about all the courses, right? So this is like kind of, um, use case, specific filter, let's say. 'cause um, like what happens is in practice we have a Slack channel, and for each course there is, um, a separate Slack channel, right? So Slack workplace, and then like for each course there is a channel, and then there is a bot that answers questions from the students. And if we know that if a question is coming from this channel, we want to filter only by this\n'slidetext': ''\n'from': 'index'\n'score': 15.815485954284668\n'normalizedScore': 0.1581548595428467\n'beforeDatePenaltyScore': 0.1581548595428467\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd7c'\n'access': 'granted'\n'part_number': 20\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': uh, a cloud version of Elasticsearch, then you can just put the cloud version UL the UL of your cloud version. And here we can do Elasticsearch info to check that it can actually connect to, to the server. So what I'm doing right now is this part. Yeah. So now we can connect to Elasticsearch and we will create now an index in elastic search. So an index in elastic search is similar to a table in a usual relational database. So in a, let's say in a usual database, if we use Postgres or MySQL, we would have a table documents here we would have, we will create an index called documents or like, I don't remember how we'll call it, but we can do it. So there is uh, uh, this indices and we create can create an index and index name, I think I call it course questions, right? Index name would be course questions. And then we need to describe what exactly is there in this table. Like when we create a schema in the relational database, we say what type, each column is here, we need to do something similar here. We need to tell, like let me show. So we have four fields here, like four in relational database. It would be four columns, right? And for this think what we want to do for the course field is we want to filter on this. So it would be similar to uh, select where, uh, where course equals to data engineers on camp. I think it's select from, right, from blah, blah blah where course equals to data engineering course. So we want to be able to filter on this field. So for example, if a student wants to ask questions about this data engineer zoom camp, we don't want to show them answers from a different course, right? So we want to only stick ourselves to this particular course. So for that, in Elasticsearch we have a special type called keyword. Usually we want to have an exact match there, which is similar to doing this kind of query. And for these three fields, we want to have, um, we want to perform text search over these fields, which means that we want to see, like as I explained there when I was talking about the difference between text search and vector search, we want to, um, break this text into words and then do kind of word mark more match than when I have a question. Can I still join the course after the start date? I want to match the words, join course start date, these things, right? So for that I use the text, uh, uh, type. And then this is, uh,\n'slidetext': ''\n'from': 'index'\n'score': 14.959874153137207\n'normalizedScore': 0.14959874153137206\n'beforeDatePenaltyScore': 0.14959874153137206\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d9ef5cff79d6f5cdc4'\n'access': 'granted'\n'part_number': 92\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': part in installation from the rest and see what happens. And I, I think that's all for me. So thanks a lot for attention. I know it was fast sometimes, uh, but hopefully you can come back to the notes. Also. Let me um, mention it one more time. So right now there is a course about that. So this thing that I described here that I explained. So this was module one of the course and module two. So the course is called LLM Zoom camp. And this course just started. So if you want to learn more about vector search or how you orchestrate track or how you do monitoring, what are the best practices? The course just started. You can join it. And also if you later have questions about this workshop, this particular workshop, you can just join the course and ask these questions in the course channels. 'cause the content we have here in the first and second one and what they presented here are the same. So there are students who are doing this. There are um, instructors. I also monitor, uh, this channel. So if you have any questions about this workshop, maybe you will have later this week or this month or next year. I dunno. You can always come to our slack and ask questions about this. And in order to do that, uh, you, yeah, you can sign up to the slack and then you join the course channel. And then if you have any questions, this is the best place to get answers. Yep. Light lectures or It's prerecorded. So for example, everything I showed here is actually split into module. Yeah. This, this talk here is recorded, right? That we can Have access to. Uh, that's not a question for me. I don't know. So maybe you know if this is recorded or not. Sorry, the question is if this content is recorded? Yes. Is recorded Website. Okay, that's good that I deleted my hugging face. Um, talking. Yeah. 'cause I de deactivated it. So if you watch this in recording, it no longer works. Okay, well I guess that's it. Thanks for attention or more. Yeah, so we still have time to um, it's supposed to finish at five, right? So we still have roughly one hour. So I can stick around if you need help. Yeah. Yes. Do You have some agents? Agents not in the course and not here. So an agent is a thing that can do actions. So for example, you can, so in addition to that, mm. And so let's say you want to build an agent that traits on\n'slidetext': ''\n'from': 'index'\n'score': 14.947824478149414\n'normalizedScore': 0.14947824478149413\n'beforeDatePenaltyScore': 0.14947824478149413\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd6c'\n'access': 'granted'\n'part_number': 4\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': happens. So I will be using charge GPD. So I just discovered the course. How many does anyone else not have an account in charge G pt, why you're missing out. Uh, can I still join? Right? So we ask alem question and then it just gives a generic answer, right? So it has no idea which course we're talking about. So it tries its best to give a question, to give an answer to this question. And this might not be very useful to you 'cause you want to know if you can join the course, right? What we need to do is to give it some context. So we need to say, okay, I just discovered the course, can I still join? And there are some entities from this FAQ document. So for example, we can take this document And um, let me just copy this and I can say I will edit it. Um, we can say your, uh, course teaching assistant and your task is answering questions from the students based on the provided context, context. Give answer to the student question. And then I say, okay, this is question, this is context. And then yeah, maybe that's enough. Let's see what happens. Yeah. So here, because we give the context and the contents context, it's actually based on the course document on the, on the course specific question. Then based on the context it says yes, you can still join the course even after the start date. So it actually knows the contents, the context, and based on the context it gives an answer. So how do we use it? So we have this document and of course we cannot expect students to copy paste entries in the FAQ document. They might as well just read the answer there, right? So what we can do is we can take all these documents and put them in some sort of knowledge base, or this is already a knowledge base, this FAQ document. But we can put it in a database in such a way that we can easily retrieve Questions and answers from there. And when a student has a question, first we go to our knowledge base with this question, we retrieve the most relevant, uh, candidate answers from this knowledge base. So let's say we have a bunch of documents, document one, document five, let's say five documents. And then based on these documents, we create a prompt that we sent to LNLM. And this is what I did here with charge GPD. So I took a question, I took some context, I created a prompt, I sent it to NLM and then LLM gave the answer. So we have two steps here, two\n'slidetext': ''\n'from': 'index'\n'score': 14.929349899291992\n'normalizedScore': 0.1492934989929199\n'beforeDatePenaltyScore': 0.1492934989929199\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a315fca798166d3ba45be'\n'access': 'granted'\n'part_number': 40\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': uh, we'll call this function LLM. And the LLM gets in the prompt and it did, does some response, right? So we already have this function. We need to implement this too, right? And this is, we just need to take the goal we already have. And yeah, put them. So for this LLM I'll create a function, Uh, here. So, and then instead of printing, I just return it. So this is our LLM and this will be our build prompt function that I'll put here, build prompt, and then we have query and um, context documents, right? So we have context, we have prompt, and we return it. So, and then I'll remove this one. Okay? Just everything. I want to have everything in one place. So everything is related to building pros here. Then we have the l, m and the rock. So what should I use here? Can I attend the course that started? Can I attend the course that started a few days ago? No, you can, any other queries? Uh, let's try, how can I install Kafka? How can I install Kafka? I don't know actually if we have an answer to that question. Um, let me check. Kafka. Yeah, we'll have we have some, well we have 74 questions with Kafka. Yeah, so probably we do not have that, the answer to that question in the context. Let me print it so it's easier to read, right? So it says that it looks like there's no answer to that question, but here's how you can do it, right? So it's very nice of, uh, this LLM to tell us about that, but it might not tell us and just strictly go to to that part. And then we have no idea if this is coming from the context or is just coming up with answer yourself. So in this case, it looks correct. Well, maybe it's not, but like it looks legit. Uh, but we can also tweak out prompt to say like, um, let's try, if the provided context doesn't contain the answer, say I don't know, right? So we can try that and see what happens. Yeah, it's good. Is there any other way much confidence I cannot hear. Can you please? Confidence level Is, were there any way to add confidence level? Not that I know of here, right? So when you use open source cell lamps, usually somehow you can get the confidence levels. Um, but yeah, I dunno, they Put confidence on the query itself. Yeah, probably. Do you know how to do that? I guess it's the elastic search feature, Right?\n'slidetext': ''\n'from': 'index'\n'score': 14.880789756774902\n'normalizedScore': 0.14880789756774904\n'beforeDatePenaltyScore': 0.14880789756774904\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd7d'\n'access': 'granted'\n'part_number': 21\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': I have a question. Can I still join the course after the start date? I want to match the words, join course start date, these things, right? So for that I use the text, uh, uh, type. And then this is, uh, a bit too much to type. So we'll just copy it from here And put it here. So I just explained this that we have multiple types. So for text, for section for question, the type is text because we want to perform text search and then which means we want to break the text into distinct words. And then when we do, uh, when we perform search, we want to do keyword matches, right? And of course is we want to match the exact we want to filter to be able to filter on this. And yeah, so this is not important for us. This is important when you have um, a cluster, Elasticsearch cluster with multiple instances. So you want to say like how many, like how exactly it's distributed across multiple instances. So in our case, we have only one instance of Elasticsearch. So we just use these settings. And of course when you use it in production and you have let's say three instances, you want to read more about this setting before you create an index. And I think I named it as client, right? So now what happened is we created this index and this is the name of this index, and we can now index or go over all our documents and put them in this index. Um, so save them to our Elasticsearch in the database. So I'll do this for documents, uh, in documents, Then Elasticsearch client index. And then we need to specify the name of the index, which is these course questions. And I think we do, what do I have here? Document? Yeah, so this is this field. The second document equals to document. So we simply go over all the documents we have, and here I'll use a progress bar that we installed, TQDM, um, and then I'll just do something like this. So we then we can monitor the progress of, uh, indexing. And now our documents are indexed. I see a question in chat. Uh, can you please show again what you entered in Jupiter from the beginning? So I want, I will show you where you can find it. So everything I do, well of course you just can follow this, um, this document and everything I type in Jupiter is in this document, but also like if you go to notebooks, in notebooks, you have this elastic search i, this elastic search notebook and it contains everything a like this is\n'slidetext': ''\n'from': 'index'\n'score': 14.811952590942383\n'normalizedScore': 0.14811952590942382\n'beforeDatePenaltyScore': 0.14811952590942382\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd6a'\n'access': 'granted'\n'part_number': 2\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': oh second repo should be this one. And then I see somebody typed it in the chat. So if you also have the chat, you can see there. So was everyone able to open this page? Good. So we're almost done for the day. Okay. So this workshop is actually based on a free course that just started recently last week. So this course is called LLM Zoom Camp. It's a free course. And if somebody attended my workshop last year, it was something similar. So it's like kind of, sort of the first module, first two models of the course. So if you like the content today, you can also check the course. It's totally free. And if you also like the workshop and if at your company you want to have a similar workshop, there is my email that you contact me. So yeah, we can stay in touch. And who of you already have heard Rock? So this thing, what is it? And can you show, okay, why are you here? Okay, I'll still, hmm. 'cause I don't know to what extent you know it, so maybe you know what it stands for. So it's the retrieval augmented generation. I want to give a brief overview of what it is, and maybe you already know what it is, but it'll be a refresher. Um, since you know what track is, I assume you know what an LOM is, right? So it's a language model, it's large language model. And what languages, language models do usually is auto completion. So everyone has a smartphone. And if you chat with your friend and you start typing, how are usually it suggests how are you, right? So, and this is a language model that we use in our smartphones. And large language model is still a language model. So the task of a language model is out, is completion. So you have something and the task is to complete what you wrote. And because it has so many parameters, it is very smart, it can complete really well. So it feels like you're talking to an intelligent being. So usually the way it works, You have an LLM, you send a prompt and it gives back the answer. And uh, yeah, usually like, what's the day the Germany was united, right? And then it can answer when was it? And for the day workshop, um, uh, we prepared a data set. I'll quickly show you this data set. So I said, I told you already that this was a part, this is a part of the course. And in Datadog Club, this is the com uh, a community that I'm\n'slidetext': ''\n'from': 'index'\n'score': 14.811952590942383\n'normalizedScore': 0.14811952590942382\n'beforeDatePenaltyScore': 0.14811952590942382\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d1ef5cff79d6f5cd87'\n'access': 'granted'\n'part_number': 31\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': of this framework is you can replace it with anything you want. Like you build a prototype with Elasticsearch and then you tweak it, you see, okay, I don't like it. And then you can easily replace this part with something else and see, okay, with Vector database it works better. Or what if I used to, right? And this part stays unchanged. So these two parts are kind of isolated, independent from each other. And then you can plug and play. Yeah. So what the ask question where we, we'll still get like top five answers. Mm-Hmm, probably. So how, how would resolve this kind of problem? So the question is, if there is no answer in the database, uh, what happens, right? So then you can, um, like if Elasticsearch doesn't, uh, return anything, right? You can, you can just stop here, right? So it doesn't return anything you tell the user. I don't know. Like for example, what we can do in our courses, we can say, I don't know the answer to this question, ask the course instructors, right? So like, there, there, there is no answers to that. That's one possible thing. Sometimes what can happen is also the Elasticsearch or like the knowledge base returns something, but it's not that relevant, right? So it returns like five documents, but none of these documents actually contain the answer. Then you can build it in the prompt. You can say, answer the question based on the context, but if the constant context does not contain the, the answer to this question return, I don't know. Right? And then the LLM would look at this, it'll see that, um, the context, context is actually not relevant and it will say, I dunno, sometimes it doesn't always work. 'cause L lamps are yeah. Identify content Relevant. Well it's up to the LLM to identify, right? Yeah. It's magic. So you just, uh, keep your fingers crossed and then it either identifies as it's relevant, like, um, with the lamps you have this problem of hallucination, right? Probably everyone has heard about that. And then sometimes it may output some nonsense. It happens. You just have to leave with that, I guess, or have some guide rails. You're getting a score back and yeah, well, yes, you can filter by the score, but like maybe like it still doesn't work. It still returns something and then LLM does not thinks that it contains the answer. Sometimes even if you tell it, if there is no answer to this question, it might just make up an answer.\n'slidetext': ''\n'from': 'index'\n'score': 14.75055980682373\n'normalizedScore': 0.1475055980682373\n'beforeDatePenaltyScore': 0.1475055980682373\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d3ef5cff79d6f5cd9a'\n'access': 'granted'\n'part_number': 50\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': later, uh, with the actual thing. Uh, then there is a text input for the query. There is a button and we have, have some results. So let me start it. Stream lead. What is the extremely trap run? Okay, so we open this Euro And, Uh, yeah, how do I run a smartphone? Then of course this thing, like right now is just a dummy. It doesn't, it just returns this result for query. Now we want to replace it with from rack import, how do you call it? Rack? Well, probably that's all I need to do now. So let me refresh it. Query is not defined. Um, yeah, so we need elastic search query search. Hmm. Yeah, actually, like I was, uh, showing you like there was an error here. So search query contains this query and I should have put it in the retrieve part. So like that. So let me upgrade it. Otherwise we query every time with the same query. Okay, let's see if it works. Um, I just discovered the course. Can I still join it? No. Cool. So that was pretty fast. Like I, I don't, I save so much time with Charge g pt, like, I don't know. I use it all the time. Like for example, recently I created a T net application without knowing C at all. And then it just told me what, I told it what I want and then it told me what to do and then I execute and then I give it feedback and then it corrects. It's amazing. Like I, I have it here. Uh, this thing in C Sharp. Yeah. That's awesome. Anyways, so I think this is it for the first part. So we have done, um, quite a lot today already. So again, just to summarize, we have a que a student, a student has a question about the course and then, uh, we indexed our FAQ database. We put all the data from our um, Google documents to our knowledge base. We use Elastic Search for that. We can query elastic search and get back the, the documents to put them in the context right then. Um, yeah, we build a prompt based on these documents and get back the answer. And actually when you were asking to try a different query, uh, because I had a problem, um, in my code, it was actually using the old query. Now we can try with something else. Um, do you remember what the query was? Yeah. Um, can I attend the course that started the days ago? Can I attend the course? That started a few days ago, so hopefully it\n'slidetext': ''\n'from': 'index'\n'score': 14.709589958190918\n'normalizedScore': 0.1470958995819092\n'beforeDatePenaltyScore': 0.1470958995819092\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd80'\n'access': 'granted'\n'part_number': 24\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': So must is it must match and should it should match, right? So if it doesn't match, okay, uh, it can still show you and must is like, there should be, there must be a match, right? Then like if there is no word course in the document, it'll not display it, right? For example. Or the, if none of the words are there. So you can like, it gives you quite a lot of flexibility. I will not talk again into, I will not go into details of building elastic search queries. I just want to show you the simplest way of building the query and then you can learn more about that later. Okay? Then we have multiple fields. I will show the document one more time. So let me put it closer so I don't have to scroll up and down, right? So in this document we have four fields. First field is course we want to filter on this. So we have this part. So this is the same as doing the wear. It's like similar to the wear clothes. So we will only consider documents for this course, we'll not consider the rest, right? Then there is other part, this multim match. So what we want to do with this multim match is we have multiple fields, we have section, we have question and we have text. And all of them are potentially relevant. All of them may potentially have a match against our query. So we want to consider all of them. That's why we use multi match, right? Then we put our actual query. So this is, I just discovered the course, can I still join? And then we want to see against which fields we want fields we want to match this query, which are three fields, question, text and section. And you know, you can notice this little hat. So this hat three means that the question field is three times more important than the other fields. Because like if we have a question from the user and the question from the FAQ document matches, they match, then this is very good, right? 'cause we find the question from the user, we find the question from the database, they match ideal, we just show it, right? But it may often happens, um, that it doesn't quite much. So we want to look at other fields too. Like maybe some of the words are in the answer. So we don't want to completely disregard text. And maybe some of the words are in the section field too, right? So we want to combine all of them, but we want to give more importance to the question field. So that's why we have this hat\n'slidetext': ''\n'from': 'index'\n'score': 14.66631031036377\n'normalizedScore': 0.1466631031036377\n'beforeDatePenaltyScore': 0.1466631031036377\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d4ef5cff79d6f5cda7'\n'access': 'granted'\n'part_number': 63\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': it sounds, it gives us the whole thing. Think if you do it locally, it's probably faster. Well at least it's doing something. Um, I wanted to to look at HO how just create a new terminal. Yeah, well it's apparently doing something and we can see that how much resources it uses. Actually it seems to be using only two CPOs. Probably there is a way to configure it to use more. Yeah, I don't know. Yeah, maybe let us replace it with um, a different, so this model is not as smart as um, charge GPT for SGPT for all of course. 'cause it doesn't have that many parameters. So maybe it needs to have a, like a better, like I just, I using the Same. Okay, good. It's Okay. And uh, do you use the same prompt? Like here, uh, I Just say I, I just found out about the course. Can I still join Uhhuh? Okay, let me, I just found out about the course, can I still join? 'cause my query was a little bit different. So with this model, you probably want to give it a smaller prompt. Um, yeah, so like, uh, in this example maybe you can tweak your prompt to something like that. Um, but also this one should work like depending on how much time we want to spend. And like, to me, the use case for this thing is mostly testing things locally. So maybe somebody is actually using this in production. Um, but for me this is just, okay, I want to have a basic crack thing working locally and it's okay for me to wait a little bit. But this is not something you would put in front of a user. I think of course you can put it to Kubernetes, scale it up, but still, like there is waiting time. So probably you want to run it on A GPU and until LAMA supports the GPU, um, and for some reasons, yeah, it doesn't want to give me the answer for you. It works for me, it doesn't. Um, okay, so I don't know what, what is different between what I have and what you have. But yeah, it should work. Like when I was testing this locally before this workshop, it also worked. Um, um, maybe if I tweak it a little bit, um, like that. So what I would do in this case, like if I wanted to make it work, I would play with the prompt a little bit. I would play with the number of, um, documents I include in the context. And yeah, it's definitely possible to make it work.\n'slidetext': ''\n'from': 'index'\n'score': 14.6338529586792\n'normalizedScore': 0.146338529586792\n'beforeDatePenaltyScore': 0.146338529586792\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd79'\n'access': 'granted'\n'part_number': 17\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': So a few people told me that they have problem with Elasticsearch. So when you start Elasticsearch it starts and then it says Elasticsearch has quit unexpectedly. So if you have this problem, what do you need do is, um, give it more memory. And for that you I need to add one more parameter here. Uh, let me just edit it here. So you, you have it in the instructions if you want to follow this, um, later. So if you get elastic search qui unex unexpectedly and give it more rum, and you do that by um, using this minus m command or instruction and then you can give it two gigs like that. So if you have that this problem, you give it more. Um, at least two people told me they have this problem. Okay? So we have prepared the environment. Hopefully, uh, most of you did that. Um, there are still some problems, but um, yeah, uh, in case you still do not, uh, manage to set up the environment, we can talk again on the next break. Um, but now what we do is we have elastic search. Like let me use this thing. So our elastic search is running and now we need to put documents here. So this is a database and we want to store our documents. And remember I showed you these FAQ files. So we need to put the content from these files to the engine to the database so that we can later retrieve them. So this is what we're going to do now. And um, yeah, I, in case it's interesting for you. So I have already passed the files, so I will show you how the files look like, uh, or not the files, like how the, um, it's already passed and it's extracted. So this is this document and this is how it looked like, looks like in a past form, right? So we have, uh, the course and then we have documents related to the course. And then we have uh, section. So section is, uh, this part General course related questions. Then this is the question and this is the answer. So in this case, yeah, section question and text is answer. Um, you know, so yeah, basically this is an extracted thing. So I do not want to spend time, uh, explaining how to extract this. This is quite interesting, but we will just spend too much time on that. If you want to know. There is, um, this, uh, file par FAQ and it uses a doc X library. So I save these, uh, documents as a doc X file. And then I use a library from Python\n'slidetext': ''\n'from': 'index'\n'score': 14.518350601196289\n'normalizedScore': 0.1451835060119629\n'beforeDatePenaltyScore': 0.1451835060119629\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd83'\n'access': 'granted'\n'part_number': 27\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': So Slack workplace, and then like for each course there is a channel, and then there is a bot that answers questions from the students. And if we know that if a question is coming from this channel, we want to filter only by this course and so on, right? So it's not in the query. Uh, this is, we preset it. There are other questions. Only works on any equal conditions. So There's no Yeah, matching. There is no matching. Exactly. So this is exactly like the work causing in sql. Yes. Say case insensitive. Case insensitive. Yeah. So I think by default it's case insensitive. Um, but yeah, you can, like in Elasticsearch there is this concept of analyzer. So you have this, uh, query, right? So this is a string, and now you need to break it into words, into tokens, right? So when you do this, you get something like, like it's a list. I just, uh, we had, I just discovered. Then Elasticsearch would probably turn it into discover. Um, then it would drop the 'cause like it's, it doesn't really care carry any, like it doesn't have any meaning, right? So we drop the course, um, then, I don't know, maybe it'll also drop. Can, uh, I still join, right? So this is, um, and this process of turning the text into tokens is called, um, so now it's called tokenization in Elasticsearch, it's done by a thing called analyzer. So analyzer kind of analyzes the text and turns it into tokens, and then you can use different analyzers. So the default analyzer, um, is doing this thing. And the default analyzer actually, like here, it'll turn I into a lowercase I, right? So it'll lowercase all the words. Uh, and if you need a specific analyzer, you can check the analyzers that already exist. And if in your case you need to be case sensitive, then you can use a different analyzer that doesn't do that. So this is instead of using a, Yeah, so like I showed here, so this is an embedding what I call vector. So this is embedding, but we follow this path. Follow this path today. Yes. Well, in my experience, this actually works better. Um, vector database, they want to sell you their products. That's why most of the content you see on the internet these days by the vendors, they're about vector databases. Better If it's very structured document. Yeah, yeah. Pretext. Mm-Hmm. Would be hard to No, no, no. Like Elasticsearch is a, a search engine, right?\n'slidetext': ''\n'from': 'index'\n'score': 14.480815887451172\n'normalizedScore': 0.1448081588745117\n'beforeDatePenaltyScore': 0.1448081588745117\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd81'\n'access': 'granted'\n'part_number': 25\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': to completely disregard text. And maybe some of the words are in the section field too, right? So we want to combine all of them, but we want to give more importance to the question field. So that's why we have this hat in Elasticsearch, it's called Boost. So we boost the question field, we give it more importance, right? And best fields is like how exactly you, you have a score, you have a matching score from this one, you have a matching score from this one, from this one. How exactly you combine it. So best fields say that you take the highest score, right? But like you can also read about that. Like Elasticsearch has a great, like, excellent documentation. You can check check it. But uh, anyways, so this is the query. So this query will perform search in, uh, all the fields. It'll filter by the course field and it'll show some results. Then it'll score each document against the query using this formula. And it'll sort all the documents, uh, showing the highest score first and so on, right? And then I will execute it. So this is not what I want to see. Uh, yeah, there's a bit of explanation here. But we want to use this ESOPs ES client dot search function. Then we specify the index where we search index would be index name, and then I think it's body, right? Lemme check. Yeah, it's body. Um, so this is the body of the search query. And then it gives us some results. I'll say the results to results. Um, so these are the results. And like in this results, we are interested in this hits part, right? So these are the things that Elasticsearch was able to retrieve, um, find in our knowledge base and find them. Yeah. And we see that, uh, yeah, here we have these documents, right? So this is the first document with the highest score, then the second highest and so on. And actually, this document contains the answer to our question. So the question was, um, I just discovered the course, can I still join and can I still join the course after the start date, right? So this contains the answer to our question. Um, so it's a bit messy. So what I want to do now is clean it a little bit. So remove all this index ID score. Just keep the, that part, the source. So for that, I'll call it, hmm, how do I call it here? Yeah, I'll, I'll just copy this one, right? So I call it response. So basically we iterate over the,\n'slidetext': ''\n'from': 'index'\n'score': 14.480815887451172\n'normalizedScore': 0.1448081588745117\n'beforeDatePenaltyScore': 0.1448081588745117\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d3ef5cff79d6f5cd9b'\n'access': 'granted'\n'part_number': 51\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': the old query. Now we can try with something else. Um, do you remember what the query was? Yeah. Um, can I attend the course that started the days ago? Can I attend the course? That started a few days ago, so hopefully it works. Yeah, well, I don't know, maybe there is still another error, but it seems to work. Let me check the comments in the chat. As far as I recall, any API access to open AI requires prepaid credit free is only interactive usage. Okay. Maybe has anyone tried Grok or something else? How is it going with Grok? Yeah. Okay. How difficult was it to Very Easy. Very easy. Uhhuh. So like, uh, with Gro, do you, can you use OpenAI client or you need to install their own library? Uh, we have their own library. So you did keep install Grok? Right. Okay. And just Changed the import, change the client and then surprisingly their a p is almost the same as Open. Mm-Hmm. So don't Have to change much. Okay. I'm unsurprised. 'cause like, I guess they set the standard and everyone else copies that. Um, any more questions or We go have lunch now. So when do we come back? Like in one hour or like, I guess one hour is enough. Can you say it again? The program Is one 30. One 30. So one hour. One hour. Okay. So I'll just, I see that some of you still have questions, so if you don't, you can leave. We'll come back to in one hour, one 30 and if you have questions, I can answer them. Yeah. Is it something, a particular Question on, On Uhhuh? Okay. Does anyone have like generic general questions? Yeah. If you Have any insight as to why this works, uh, with you. Rephrase the Question, Uhhuh. 'cause Uh, is there cement similarity actually? Mm-Hmm. So the question is, uh, if I have any ideas why when I rephrased it still works. And I think like if we look at the documents that, um, we get, and let me check here, um, look out query, because I think it, uh, Elasticsearch can still retrieve relevant documents. Do I still have now here? I don't remember if I fix it because like it still has, uh, things like course and started. I think this is, uh, um, even though like you see that this is not the most relevant one, um, yeah. So this is like fourth most relevant one. It's still there. So for an LLM, it's uh, still enough?\n'slidetext': ''\n'from': 'index'\n'score': 14.444624900817871\n'normalizedScore': 0.1444462490081787\n'beforeDatePenaltyScore': 0.1444462490081787\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d9ef5cff79d6f5cdbe'\n'access': 'granted'\n'part_number': 86\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': else. Like I showed you everything I prepared. Um, like do you have any questions or, Sorry. Yeah. What was the last change That you made that the, the result, the problem, uh, the Context. I did not do anything. Okay. So the question was what the, was the change I did. Um, so here actually the prompt is a little bit different, uh, from here. Like it was not, I just found out about the course. I just discovered the course. I don't think it matters. It's just the nature of LLM. So maybe we can play with this temperature parameter to make it more consistent. But yeah, I did not do anything here. You have to place the combine the docker, um, in the please. So, um, I already uh, pushed it. You Pushed it? I pushed it. Yeah. Is it in, uh, is it here? There was one link before anyway. Yeah, I'm just curious. Like if I put it, it's Also in, It's also in the, yeah, so here. Docker gabo. Exactly. So it's a little bit different. 'cause here you don't have volume mapping for Elasticsearch. And maybe I will actually replace it now. 'cause here we have, um, we set the, the memory for Elasticsearch, which we not did not do. And we also set the volume. Um, so it's better if I just update it here. Okay. So I updated it in the main workshop. Um, yes. One more general question, um, sort of restricted ourself to the context of answering around the document itself. Would creasing the temperature now mean that it less or that it's more likely to answer in a more general way, but it might even ignore that context? So the question is if we increase the temperature, would it ignore context or not? Right? Sure. Summary. Um, I don't know if we can check. Uh, I think so when we increase temperature, what happens is usually it's more, um, how like it starts to improvise more. Like, uh, it tries uh, uh, like when it's slow, the answer will always be the same. When it's high, then it'll be different all the time, right? Um, I don't think it makes it ignore the content context. It'll just give different answers all the time in it feels this way. I have not tried it. So yeah, you can just experiment. So, and yeah, speaking of temperature, uh, it's not, um, so this is how you can do this. Um, where we have alama. So in alama we have this customizing the model.\n'slidetext': ''\n'from': 'index'\n'score': 14.27489948272705\n'normalizedScore': 0.14274899482727052\n'beforeDatePenaltyScore': 0.14274899482727052\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d4ef5cff79d6f5cda4'\n'access': 'granted'\n'part_number': 60\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': to think. Or for P three, I can also run it locally. So locally I have eight core, so it should be faster. Let's see. Okay, I need to install it. Um, yeah, but I can hear that my laptop is uh, working and then, yeah. Um, yeah, so here, apparently it's not that smart to figure this out. Uh, maybe we need to rephrase it. Um, that, that was The, uh, Yeah, that was exactly. So now if I, that was, I was going to say if, uh, if we rephrase it, let's say, um, I just discovered the course, can I still join it? So let's try to send this one and this one I'm running this locally I think actually. Yeah. So it took a bit more time for code spaces to reply. Mm, yeah. And yeah, right. The prompt is also like, it doesn't have the answer. That's why it does not answer. Let me try this one. So this is, I took it from GitHub. Uh, this prompt is a little bit different. So I have this question context, right? And then the answer, and also this prompt is shorter, right? It only has three documents in the context. So when I was preparing the workshop, uh, the reason I did it, uh, I just wanted to make the context shorter. So then we have the answers faster, right? Uh, 'cause the, the longer the prompt, the more time an LM need to process it, uh, and start output something. Um, yeah, so you can still enroll. So this is how you run it from the terminal. But of course, like this is not how we want to use it. We don't want to use it from the terminal. We want to use it from first our notebook and then from let's say the Streamlet app that we built, Uh, for that, uh, how do I close it? Okay. We need to do to install a few things. So, 'cause this is a fresh cut space. I need to install notebook, I need to install, um, open AI and I need to install elastic search. Alright? And te QDM too. I'll install it again like in the next, uh, command. Oh, it's actually download into QDM, probably it's dependency in some of the libraries. Then I'll do Jupyter Notebook. So it, I'm doing the same thing we did previously. And I also want to start, uh, Elasticsearch. So now it'll start elastic search. I have started Jupyter Notebook. I can open it. So then I get the token from here.\n'slidetext': ''\n'from': 'index'\n'score': 14.231377601623535\n'normalizedScore': 0.14231377601623535\n'beforeDatePenaltyScore': 0.14231377601623535\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d3ef5cff79d6f5cd99'\n'access': 'granted'\n'part_number': 49\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': does the filtering, and then we probably present it to the user. But like we can spend a lot of time talking about that. Let me just quickly show how we built a ui. We have 20 minutes before the lunch break. Um, so we'll use Streamlet for that. Um, this is like, we'll just instead of using that in a Jupyter notebook, we just can have a nice UI there. We can talk with this. And I really like using charge GPT. So what I want to do now is I can say, um, I have a function rock and I want to in walk it from stimulate, um, the UI should have, um, so we need to have an input box. Input box for the user query, um, button for invoking the function, output, um, text, um, just output for showing the results and when, uh, the function is working, um, show a loading symbol, something like this. So now we should generate some code. We'll need to install it of course. So I will do it right now. P install stream, delete. And we'll need of course to create a file, I'll call it, I'll call it, I'll create two files rack pi, where we'll put all the code from the notebook and we will have, um, app pi where we'll have the interface, right? So right now, while charge PT is giving us the answer, what I want to do is take the code we need from here and put it to the R pi function. So what we need is we need a bunch of, um, imports. So we need elastic search, Right? So that's one thing. Index name, uh, we don't need indexing 'cause indexing happened outside. We need the search query. Um, we need to have this CDT function then Yeah, from OpenAI, we need to import, we need to have this OpenAI client. And then all these prompt templates, uh, I'll put them here. Um, yeah. And then this is the final thing, uh, two final thing. So in working on LLM and then the rack thing, right? And now we will import it from, oh, let's see what change p actually applied with. So this is the, this is the code. Hmm. That's quite simple. So what we do here is we use stream lead. This is our rack function, which we'll replace later, uh, with the actual thing. Uh, then there is a text input for the query. There is a button and we have, have some results. So let me start it. Stream lead. What is the extremely trap run? Okay, so we\n'slidetext': ''\n'from': 'index'\n'score': 14.231377601623535\n'normalizedScore': 0.14231377601623535\n'beforeDatePenaltyScore': 0.14231377601623535\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d1ef5cff79d6f5cd8b'\n'access': 'granted'\n'part_number': 35\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': back. How long break? Let's have uh, 10 minutes. Oh yeah. So 11, 18, 11, 12. Let's 1120. So we continue. And right now, so we, We finished this part. Now we'll do this part. So we'll now use OpenAI. I spoke with some of you, you still don't have, uh, 'cause you need to create, uh, an account in OpenAI and also put some money. I realized that it might be problematic for you to do this on the spot. Um, we'll in the second part of the workshop we'll replace this part with something else. But also if for you just sitting here and watching me doing with it, with open AI is boring. Could be, right? So there are some alternatives. I want to quickly, before we start, I want to talk about these alternatives. So then, uh, maybe like if for you it gets very boring, you can try to use something else while we do this workshop. But in the second part we'll run these things locally. It should be, okay. So I want to show you another, if you put in Google LLM Zoom camp or like, there is actually a link here at the beginning of this document. This LLM Zoom camp, this is the course I mentioned. And in this course we have a list of open AI alternatives. So these are services that you can use, uh, to send a prompt and get back the answer. So services, uh, LLM services and many of them have actually free credits. Some of them are totally free like grok. Um, some of them they give you free, free credits, um, like some of them free. But with rate limits, um, yeah, you can check. Like for example, uh, many students use grok and it's fine. So if you find yourself, um, if you think it's boring for you, 'cause I do, I use open AI and you cannot follow, you can maybe create an account in grok and use that. Right? So this is, uh, let me show you one more time how to find this link. So at the beginning of the workshop document, there is a link to LLM Zoom camp. You click on that, you go to the first module. And in the first module there is these open AI alternatives. So I see that there is a question. Yes. Uh, do you also go into details on uh, setup? Yeah, so the question is if I go to into details for local alarms, yes. In the second part of the workshop we will use all LAMA locally and then we'll use some other\n'slidetext': ''\n'from': 'index'\n'score': 14.231377601623535\n'normalizedScore': 0.14231377601623535\n'beforeDatePenaltyScore': 0.14231377601623535\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd7f'\n'access': 'granted'\n'part_number': 23\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': shouldn't matter. Exactly. I get connect. Okay. I don't know. So probably you are trying to connect with to Elasticsearch from outside. I dunno. We can look at this, uh, during the break, but yeah, I, I don't have, uh, much experience with firewalls. Maybe you have a corporate laptop. So I have my personal one and all the firewalls are disabled 'cause I hate them. Uh, is it possible? Of course it's possible. Like with firewalls, I, well, since you experience it, it's possible, right? SSL certificate. Um, so yeah, you see we have this thing when we start Elasticsearch, there is this command X pack security enabled false by default. Elasticsearch wants to use uh, S-S-A-S-S-L certificates. But on local host, we don't want to do certificates, right? So that's why we tell Elasticsearch to just relax and don't not care about security. So then we can connect this using HT TP instead of HTPS, right? So maybe something happened along the way when you were executing these comments. Yeah, did you do this? Ah, I see. Um, okay, so where were we? So we index the documents and now let's retrieve these documents. So the query I have is I just is Covered the course, can I still join, right? So this is the query we want to use and now we want to find all the documents that we just indexed that matched this query. And again, like, I don't know who came up with this syntax for querying in elastic search. There is no way on earth I will ever remember that and type it right now, so I just have to copy it. Um, so I'll explain what's happening here. Um, so this is our search query. Like in relational databases, you use SQL in Elasticsearch you use that. And here what we do is we say that we want to have five results. So it's kind of similar to limit five. Then the query is, um, it's called a bullion query. I, there is some history behind it. I don't know why. Uh, you can ask JGPD. Um, yeah, but like this is what you need to put. And then there are two parts in this bullion query must and filter. Um, so must is what, like in elastic search, you can have must and shoot queries. So must is it must match and should it should match, right? So if it doesn't match, okay, uh, it can still show you and must is like, there should be, there must be a match, right? Then like if there is no\n'slidetext': ''\n'from': 'index'\n'score': 14.231377601623535\n'normalizedScore': 0.14231377601623535\n'beforeDatePenaltyScore': 0.14231377601623535\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd7a'\n'access': 'granted'\n'part_number': 18\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': much time on that. If you want to know. There is, um, this, uh, file par FAQ and it uses a doc X library. So I save these, uh, documents as a doc X file. And then I use a library from Python called Doc X to parse it, right? And then you can go through this, uh, code, you can use, uh, like for example, let's see JGPT explain this code. Yeah, so then you can ask chair GPT to explain this code. If it's, uh, for you, difficult to follow it. Um, I will not spend a lot of time, uh, here, but this is how I extracted it. So at the end we have these documents, the Jason file, and it contains all the questions, all the answers from the FEQ documents, from all the three ones. So I think this is the engineer one, then at some point, yeah, there was here, this is machine learning one. So we have three of them here. And what we want to do now is get this file. And for that I will think there is, uh, in the instructions, we actually have the path to this file. So the retrieval, yeah, so this is the comment to download this file. I will just copy it. Um, but this is just the path to, to do this document. We'll now download it and read it to, in our, uh, in our Jupiter notebook. So I think what I will do is I will go to our, um, Jupyter notebook and then I will execute that. So exclamation mark w get and the path. So when we add exclamation mark in Jupiter, we execute a b command. So right now we're just executing a b command. We are downloading the file and right now we can read this file. So for that we need to have, uh, Jason, uh, library and we do with open, um, documents. Then, um, so right now we want to load the content of this j file. This is what I do. So we do it in the read mode, in the text mode as f in, and then documents. I'll call this variable documents all. Um, we use json a lot F in. So F in is short for file input 'cause we read, and usually if I want to save something, I call this variable F out. So now this documents all contains all these, all these documents that I showed you previously. So, and for example, if I want to see the first, For the first course for the data engineering course, uh, the documents, the first document, This how it looks like,\n'slidetext': ''\n'from': 'index'\n'score': 14.231377601623535\n'normalizedScore': 0.14231377601623535\n'beforeDatePenaltyScore': 0.14231377601623535\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d4ef5cff79d6f5cda3'\n'access': 'granted'\n'part_number': 59\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': dream, I have not tested it, but like they should be better for local environment 'cause we don't like, even though our laptops might be quite good, especially if you have a MacBook Pro, like it's probably super fast, um, but still not fast enough to uh, compare it to A GPU anyways. So we'll use this, uh, lama or I'm sorry, P three, but before we can use it, we need to download it. So we use this command all llama pool P three. Okay. And of course I need to start it all llama start. Okay, so because it has this client server architecture. So I just started the server and then in another process, in another terminal, I will do all lama pull P three. P three, okay. Now because the server is running, I am able to download the model and I think all the steps, let me check that they are actually recommended. Yeah, so Alama start allow, I also did alama pool P three, but I think if you just do alama surf P three, it'll also work. Hmm. Is it actually surf? Because I think what I did is alama run PP three. Yeah. So I don't know if this one works. I probably need to update it. Okay. Yeah. So I was probably typing it from memory there. Uh, so let me change it to run, uh, in one terminal. In another terminal. Okay. So I did alama start and then in another terminal window I did a lama run P three. And what we can do now is chat with this thing. Okay, how are you? And then this is an lm So this LM is now generating answer. Uh, so we can for example, ask um, how long is how Mara, okay. Yeah. So it is answering something. Um, of course we are here not to ask it. How long is the microphone? We want to use it for rack purposes. And if we take a prompt, um, I should have it somewhere, I think. Do I, uh, I think I lost it. So I wanted to ah, we have this prompt, right? So you, we can take this prompt. Maybe I have, um, an example here. Um, yeah, this prompt. So I'll use the prompt we had previously, and this is a fairly long prompt. So a llama will, it'll take some time for this to think. Or for P three, I can also run it locally. So locally I have eight core, so it should be faster. Let's see. Okay, I need to install it. Um, yeah, but I can hear that my laptop is\n'slidetext': ''\n'from': 'index'\n'score': 14.189891815185547\n'normalizedScore': 0.14189891815185546\n'beforeDatePenaltyScore': 0.14189891815185546\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d2ef5cff79d6f5cd8e'\n'access': 'granted'\n'part_number': 38\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': this is kind of the same here, but usually there is just one response Number choices. The number of choices Maybe, I don't know. So the, the question was if there is a, uh, if it's a perimeter, if you can specify the number of choices, um, yeah, probably you can. Okay. And then message and then content. Okay. So this is how we get to the actual, uh, response. Okay? And here we did not need, we did not use any context. That's why it again, gave us this, um, generic answer, right? So check official policies, contact the instructor, blah, blah, blah. So we need to give it the context and we can do that by incorporating it in our prompt. So let us build a prompt template. So I'll call it prompt template, prompt, temp late. So the template could be your, uh, course teaching assistant. You need to answer questions, uh, question from students, uh, based on the provided context, uh, question. And here we have our query. Do I need to put two, probably one context. And this is our documents. Uh, we can also add more things here. Like, um, if there is no, if the context does not give an answer to a question, just say you don't know. So you can tweak this prompt how you want. And um, yeah, I'll just also tape it so it doesn't include like any, um, extra things like here. So these our prompt. So now we need to, like, for the query, we already have the query from, um, from the user. This is liquidity. And now we need to build the context, right? So I'll have another template, context template. Um, and it could be something simple like uh, section. 'cause this is how our documents, uh, So this is how our documents look like, right? So we have section question course. Um, so we can have, um, let me just copy it from here, section Here, Question here, answer text, right? And if this is our document, then we just do something like context, template, format, and we specify the document, right? May footprint it. So this is how it looks like. So now we just need to get the responses from Elasticsearch, put them all in one, one thing, and then send it to open a and yeah, let me take this. So I'll call it context documents. Now for document in context documents, I'll have something like context, which will be a string. And now we'll have something like context plus us, um, how, I'll call it context piece. I am very bad\n'slidetext': ''\n'from': 'index'\n'score': 14.189891815185547\n'normalizedScore': 0.14189891815185546\n'beforeDatePenaltyScore': 0.14189891815185546\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd76'\n'access': 'granted'\n'part_number': 14\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': run Jupyter Notebook and this is where we'll work. And in order to finish preparing the environment, we need one last thing is we need to run Elasticsearch. 'cause one of our components is the knowledge base and we'll use Elasticsearch for that. Uh, so I will, so there is a comment that you need to execute. I copy this comment, I go to my terminal to visual studio code. I create a new terminal and I copy paste this comment there. So what it'll do is it'll run um, a particular version of Elastic Search 8 4 3 in Docker. So after we finish preparing the environment, I want to have a small break. And if some of you have uh, problems, I can also come to you. But like we will not spend too much time 'cause I want to have like a few small breaks and then one larger break. Then we'll have also lunch during the lunchtime. I can also check, uh, if you have any problems. And then we'll also have a longer break during the second part after lunch. Yes. How is this elastic search different from other like, So elastic search is a text, is a text search engine. Um, so yeah, I'm just thinking maybe I should draw. So, um, we can divide search into two categories, broad categories into text search and into vector search. So text is uh, just like we can have a uh, a query I just discovered in the course, can I still join, right? So this is uh, a query. So what happens with text search is we just take this as is As and sent this query to our index to our search database. And then it looks at words like discovered course join, right? So these are the important words. And then it looks at documents that have this word too, right? So it also does some pre-processing. So for example, discover this past tense so it can look at discover, not discover it, right? Um, but basically what we do is we try to look for documents that contain these words. What happens in case of vector search is we turn this document into an A of certain dimension, which can be like just a bunch of numbers. Uh, Um, right? So it could be like, I don't know, um, 100, 1024 numbers. So we have numbers. So we turn this, uh, query into numbers and then all other documents. So here in this case, in text search, all the documents are represented as text. So we have texts here. Here, all the documents are represented as vectors two, right? So we have uh, vector\n'slidetext': ''\n'from': 'index'\n'score': 14.189891815185547\n'normalizedScore': 0.14189891815185546\n'beforeDatePenaltyScore': 0.14189891815185546\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36daef5cff79d6f5cdc5'\n'access': 'granted'\n'part_number': 93\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': some agents? Agents not in the course and not here. So an agent is a thing that can do actions. So for example, you can, so in addition to that, mm. And so let's say you want to build an agent that traits on Forex or some, uh, so then it can analyze some things and then put a bit to buy, I dunno, apple shares, right? So it can perform some actions and usually these actions are quite, um, how to say could be dangerous like this. What happens if this agent gets out of control and spends all your money, right? So that's why it's still, people still figure out how to exactly use them m based agents. So 'cause you can open a lot of possibilities with this, but also you need to be careful so the agent is somehow controlled. Um, yeah, I don't like, I don't have a lot of experience trying it. Um, there is, I try it one thing that, um, it was called auto GBT or something like this. I don't remember exactly the name. So the idea there was that you give a prompt, I want to build the jungle website for, I don't know, e-commerce, right? And then it would ask you some follow up questions and then just build the website, right? And the agent would actually execute the code and like do all these things. Um, at the end this thing didn't actually work. Like the code it produced wasn't working. And for me it was better to just use charge GPT to ask it and then copy paste the code and then do a little bit of tweaking because I am involved in the process so I know more what's happening. And with the agent it was like, okay, it's doing something, but like it's working, is it not? I dunno. So yeah, maybe somebody else has other experience with agents, but I haven't experimented with like good agents yet. Will it create a JGP account? Maybe. Maybe. We'll see. Well probably you haven't, you haven't created it for a reason, right? I I just set a hook so maybe I Won't be there. Uhhuh. Okay, well I guess I'll be around. Um, I want to have a small break, but then I'll come back here and then if you have any problems with some of the content here, I will help. So we still have time till five, so thank you.\n'slidetext': ''\n'from': 'index'\n'score': 13.799471855163574\n'normalizedScore': 0.13799471855163575\n'beforeDatePenaltyScore': 0.13799471855163575\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d9ef5cff79d6f5cdbd'\n'access': 'granted'\n'part_number': 85\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': I will add this thing here. So then in our um, stream lead application, we can um, can use that instead of OpenAI. And then the other thing we need to change is the this LLM function. So here I will command that part and put P three here. Okay, Did it stop? Yeah. Okay. So now it actually gave a better answer. I think we can still tweak it. 'cause we don't want to output based on the provided context, right? So for the student, like they don't care about the prompt. We give like we don't care. They shouldn't even know that there was some context and so on, right? So we want to actually do that, right? Um, I think this prompt requires a bit of tweaking, but at least it works for me now this time, which is good. Okay. And what I will do now is I want to experiment a little bit. So I want to stop docker compose. So right now it should not work. So when I run it one more time, yeah, there is connection error. So it doesn't work. And let's see that, uh, all the state is preserved. So I run it one more time. So Alama should have the model. Elasticsearch should have the data. And to make it even more fun, I will start the stream lead up. Um, actually I don't know if we have um, streamlet. I think I only need to install it. Yeah, people install it And then let's run it and then I hope it'll just work. I just discovered the course. Can I still join it? Well maybe it shouldn't remove the query. I think the code is, um, I, Yeah, well it's probably adjustable. I could change it. Yeah, we can even take it one step further and put this team lead in Docker two in this docker compose. I don't think I want to do it here right now. It's not, I have not prepared. Um, any materials for that we would need to improvise, but like you, it shouldn't be too difficult, right? Probably stream lead already has a docker container. Um, yeah, I like this answer more. It didn't say pro given, like provided the given context, whatever, like it just went directly to the answer. Okay, so this is it. We actually finished earlier. Um, like I don't have anything else. Like I showed you everything I prepared. Um, like do you have any questions or, Sorry. Yeah. What was the last change That you made that the, the result, the problem, uh, the Context. I did not do anything. Okay.\n'slidetext': ''\n'from': 'index'\n'score': 13.752570152282715\n'normalizedScore': 0.13752570152282714\n'beforeDatePenaltyScore': 0.13752570152282714\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d6ef5cff79d6f5cdac'\n'access': 'granted'\n'part_number': 68\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': the implementation for this, um, for this simple uh, search engine. And then there is another piece of code that um, so what is happening here is we get the documents that Jason file I showed you, we un nests it. Like we run through the same thing like as we did previously. And this part here is taking this and indexing it with this mean search. And then we can use this mean search to perform search. It's very simple to, it's very similar to some extent to what we did with Elasticsearch. So we can give a boost to certain, uh, fields and we get back the documents. So for our purposes here, we can just pretend this is elastic search that we can run in Google collab, right? So it gives us some um, some, so this is our knowledge base. It gives us some documents, right? Um, so which means we will need to slightly adjust this uh, retrieval function that we had previously. Um, here, this one retrieve. I will copy it and I will put it here and I will replace this. So now I just discovered the course, can I still join it? So then it gives some some results and because uh, the way we designed our rack flow is uh, it's modular. So we can simply take this out, put something in, we can put a vector, the waste here later if we want. So here we'll use this min search. And yeah, so this is how we get the documents. So now to the interesting part, now to how we run the model, We will use this uh, Google TY five. And usually like when we have a name of the model and what I do with this, with this name, I just put it to Google and then it usually brings us to it, it's page on hack face. So then I click and then, then it says okay, there is a model, it's called T five. There are different versions of this model. Like this particular one is Excel and there are smaller models, there are larger models. So Excel is something that fits on HBU that we have. Um, I think they describe um, um, yeah they probably have somewhere like farm T five, we can see what sorts of models are there. Um, so there is small, there is large, uh, small base large Excel X XL six XL is too large, but Excel will fit our GPU. And what is more, so when I googled this name TY five Excel and I found this, um, this document usually describes what we need to do to actually run this model. So I\n'slidetext': ''\n'from': 'index'\n'score': 13.752570152282715\n'normalizedScore': 0.13752570152282714\n'beforeDatePenaltyScore': 0.13752570152282714\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d6ef5cff79d6f5cdab'\n'access': 'granted'\n'part_number': 67\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': get access to the GPUs and they only have have like a limited amount of them, sometimes you might get disconnected, but it happened to me only once. So like usually it's pretty stable. Um, but of course there are limitations that uh, it's only for exploratory work, right? So this is not something you can use let's say in production, right? So there are, um, if you want to, like, if we again talk about this picture here. So right now if we want to completely replace it with uh, o open source NLM that is running on GPU here in co-op, we just can experiment with this, see how select the best prompt, tune the model a little bit, select the best parameters. But once you want to replace a call to open AI with a call to your LLM, you need to host it somewhere, right? So you need to take the model, put it into some service, um, and then deploy it to your Kubernetes or to Amazon SageMaker or whatever. So then it's always available when you need it, right? Okay, I think it finished installing. Um, and yeah, so here in this environment, uh, it's a pretty limited environment. We do not, we cannot do many things here. Like for example, R Docker is not Rhino, Docker is not possible. If I do docker, run a law world, it'll say that. Um, yeah, I don't know what it is. It is still possible to run Elasticsearch in uh, co-op without Docker. Um, but yeah, it's a little bit tricky, right? So that's why for this workshop we'll use a slightly different search engine that works in memory. So you don't need to use Docker. It's very simple one. Um, it works kind of similar. And I think I mentioned, I talked about, um, this sim built your own search engine. So this is the search engine we built in this workshop. It's super simple, it's in memory, you can use it in co-op in any other notebooks, but it's only just for playing with this. So it's in memory, you cannot save the results anywhere. Uh, you cannot scale it. Adding more documents to the indexes problematic. But like for our workshop, it serves as a simply replacement for Elasticsearch. And yeah, I will just download this mean search file which has the implementation for this, um, for this simple uh, search engine. And then there is another piece of code that um, so what is happening here is we get the documents that Jason file I showed you, we un nests it. Like we\n'slidetext': ''\n'from': 'index'\n'score': 13.752570152282715\n'normalizedScore': 0.13752570152282714\n'beforeDatePenaltyScore': 0.13752570152282714\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d5ef5cff79d6f5cdaa'\n'access': 'granted'\n'part_number': 66\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': that we have the newest version of these libraries. And uh, the way we do it, again, we prefix our command with an exclamation mark and we execute. We need to do p install, uh, install minus U 'cause we want to update. And there are a bunch of libraries we need to install. So these are transformers accelerate and bits and bytes. So the most, uh, the library we care about is transformers. So this is library from hugging face. Um, so hugging phase. So hugging face is a place where companies can host their models. And this is from where we will get the model. So this is like, um, in a way like it's model repository where you can save your model, where you can download the model from. And many companies like Microsoft, Google, Facebook, or Meta, they save, they store the models there in this, in this hack and Face hub. And the Transformers library allows us to connect to this, um, hack and face hub, download the model and use this model. So this is why we need this Transformers library. And then accelerate and bits and bytes, um, is for like, sometimes we need to do some extra things with the model, like make it faster, make it smaller. Um, so these two libraries are needed for that. Uh, okay, so let's execute this common too. And this Transformers library depends on PyTorch. Most of the models, if not everyone, if not all the models on Hack Face Hub, they are PyTorch based. So when we pull the model from Hagen Hagen Face Hub, what happens is we get the weights and then we also get the model itself like the, the neural network. And then this neural network is implemented in PyTorch and then it loads the weights and then we have an instance of a model that we can just use. Okay, yeah, it takes a bit of time. Yep. Are there limitations on Google? So I use, So the question is if there are any limitations, uh, not that I know of. Like if you probably start mining Bitcoin, they will block your account. Um, but like for simple purposes like this one, just doing some exploratory work, I did not observe any limitations. Sometimes it may happen that um, if you get disconnected from your environment, maybe when there are a lot of people who want to also get access to the GPUs and they only have have like a limited amount of them, sometimes you might get disconnected, but it happened to me only once. So like usually it's pretty stable. Um, but of course there are limitations that uh,\n'slidetext': ''\n'from': 'index'\n'score': 13.752570152282715\n'normalizedScore': 0.13752570152282714\n'beforeDatePenaltyScore': 0.13752570152282714\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d4ef5cff79d6f5cda5'\n'access': 'granted'\n'part_number': 61\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': it, I'm doing the same thing we did previously. And I also want to start, uh, Elasticsearch. So now it'll start elastic search. I have started Jupyter Notebook. I can open it. So then I get the token from here. But you also need to reindex Those. I need to reindex exactly because uh, this is like a fresh environment. I need to redo all the steps. Um, luckily it shouldn't take long, so just need to wait um, till elastic starts. Yeah, we can see that it's up and running. So I will, yeah, I need to download. So basically I need to redo, redo the whole thing. So Elastic is running, create an index, index the documents and then let me also execute that. Um, so right now, uh, I do not have dear end here. So when I run this thing, when I run open AI client, it will probably complain saying that there is no key. So let's wait till it in excess. Yeah, so it says there is no key. We can just say, um, doesn't matter. 'cause what we want to do is we want to use our alama here. And there was a question about uh, providing, like setting the base here on this is when we actually, how do I close it? This is when, this is actually the time when we want to use it. 'cause right now our alama service is running on local host. I don't remember the port. Um, but we Um, so this is how we do it, right? So we say that, Here, Let me close It. Um, so the API key doesn't matter. And the base URL is this one. Let me check that. It's actually the right port. Yeah. So this is where Alama is running. And so this is how we connect. We can connect to Alama from our OpenAI, API or OpenAI client. And now, um, so the way we speak with the model is slightly different. I will Just Take this piece of code. So if I just take, if I just run this code, it'll say that this model does not exist. Oh yeah. And the prompt is, let me just put some prompt is how do I join the course? So right now it'll say that this model does not exist. Yeah. So the model doesn't, is not found. And yeah, so what we do simply replace it with the model we just put. So if we put a different model then we would put something else here. But now we put P three. Now it sends a request to our, maybe we can see it\n'slidetext': ''\n'from': 'index'\n'score': 13.752570152282715\n'normalizedScore': 0.13752570152282714\n'beforeDatePenaltyScore': 0.13752570152282714\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d3ef5cff79d6f5cd98'\n'access': 'granted'\n'part_number': 48\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': we have a computer and a batch of documents, there are texts, images, whatever. And then I want to have an interface where I chat with this thing and say, hey, like give me pictures of my last vacation, right? So here in this query, we need to understand if the question is about text or images, right? And I think you just like when you were asking this question, you kind of already said how you would implement it, right? So first we need to understand what the question is about and then we can actually use an LLM for that or maybe some classifier to understand what kind of content we're interested in then, right? Yeah. 'cause Here we're asking specifically for one particular course and I would need To know, yeah. Now I'm Asking maybe modality Type data. And based on my product, I would like the whatever Yeah. To understand. So you would need to figure out the user intent. You would need to have a model for that. And then if I needed to implement something like that, so there is a thing called clip a model from OpenAI two. So the idea behind clip is then it can turn texts and images into embeddings. So this process, but the process of creating embeddings not only works for text, but also for images. And in such a way that if a text and image are semantically similar, the embeddings will be close in this space. So this is Data or Yeah. So it would be an embedding for each image would con contain, uh, would, we would turn each image into embedding using clip for example, and we would index it with a vector database. And we would also turn the query into an embedding. And then we would perform a vector search. And I suspect that if we have pictures about vacation and we have um, the query about ation, they should be quite close. So the vector database will be able to retrieve it. And then we will just need to add this last part of last location, right? So then we will also need a, a vector database. We that can store metadata, not just vectors, but also metadata about the images, right? Yeah. For example. And then, um, so, um, we just send it to an LLM to select, to ask for example, we have these queries, these results, which of them are relevant for the user query, then it does the filtering, and then we probably present it to the user. But like we can spend a lot of time talking about that. Let me just quickly show how we built a ui. We have 20 minutes before the lunch break. Um,\n'slidetext': ''\n'from': 'index'\n'score': 13.752570152282715\n'normalizedScore': 0.13752570152282714\n'beforeDatePenaltyScore': 0.13752570152282714\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd84'\n'access': 'granted'\n'part_number': 28\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': on the internet these days by the vendors, they're about vector databases. Better If it's very structured document. Yeah, yeah. Pretext. Mm-Hmm. Would be hard to No, no, no. Like Elasticsearch is a, a search engine, right? Usually do some pre-process. Yeah. Right? So the stop words and limitation and whatever, and uh, where, where does that fit into that? Yeah, so Elasticsearch is doing that. So stop words doing like this. I was just talking about this analyzer. So when you send a document to Elasticsearch, it's doing that. So when it creates an index, it's doing all that. You have control over this process by specifying what analyzer you want to use. Um, so there are a bunch of analyzers that, that already exist. You can probably write your own in Java. What about Samantha? Similarities? So Elasticsearch can also do that. Mm, yeah. Paragraphs. Yeah. Elastic. Well, you cannot send a PDF to Elasticsearch, right? You need to first extract text from the PDF. No, understand. Yeah. What you're gonna send to the, could you use elastic search to find the paragraph? Or you, So like for example, the way you would do it with vector search, you would take the document, you would break it into sections, subsections, or maybe you would just have like a window and you would slide the window over the document, right? And then each time, like when you slide a window over the document, you have a document, you create an embedding for this and you put it into vector store. You can do the same here except you skip the embedding, creating the embedding part, you just index it directly with Elasticsearch, right? And then at the end, it's tech search, it works quite well. It's fast, but like one does not exclude the other. You can have both or you can have just that, right? And what I see, if you go to a conference, there is usually a booth by one of these people. And of course in their demos, they use their product. It doesn't mean tech search is bad. Vector search is also good, right? But I, what I would do is like just experiment with two and see what works better for your specific case. And uh, finally thing is like I did this workshop already a few times and then somebody approached me saying, Hey, I replaced the vector that the base with Elasticsearch and it became better. So like, you just need to try, I Think about elastic. Yeah, Uhhuh recommend probably the quality embeddings, right? Which Yeah, like\n'slidetext': ''\n'from': 'index'\n'score': 13.752570152282715\n'normalizedScore': 0.13752570152282714\n'beforeDatePenaltyScore': 0.13752570152282714\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd75'\n'access': 'granted'\n'part_number': 13\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': happens is, uh, at least with Visual Studio Code Desktop, I don't know about the browser based visual studio code. So with the desktop version, it detects that there is a port on the remote machine and then it forwards this port to my local host, which means that I can now take this URL from here and put it to my browser And have access to Jupyter. So this Jupyter notebook is running on a code basis, on a code space, but I can access it from my browser because of port forwarding 'cause it detected that I started a process and it automatically forwarded to my host machine. If you don't see that, you can add port yourself. So you just type uh, 8 8, 8 8. This is the default port from Jupyter and then it'll forward this port to you for you. And here I will start a new notebook. We can call this notebook. Oops. I just rename it to rock. And then what I also like to do is do not show the heater. So then there's more space, but also because I have uh, like um, quite a large Zoom. And here we have open AI already we have uh, Elasticsearch. Yeah 'cause we installed them previously. Edit the rack, Right? Sorry, The local version, it's rack is The, no, like you need to in local version, if you use Jupyter Notebook or you, you will actually, you should use Jupyter Notebook. And when you use Jupyter Notebook, like it'll be rock, whatever extension Jupyter notebooks have, you can of course use plain Python, but you will not have this interactivity you have with Jupyter. So you will need to execute your Python file every time you change it. And I find it annoying, but if you want that, yeah, by all means. Okay, so what we have done so far, so we created a code space. We took a key from OpenAI and put it to the code space. We used Dear N to manage the key. So we created this, uh, n for C uh, and put the key there. So right now our key is safe. You will not accidentally expose it. I will not accidentally expose it. And we put um, this data for seat to git ignore. So you do not accidentally commit it. You don't accidentally push it to GitHub, right? And now we run Jupyter Notebook and this is where we'll work. And in order to finish preparing the environment, we need one last thing is we need to run Elasticsearch. 'cause one of our components is the knowledge base and we'll use Elasticsearch for\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d8ef5cff79d6f5cdb6'\n'access': 'granted'\n'part_number': 78\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': So here, like the other one is the same. You can just check what are the models here. Uh, usually um, yeah, so they are ranked by a particular metric and depending on the GPU you have like if you have uh, A GPU that is only six gigs, 16 gigs of ram, then you're interested in seven B models or eight B models. So these ones are too large for this. And then you can just scroll down. Like for example, metal lama should be fine. You can play with this, you can try it. Then this one can actually, I didn't manage to run it but um, you can try it. So usually you're interested in this seven or 8 billion models. So MR Trial is also here. So this is how you can find models. Um, and also of course we can use um, cha GBD. So I can ask, give me a list of recent seven B open source LM models available On hugging face hub. And then, uh, yeah, it'll just give you a summary, right? And this is what you can try this Google gam, gemma gamma dei lama, open Lama mistral. Um, yeah, so there are many, many models like you can experiment with that. Then most of them have the instructions of how to run them. So for example, this one, I dunno what is it? But usually like there is, there is, there are instructions of how to run these models. And if there are no instructions, uh, probably you can again just ask JGPT to like okay I am like let's say uh, I want to use, how do I do it to run, uh, on A GPU on call up, right? And then it'll tell you the code you need to run. Like sometimes it happens that these models, the model cars you have, uh, on hack phase hub, they do not always have these instructions, right? So you want to have these instructions and usually you can just check GPT just knows how to run them. Hmm, yeah. Okay. Yeah. Does anybody out the form GT or Um, uh, like models that outperform GPT uh, four will not fit on this uh, gbs. Okay, so it's not something you probably want to run yourself 'cause they're just too large. How much battery? I dunno, like this seven B uh, they usually, like we saw with um, this uh, flan T five, we took roughly 11 gigs of ram. And if we look at this, uh, performance, so apparently this one is super performant 20 BI think they have, well they actually don't use that much memory,\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d7ef5cff79d6f5cdb2'\n'access': 'granted'\n'part_number': 74\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': down to just taking these calculations and then seeing what makes, makes sense to you. At some point, this becomes cheaper than this, but you have to have a lot of load. And then there are also spot instances here with GPUs. You can also play with them so they're, uh, less reliable than non spot in, like on demand instances. So maybe you can have like half of GPUs or on spot, half on uh, non spot, but you have to be pretty advanced with your product to do that. I think. So probably it's way easier to start with just, you know, using the managed service. And when you at some point look at the bill and it's too expensive, then you start experimenting with that. In my opinion. Uh, actually here, there is a very, like, there is an upside. There is a big advantage of doing that, uh, with um, uh, open ai. They might change the model anytime they want. And the prompt you designed might just stop working. Like you have a prompt, you have a thing that works and then one day you wake up and it stops working 'cause they rolled out a new model. And like even slight changes in the model might cause your prompt to give a different answer, right? And if you expect your answer to be in a certain way, but now it's different, you'll need to uh, fix it to make sure it works with a new version. And then month, one month later, they will release a new version and your pro break breaks. Again, when you host it yourself, you always know what you host so you don't have these breaking changes. But yeah, of course, like you have a lot of overhead in maintaining this scaling top down. Having a person who looks after that, like maybe if you do this in Kubernetes then somebody needs to know how to use Kubernetes and so on. Any more questions? But I think this is still pretty fun, like, uh, um, to experiment with these things. Um, this is not necessarily something you will use in production, uh, but at least you can play with these things locally and then you use something like grok to actually, um, to use it. 'cause like Grok has all these models. It has llama, it has, uh, I dunno about Flay five, maybe it doesn't have that. You can play locally and then you can start using, um, like a service. Another use case for that is when you have, um, a lot of files and maybe you want apply to, hmm, let's say there is a competition right\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d7ef5cff79d6f5cdaf'\n'access': 'granted'\n'part_number': 71\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': the LLM function here. We will need to replace this part with um, the thing we just implemented. And then the final thing, this rock is what it puts, like what puts everything together. So now let's replace this thing. So let me remove this one. I will now put everything in one cell. So I take this one and I put it here and then input text, I replace the prompt, and then instead of uh, printing, I return it. And I think that's it. This is what we need to, so let me check. I just discovered the course. Can I still join it? Yeah. So we can see the answer. The answer is uh, well it cuts the answer at like, it starts generating and then we cut the answer, right? And here, the reason we did it that there is some default parameter. It's a p that autocomplete doesn't work for me. I don't know for, oh, okay. Yeah, it took some time. So there are some parameters in this model, right? And this number of beams do sample whatever. Um, yeah. So here one of the parameters, uh, I hope I remember correctly is, um, max, no, lemme just check it. Max, uh, yeah, max length. So we can just put it here. We can say that max length is 100 and this way I think it's 100 tokens, which means that it'll not stop after the, it'll try to continue. So let's see if it works. Yeah, so with, uh, we just need to tell it return more. There are more parameters that we can, uh, play with. And um, here, yeah, you can read about other parameters, you can experiment with, um, like in this piece of code, but like just replacing it, just giving, just setting max length, 200 already gives us good result. Um, and this part is a little bit annoying. You can simply remove it in pre-processing, like pad and slash as I think, uh, this is some um, um, like you, you see it always starts with this and it always ends, ends with that. So this is just some extra special tokens that they used in their training data for all the input and output. So we can easily remove this input processing. What, let's check, uh, where do we have the prompt? Don't add NS in your answer. I think it'll still add this. Let's see. Yeah, well this is very easy to fix. You just, uh, yeah, I replaced or you do a subtext, um, sub string. Okay, so this is not the only model we can use. So this one\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d4ef5cff79d6f5cda6'\n'access': 'granted'\n'part_number': 62\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': so what we do simply replace it with the model we just put. So if we put a different model then we would put something else here. But now we put P three. Now it sends a request to our, maybe we can see it in the locks to this service in ar lama. Mm. Yeah. It's not showing if something's happening, but we can probably see it in um, h stop that. Yeah, the course we used. Yeah, it takes some time. Let me actually, yeah, there are no questions in the chat. Okay, good. Um, Yeah, I'm surprised it takes that much time 'cause like the prompt is short supposed to be fast. Um, yeah. Okay. Yeah. The reason it was taking that long is it's a bit verbose from what I see. Like it wrote a poem. Um, yeah, we can actually see what exactly it wrote by printing it. Um, yeah, so it's a very long answer, but this is how we connect to FI three right? Locally. And all we needed to do was just change the base euro, right? Well put some key, it doesn't matter. And then also change the model to P three and the rest stays the same. And because it stays the same, I will just take this thing now and put it to our LLM and I think I will save this notebook as rock or llama. So later when you think, I'm just curious what happened now. Okay, so this is actually a new notebook. What I wanted to do is rename it. Okay, let me, where's renaming? Okay, doesn't matter. I'll just rename it later. So, 'cause I don't want to re rerun the whole thing. Okay, I'm still here. Um, and I just noticed that uh, it looks a little bit different, the interface, Whatever. So, um, where we were, so we changed this thing, so now it's P three, uh, let me execute like this, this, this, and this. Um, okay. And I lemme just remove all that and let's test it with this, um, query way. Well it might be right in a poem right now too. 'cause like when we use it from the terminal, we see that it's riding, it's uh, we see that it's doing something and here we just need to wait till it finishes. And then it sounds, it gives us the whole thing. Think if you do it locally, it's probably faster. Well at least it's doing something. Um, I wanted to to look at HO how just create a new terminal. Yeah, well it's apparently\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d2ef5cff79d6f5cd97'\n'access': 'granted'\n'part_number': 47\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': on score. So you have some control over what is happening here. You just send a prompt and you hope for the best, I guess. Well of course like fine tuning has some parameters probably it works well. But yeah, again, I haven't tried it. Maybe it's excellent and you don't actually need to worry about that. So let us quickly build an interface. So yes. Yeah. So what, uh, you images Mm-hmm. Next to Mm-Hmm. Um, how would you make Yeah, Yeah. Setting up this kind of Mm-Hmm. How would you Connect this? Make the answers So short? I'll repeat the question. Like how do I include non-text things in my prompts, for example, images and something else? I don't know. But let's ask how to include images in my open ai. Hmm. GPT-4 O model. Yeah. Multimodal capabilities. Um, yeah, just give me the code. Ah, okay. Like if it's not hallucinating, then this is how you give images. Uh, but like it doesn't, I think this is, uh, an old, uh, API. So right now it's V two, so it's, look, it looks different, but apparently there's like, let's see, extra body. I would like to Yeah. Ask how this image relates to my application. Mm-Hmm. Okay. So, um, for that you need to be able, okay, I'll, I'll repeat the question. So what if I have a database, I have a bunch of images, a bunch of text documents, and I want to be able to perform search on that. Um, and um, a question could be like, can you repeat it? Like what kind of query can you have? So Let, let's take the use case. I have my computer like to search my computer in a check. Yeah. So about vacation pictures, right? Yeah. So, so I might have documents and uh, whatever text content Uhhuh, I might have my images uhhuh. Now I Need asking a question about my vacation Uhhuh. Okay, so How would I actually make, How would you implement that? You know, Interface First Understand, uh, this is related question related to images. Mm-Hmm. And Is this like the database for Mm-Hmm. Um, and not the text one. Okay. So I'll try to repeat as much as I remember. So let's say we have a computer and a batch of documents, there are texts, images, whatever. And then I want to have an interface where I chat with this thing and say, hey, like give me pictures of my last vacation, right? So here in this\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d2ef5cff79d6f5cd96'\n'access': 'granted'\n'part_number': 46\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': And it works quite well. Usually with LLM, like they have so many parameters. I don't even know how long one iteration will take. Uh, I think now it's probably not that long, but okay, does it really make sense to do it for 1000 documents? You have to have a lot of documents, right? For this to make sense. That's one thing then Pretty well supports it. And you don't even have to many box. Okay? Works out the box. So the comment is like, it works out of the box really well, not even for many documents. But then another thing, let's say the document this FAQ changed, right? How fast do we need the update from there? Right? So if we fine tune it means that we need to take our data again, do another iteration on fine tuning, and then have a new version and then use it. But with rock, we can have it instantly, right? The moment the document changed, we can update our knowledge base and the, the next time user asks, we just retrieve a fresher version. Fresh version. Mm. So well you say it's fast, so then it's probably fast. But then I would just experiment what works better. So from what I see rock is, uh, it has many advantages, but the disadvantages, um, that you have like one external thing, right? Well, not external, but like instead of one thing, you have two things that you need to look after and maybe you don't want that. Do you have s like there's difference in quality in the first approach in the second one because In the second, the difference in quality, putting That knowledge into the model and here you're providing it externally in the context. No, I have never experimented this. I never fine tune an LM, so I don't know. But yeah, um, I would just test it I guess. So to me this seems more, um, flexible 'cause like there are advantages. Yes, you'll just have one thing, but like how do you control, like here, you can still control whether you have relevant documents here, right? Because like if the documents, like if it doesn't return anything, you know, that you don't have an answer or maybe like there is something with the search, but like you cannot do anything with this. Uh, probably. Or you can also do a cutoff based on score. So you have some control over what is happening here. You just send a prompt and you hope for the best, I guess. Well of course like fine tuning has some parameters probably it works well. But yeah, again, I haven't\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d2ef5cff79d6f5cd8f'\n'access': 'granted'\n'part_number': 39\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': So I'll call it context documents. Now for document in context documents, I'll have something like context, which will be a string. And now we'll have something like context plus us, um, how, I'll call it context piece. I am very bad at coming up with names, but we can do something like this. And then context, we will just strip unnecessary land breaks context. So this is what we have, right? So we put all the documents, we started from Elasticsearch into one thing, and now we just take our prompt template and I'll just do it here, uh, format. And then query is query context is context. And we have our prompt. I realized that I might be typing quite fast, but like all the code is here, you can just copy it. Okay? So this is our prompt. And now we can send this prompt, uh, put it here. We can send this prompt. This is the prompt. We can send this prompt to OpenAI, and then it'll respond as, yes, you can still join the course even if you discovered it after the start date. So basically it gives us the answer to the question based on the context. Try phrasing the question. For example, the words that started. Yeah. So let us first clean it a little bit. So I want to put everything into separate functions, and then it'll be easier for us to do that. 'cause right now we have the, so there are three, three steps here. And let me call it, uh, I'll call it track. So this is the whole flow that we described here on the flip chart. So it starts with the query from the user. Then based on the query, we need to, um, where this piece of code is like, I'll just clean it a little bit. So this relief part, right? Um, so the first part is the, the relief part. We have, um, the context documents, right? Right. This is the first step. Then we build a prompt, build prompt, then the input to the prompt query and context documents. And the final response is, uh, we'll call this function LLM. And the LLM gets in the prompt and it did, does some response, right? So we already have this function. We need to implement this too, right? And this is, we just need to take the goal we already have. And yeah, put them. So for this LLM I'll create a function, Uh, here. So, and then instead of printing, I just return it. So this is our LLM and this will be our build prompt function that I'll put here, build prompt,\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d1ef5cff79d6f5cd89'\n'access': 'granted'\n'part_number': 33\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': just one, right? So the question was, um, so with vector search, we'll also like with um, tech search, we get like, let's say top 10 to five documents with documents Yeah. With lots of doc mm-Hmm. So We dunno, Mm-Hmm. But like usually when you do vector search, vector search also it computes similarity and then you get the score right? And then you can rank your results by the score, usually vector that the basis do this for you and you get top ranked documents. It can be five 10. And then it's up to you to like, it's also one of the parameter, like how many documents from the contacts you want to put in the prompt could be three, could be five, could be 10, right? So it's also one of the parameters. 'cause like the longer the document, the more you pay, right? You don't want to send like 15 documents, maybe three is enough, but sometimes maybe the answer is in the third one. So you need to experiment. And also of course, uh, here I will not talk about that, but you can, um, and you should actually have some metrics here in place to assess the quality of search and to assess the quality of the answers here for search. Like there are all the metrics like precision recall mean, average precision and DCG, like all these metrics that you can use here. And I would actually start with making sure that this part works well. If your retrieval is good, then this one will more likely to work better cosmetics in this space, in the search space, they're really well developed. They're well researched, they're used in industry very widely. Um, it's common practice to use them. But here, like there are not so many metrics to actually evaluate that. Like there is this ragas, um, but not so many of them. Like, it's still an area of active research, right? So I would actually spend quite a lot of time making sure that this retrieval part works in practice. Uh, and usually if it does, then this part is like, columns are pretty smart, usually. Like, except when they ate. What makes this meeting you? Uh, Yeah, Generally. Mm-Hmm. What is the best way to extract into, uh, extract format? So the question is about parsing, turning documents into this JSON format, right? Well, it's, it depends like on your documents, I guess. Like if you have a bunch of PDFs and you need to do OCR r on them to extract them, this is what you need to do, right? Uhhuh, I, I don't\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd77'\n'access': 'granted'\n'part_number': 15\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': query into numbers and then all other documents. So here in this case, in text search, all the documents are represented as text. So we have texts here. Here, all the documents are represented as vectors two, right? So we have uh, vector one, vector two, et cetera, vector N. And here we have document one, document N with text, right? So here the extra step we do is we turn it into vector and then we compare this vector with all the other vectors we have. And then we look for a vector for vectors that are as similar to this one as possible. Here we look for documents that are as similar to this one as possible here. Similarity is how many words in common we have here. The similarities how close these vectors are to each other, right? And the benefit we have here when we turn a query into vector is sometimes we might use, um, so for example, we have a document that is, I just discovered the course, can I still join? Um, but maybe the query from the user I just found out about the program, Can I still sign up? Right? So here none of the words would match, but when we turn it into a vector, um, the way it happens is words are close, semantically close, they would have similar representation. So at the end here, maybe none of the documents will match. But here some documents will still match because vectors they can um, use this um, semantic similarity, right? Um, but usually, so this texts search engines, they're quite old, they're really time proven. They work really well. There are many techniques to actually fight this sort of thing. So like synonym expansion and all, all that stuff, they're very fast and usually in practice they work quite well out of the box. This is a more recent technology. So this one still might require some tuning than the way you choose embeddings. Um, I call that. So it'll require more setup and out of the box it'll not necessarily perform better than this one. So this one you just start a container, you start working and then it'll give you an okay performance. This one might give you great performance and might give you very bad performance. Like you'll have to spend a lot of time tuning it. And the 10th what happens, what specifically for AR people do and for search in general, like for example, this how Google works, Google combines both, right? So then you have some documents here, you have some documents here. And then you come up with, you join basically two things, right? So you look for\n'slidetext': ''\n'from': 'index'\n'score': 13.708840370178223\n'normalizedScore': 0.1370884037017822\n'beforeDatePenaltyScore': 0.1370884037017822\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d9ef5cff79d6f5cdc1'\n'access': 'granted'\n'part_number': 89\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': how they match or, So I'm quite like, it looks like they have the rack flow implemented. Like you upload some files there and then they index them somehow. Yeah, yeah. Same. And then it's uh, Germany, uh, notebook, uh, nope, It's just yet notebook held. LM Google. Lm you said lm. No, that's cool. So it's free to use, it's a way to get to use their Gemini without Mm-Hmm. Because for me personally, uh, for running the business, for running the community where we host courses, I have a lot of process documents in Google, in Google Drive. And in order to find the right process document you need to know, like for this task, you need to use this document. For this task, you need to use this document. And I was thinking of building an application that can easily determine for which task, which document it needs. But looks like this is the solution to this problem because uh, I can just connect it to my Google drive and say, okay, I have this task, which document is the relevant one and it'll just give me, I'll experiment with this. Thank you. Cool. Yes. How you evaluate the performance, the, Okay. mm-Hmm. So I you talk about LMS or about the rack system? Mm-Hmm. So about the rack system, we talked about this. So maybe I can just write it down. So I'll just draw this picture one more time. So we have a user, a user has a question query, then we send this question to, uh, knowledge base, elastic search or some vector database, doesn't matter. Some knowledge base. And then we get back, so this is our query, we get back some documents, right? And then based on these documents, we create a prompt and then prompt goes to NLM and then we get back the results, right? So this is what we do. So this is what we did today. And you can evaluate the whole thing, the whole flow. First go into the document, uh, database and get in the documents and doing the prompt. And for that you use complex metrics like rack, rga, sorry. So then it kind of relates the whole thing. But also in practice, usually you want to optimize this part, how good you retrieve your documents. And then for that you use ranking metrics around king metrics. So these metrics have been around for quite some time. So this is how you evaluate information, IV systems, search systems, this is how you evaluate, uh, recommender systems. 'cause usually in these problems you have like a lot of doc, a lot of things you want\n'slidetext': ''\n'from': 'index'\n'score': 13.466436386108398\n'normalizedScore': 0.134664363861084\n'beforeDatePenaltyScore': 0.134664363861084\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd69'\n'access': 'granted'\n'part_number': 1\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': Workshop: Build a Chatbot with LLM This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information. What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base. Hi everyone. So it's my pleasure to be here. It's actually second time I see familiar faces. Hi. Uh, so last year I was doing here a workshop about ML lops. This year everyone talks about lms. So I will also talk about MLLM or lms and we will cover something called Iraq. Um, let me make sure you see my screen. 'cause what you will need to do now is, um, open this page. So this is the, the URL. I don't know if I can send a URL somehow. If I send it the chat, will people see it? I don't know. But yeah. So this is what we are going to use today. So all the instructions, everything is in this, uh, GitHub Read. Sorry, I can, can you read it? Can I read it? Yeah. Can we make it a little bit more contrast? Is it possible? Yeah, Write it. I will write it here. So, oh good. That's too dark, I guess. But at least now you can type the, the thing. So this is my GitHub, uh, login. You can type github.com/this. And then the first repo, like if you go to my GitHub account and you go to repositories, the first, oh second repo should be this one. And then I see somebody typed it in the chat. So if you also have the chat, you can see there. So was everyone able to open this page? Good. So we're almost done for the day.\n'slidetext': ''\n'from': 'index'\n'score': 13.466436386108398\n'normalizedScore': 0.134664363861084\n'beforeDatePenaltyScore': 0.134664363861084\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a313dca798166d3ba4494'\n'access': 'granted'\n'part_number': 3\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': LLM, you send a prompt and it gives back the answer. And uh, yeah, usually like, what's the day the Germany was united, right? And then it can answer when was it? And for the day workshop, um, uh, we prepared a data set. I'll quickly show you this data set. So I said, I told you already that this was a part, this is a part of the course. And in Datadog Club, this is the com uh, a community that I'm running. We have many free courses. And usually when there is a free course, there are many participants of this course. And there are many, many, many questions. And in our community we have, for our courses, we have these documents with frequently asked questions. So we usually ask people to check this document. So when they have a question, they first check the document. And if they don't see an answer to their question, then they ask this question in Slack. And this document, uh, this is for our data engineering course. This document is 326 pages long, right? So this is how it looks like. So there are all sorts of errors or all sorts of, um, issues people have. And um, yeah, there are answers to these questions too. So this is community maintained. So students, when they come across a problem, they go there. If they don't see the answer, they figure this out and put it there. But you can imagine that this is very difficult to actually find anything in this document, like the more than 300 pages. And what we're going to do today is we're going to take these documents, there are three of them, Oops. So for our data engineering course, for our machine learning course, and for our ML lops course, all documents look similar. And we want to build a bot, a q and a system that given a question from a student will answer based on the content of these documents. And, and if we talk about a usual LLM, so we know that l lms, they are good at answering questions. And let's say this is a student, so a student wants to ask. Um, so a very common question is, I just discovered the course. Is it too late for me to join? So the course for example, our LLM course started last week. Somebody like, maybe somebody of you does not know about the course, they find out about this and they ask this question, okay, I just found the course. This is very interesting. I want to join. And if the we ask this to an LM, maybe I can quickly show you what\n'slidetext': ''\n'from': 'index'\n'score': 13.34361457824707\n'normalizedScore': 0.1334361457824707\n'beforeDatePenaltyScore': 0.1334361457824707\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a3144ca798166d3ba4514'\n'access': 'granted'\n'part_number': 19\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': and then documents. I'll call this variable documents all. Um, we use json a lot F in. So F in is short for file input 'cause we read, and usually if I want to save something, I call this variable F out. So now this documents all contains all these, all these documents that I showed you previously. So, and for example, if I want to see the first, For the first course for the data engineering course, uh, the documents, the first document, This how it looks like, you know, um, the structure here is, um, not the most convenient. So we have course, so it's a bit nested. So we have course, and then inside the course we have uh hmm questions. So we want to un nested. So we want to move this course part here. Uh, I cannot do it, but like we want to, for each document, we want to know which course it belongs to. So then we have a simpler document. Instead of having this nested structure, we'll just have a bunch of document with this field course inside. So we can do it, I can simply go over all the documents, I'll call it documents. I'll go for all the courses course in and like I'm probably typing very fast. There is actually just this bit of code, this piece of code that you can copy and execute that will do what I'm doing. But I just want to type it. So I'll do for course in all the documents. Then I want to go over each document in course documents do, let me just copy it. And then I'll have an extra field here. Coming from course, what's the name? Course? Yeah, course, course. And then I forgot to append documents. Append look. So now we have, um, like almost 1000 documents. And then if we look at each of the document, it looks like that. So we have course which course it belongs to. Then which section of the FAQ, what is the question and what is the answer, right? And then for example, this is like, can I still join the course after it started? Um, this the answer yes, even if you don't register. Um, so now we have all the questions and we want to put them in Elasticsearch. So I already like, let me just type it one more time. Import Elasticsearch. Now we want to create an Elasticsearch client. I think I should do, uh, from Elasticsearch import. What do we have here? Elasticsearch and I call client Elasticsearch. And here hosts, uh, we have HTDP, local host 92 0 0. And um, of course if\n'slidetext': ''\n'from': 'index'\n'score': 13.031683921813965\n'normalizedScore': 0.13031683921813964\n'beforeDatePenaltyScore': 0.13031683921813964\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a3152ca798166d3ba4550'\n'access': 'granted'\n'part_number': 27\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': course, not about all the courses, right? So this is like kind of, um, use case, specific filter, let's say. 'cause um, like what happens is in practice we have a Slack channel, and for each course there is, um, a separate Slack channel, right? So Slack workplace, and then like for each course there is a channel, and then there is a bot that answers questions from the students. And if we know that if a question is coming from this channel, we want to filter only by this course and so on, right? So it's not in the query. Uh, this is, we preset it. There are other questions. Only works on any equal conditions. So There's no Yeah, matching. There is no matching. Exactly. So this is exactly like the work causing in sql. Yes. Say case insensitive. Case insensitive. Yeah. So I think by default it's case insensitive. Um, but yeah, you can, like in Elasticsearch there is this concept of analyzer. So you have this, uh, query, right? So this is a string, and now you need to break it into words, into tokens, right? So when you do this, you get something like, like it's a list. I just, uh, we had, I just discovered. Then Elasticsearch would probably turn it into discover. Um, then it would drop the 'cause like it's, it doesn't really care carry any, like it doesn't have any meaning, right? So we drop the course, um, then, I don't know, maybe it'll also drop. Can, uh, I still join, right? So this is, um, and this process of turning the text into tokens is called, um, so now it's called tokenization in Elasticsearch, it's done by a thing called analyzer. So analyzer kind of analyzes the text and turns it into tokens, and then you can use different analyzers. So the default analyzer, um, is doing this thing. And the default analyzer actually, like here, it'll turn I into a lowercase I, right? So it'll lowercase all the words. Uh, and if you need a specific analyzer, you can check the analyzers that already exist. And if in your case you need to be case sensitive, then you can use a different analyzer that doesn't do that. So this is instead of using a, Yeah, so like I showed here, so this is an embedding what I call vector. So this is embedding, but we follow this path. Follow this path today. Yes. Well, in my experience, this actually works better. Um, vector database, they want to sell you their products.\n'slidetext': ''\n'from': 'index'\n'score': 12.660113334655762\n'normalizedScore': 0.12660113334655762\n'beforeDatePenaltyScore': 0.12660113334655762\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a314eca798166d3ba4545'\n'access': 'granted'\n'part_number': 26\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': can I still join and can I still join the course after the start date, right? So this contains the answer to our question. Um, so it's a bit messy. So what I want to do now is clean it a little bit. So remove all this index ID score. Just keep the, that part, the source. So for that, I'll call it, hmm, how do I call it here? Yeah, I'll, I'll just copy this one, right? So I call it response. So basically we iterate over the, the results and display them. Yes. Client. Yeah. So this is a more readable version of what, what I just showed you, right? So we need to go to hits, then to hits again. And then the document itself is in the source field. And this is how we can show it, right? So these are the top, uh, the most relevant questions from our database. And yeah. Now what we can do is we can just put this into a separate function. I call this code to make it a little bit cleaner. So I'll do it right now. So let's have, I can call it, um, I already have documents, um, uh, relevant docs, I'll call it relevant docs, right? And then I'll just append these relevant docs And we don't need to display it. And then the relevant docs contains the, the results. This is something we want to now, so what we did not right now is we have our query, we send this query to Elasticsearch, Elasticsearch gives back these documents, right? So we almost finished the first step of our rack. So now we just want to clean it a little bit. 'cause it's a messy Jupyter notebook. What we can do right now is just put it in a function that in one function, it takes the user query, goes to database, returns the results, right? And then later we'll use it for building the prompt. Do you have any questions? Yeah, The filter. Can you say it again please? The filter, the filter functionality. So is it somehow can need to select beforehand because it's not part of the, Um, yeah, so the question is where the filter comes from. So imagine a scenario that a student is taking a particular course, right? So when they ask questions about the course, they want to get answers about this particular course, not about all the courses, right? So this is like kind of, um, use case, specific filter, let's say. 'cause um, like what happens is in practice we have a Slack channel, and for each course there is, um,\n'slidetext': ''\n'from': 'index'\n'score': 12.660113334655762\n'normalizedScore': 0.12660113334655762\n'beforeDatePenaltyScore': 0.12660113334655762\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd78'\n'access': 'granted'\n'part_number': 16\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': search in general, like for example, this how Google works, Google combines both, right? So then you have some documents here, you have some documents here. And then you come up with, you join basically two things, right? So you look for semantic uh, similarity, you look for exact keyword matches and then you come up with a list and it's called hybrid search. And this is what in many rack applications and search in general, this is what we use. Yes. Um, semantic, I don't know what you're talking about. What is uh, open AI search? So Azure AI search, Azure AI search. Okay, Options. Mm-Hmm, yeah. Mm-Hmm. Okay. Mm-Hmm Mm-Hmm. Okay, okay. Yeah. So the comment for those who do not who uh, connect online. So there is a tool called Azure AI search and it can do all the searches, which is cool. I have never used it. I did not know about it, but I think it's very convenient 'cause you can just index your documents once. And then actually Elasticsearch has also vector search component will not cover today. But also it's possible to do vector search with Elasticsearch and probably with like some vector databases, they also allow tech search. So at end, like I think like many of these databases at the end we'll be able to do both. With Elasticsearch it's really convenient 'cause you can merge the results immediately there, right? So you can say I want to perform tech search, I also want to perform vector search. I want to take, uh, the score coming from tech search with this coefficient. I want to take the score coming from Vector search with this coefficient, and then it'll provide you like a total score and it'll rank according to this score. So really cool. Okay, so moving on. I have started Elasticsearch. Um, yeah, so one thing I want to do quickly is to check that elastic search is running. I will do curl local host 92 0 0. So what we do is we send a query to Elasticsearch. It's running on this port and then it is sponsored with something. So it means that our elastic search works. So right now I want to have a small break. So if you need to catch up, you can catch up. And I'll also, if you have questions, I come. All right. Okay. So a few people told me that they have problem with Elasticsearch. So when you start Elasticsearch it starts and then it says Elasticsearch has quit unexpectedly. So if you have this problem, what do you need do is, um, give it\n'slidetext': ''\n'from': 'index'\n'score': 12.458775520324707\n'normalizedScore': 0.12458775520324707\n'beforeDatePenaltyScore': 0.12458775520324707\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd74'\n'access': 'granted'\n'part_number': 12\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': Sea. So I need to start a new shell so that we can start using Durant. And right now what it tells us that we need to do dear end follow to approve the content and we can see that we have this thing. Dear end f export open AI key. So right now when we are in this environment, we have the access to the key we saved previously. I am just thinking if I should continue. Everyone seems to be busy doing something. Oh wait, a little bit like if you don't want to install this deer end right now, what you can do is just do export open AI key, then your token in the terminal where you run it. But next time you open this, uh, shell right? It'll not remember the the token, right? But here with different, every time you CD to your directory, it'll automatically load the token and then when you exit it, it'll unload it, unset it. But yeah, if you want to save time, you can just do expert open, I key put the key here and then you are good to go. And for Mac, um, if you use Bash, then in Mac then this is what you do. So you just need to, it's blue or what? On Mac you just do bru install Durant, right? And then if you use Bash, then it would be D Hook B. If you use uh Z Shell, then yeah, you would need to adjust this command Works for me. Okay, put the, This one with this one. Okay. Oh that's right, that's right. Sense even if, Yeah, right. So you are on a remote computer so it feels local 'cause you use Visual Studio Code. But actually like everything we type here is remote. Yeah, this is why I really like Code Spaces. It's such a convenient tool. Like you just click one button and then you have a remote environment. Okay, I think I'll continue. So I'm here right now writing Jupyter Notebook because we did Pipa install and a things and one of them was uh, one of the libraries was Notebook. So right now we have Jupyter Notebook. So I'll execute it Jupyter Notebook and we will start the Jupyter Notebook. So this is where we'll execute all the code go away. And when I do that, so if you do this in code spaces, you see this tab ports. So what happens is, uh, at least with Visual Studio Code Desktop, I don't know about the browser based visual studio code. So with the desktop version, it detects that there is a port on the remote machine and then it forwards this port to\n'slidetext': ''\n'from': 'index'\n'score': 12.458775520324707\n'normalizedScore': 0.12458775520324707\n'beforeDatePenaltyScore': 0.12458775520324707\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd73'\n'access': 'granted'\n'part_number': 11\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': right now, which is called Dear N. So this tool, every time you go into a directory, it plots all the content of this dot uh, NC file. And when you lift the directory, it unloads these variables. So it kind of works similar to bahar C. So bahar C is executed every time you start a B session here, every time you go to a directory, you kinda execute this dot n far C file. This is how I understand it. But I think like there is little difference. They work similarly. Yes. Any Problems with uh, Python three 12? No. And what was the name of the environment is the name of the environment. So, so it's just some it out generates a name so you don't need to worry about that. So you just do peep and Shell, you enter this environment and then you just do mutual things or you pre perfect everything with Peep and Front. So here in this tutorial, actually I prefix everything with people in front, right? So then you can just, if you do it locally, you can just follow along and copy things from here. So you don't need to worry about the environment name. And if you want to check it, um, you will need to type which Python and it'll point to which Python you use. And you will see the name of the directory where this python's Of activity activity. So when you do ppen shell, you kind activate it. Okay? Yeah. And if you prefix all, everything you start with front, it activates the environment, executes the think and deactivates it in one goal. So you don't need to, uh, which is why I like PF. So you don't need to think about like activating and deactivating it. Uh, yeah. Okay. So right now I'm going to install this library, uh, dear n So this is a very convenient library which will, um, read the content of this NC file and it'll read the, the, the variable name which we can use later as a key. So right now I'm installing it. And then one last step. And right now I will need to restart my show. Oh, I think it's reset. Yeah. Uh, let make sure I start a new one. Yeah, so I just started a new show because uh, one of the things I did there was uh, to add this to Bahar Sea. So I need to start a new shell so that we can start using Durant. And right now what it tells us that we need to do dear end follow to approve the content and we can see that we have this thing. Dear end\n'slidetext': ''\n'from': 'index'\n'score': 12.458775520324707\n'normalizedScore': 0.12458775520324707\n'beforeDatePenaltyScore': 0.12458775520324707\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd72'\n'access': 'granted'\n'part_number': 10\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': a new secret key. So I'll call it work shop. And right now I'm going to unplug my um, cable. So you don't see my secret key. 'cause it's secret. You are not supposed to share it it with anyone. 'cause if you accidentally share it with like, let's say you can see I share it with you, you can take a picture and you can already start using my key and then I'll pay for what you do, right? So I don't want that. You also don't want to uh, share your account with anyone. That's why I will unplug it. Okay? So then you click create secret key and you copy the key and I'll put it back. So right now I have the key in my clipboard. Uh, next we want to put this key somewhere, right? So, uh, what I will do right now is I will create a file here. I will call it NR rc. If it's too fast, all the instructions are here. So you will need to create this do m RC file and put this export open. AI what key. And then you put your key here. I will again unplug it. I will put my key there. I will save this file, close it and then you will not see it. And it's very important that you do not commit this file. Like do not accidentally commit it to Git. So for that we'll create gi ignore and put this NFRC file there. So you do not accidentally commit it. 'cause if you, if you commit it, there are scrapers on GitHub, they see that you committed a open AI key, they will take it and they will start using it and then you will get a huge bill. So never, never, never, ever commit your secret keys. Especially open AI to GitHub. Especially if you have a public, uh, rapport. Sorry, yes. The API. Yes. What you should do is you should um, create this NRC file in your project directory and put the file, put the key there. Yes, Between C. So the difference between N and nrc, just the file main. So I I, I don't actually know what RC stands for. You know that you have this uh, BRC Do brc. So I guess do 10 RC is kind of similar. So this is a thing that runs, uh, that like we will use a two right now, which is called Dear N. So this tool, every time you go into a directory, it plots all the content of this dot uh, NC file. And when you lift the directory, it unloads these variables. So it kind of works similar\n'slidetext': ''\n'from': 'index'\n'score': 12.458775520324707\n'normalizedScore': 0.12458775520324707\n'beforeDatePenaltyScore': 0.12458775520324707\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd71'\n'access': 'granted'\n'part_number': 9\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': one. So now you have this green button. Oh yeah. Okay. Try try. Interesting cost basis. Okay. Yeah. So cost basis here. That's It there. And then I just, yeah. Second, welcome. So yeah, turns out it's important to create the rhythm. Me, otherwise you have a different interface. I did not know. Yeah, I didn't see what you did. I have exactly a problem. Three buttons. I dunno What you just did For him. Okay, so what I did is when I created, when we created a new right, we add the readme file. Oh I see. So, Uh, yeah. So then uh, it looks differently. Can you Do that after you've done it without clicking that button? Um, start a new Sorry, Should I just start a new one? Yeah, start a new one. It's easier. Yeah. Like otherwise you would need to do this thing. Okay, I will be moving on then you can catch up later. So, uh, and yeah, do not forget to install, to run p install and take these commands, these libraries. So right now what we are going to do is we're going to get our API key from OpenAI. Hopefully all of you have an account in OpenAI. This is, uh, I don't know how it works. Like, uh, I ask the organizers to say that you should have created it. That's fine. Late. Late, that's fine. Uh, like you don't have to, um, 'cause right now creating an account might take some time, right? We'll later replace uh, OpenAI with uh, uh, open source. But for now, maybe let's try to create an account together, but I don't think you will actually need to put some money there, right? Which is uh, but this is this link platform.openai.com. So I already logged in, but you will need to like, let me see if I log out, what happens. Yeah, so you will have this address and if you don't have an account, you sign up. But I do have an account already. So I click on continue with Google And you will have a window like that. Mm. And here you have the section with API keys and we'll need to like, if you already have a key, you can just use that. If you do not have a key, you need to create a new one. Create a new secret key. So I'll call it work shop. And right now I'm going to unplug my um, cable. So you don't see my secret key. 'cause it's secret. You are not supposed to share it it with anyone.\n'slidetext': ''\n'from': 'index'\n'score': 12.458775520324707\n'normalizedScore': 0.12458775520324707\n'beforeDatePenaltyScore': 0.12458775520324707\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd70'\n'access': 'granted'\n'part_number': 8\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': on isolated environment in code spaces, there's nothing else on that environment. I just use pip. So PIP is the package manager from Python. And when you run PIP install it installs packages to your system. Python. It's a embarra but maybe only person. We're using GitLab. So GitHub, yeah. So how can we create this? Okay, um, how you can create this report. So you go to GitHub, you click new new repository and then you give it some name and then you click create. How do you create a code space? I don't see any. Okay, let me do it one more time. LLM Zoom camp work Shop two, create rapport. Okay, I should have finished last it with uh, read me, but I see already there is start coding with code space. Create a code space. So what I had previously was this button code, right? And then there are multiple tabs, local and code spaces. And when you are here on code spaces, you click here, create code space on mine. If for some reasons it looks like that for you when you did not create a rhythm. So then there is this start coding with code space. So you just click here, But we can just do it without kit. You can do it without GitHub. Yeah. So then I recommend to have, uh, pamp for you. So you just install it with pip install ppen and then I will just do PIP install because I'm doing it in con spaces. But you can do it with Ppen install and then it'll install all the libraries in isolated environment. And after the workshop you can just do Ppen FM and it'll completely remove the environment. So it'll not affect your system Python in any way. Okay. Yes. So do you have this green button? Do you have this gray button? Okay, I can quickly check. So in here, I created This one. Yeah, this one. Oh no, no, no. This is different. So when you create a, can you create apo? Yeah, I can now that's it. New repo. And then you, I i Create this What was created Another one. Okay, start and then click here. Add read me and then type some name here, another one. Then I create Another one. Yeah. And then maybe this one will actually have, Alright, That's, and then yeah, try to create a, So this is the one. So now you have this green button. Oh yeah. Okay. Try try. Interesting cost basis. Okay. Yeah. So cost basis here. That's It there. And then I just, yeah. Second, welcome. So yeah, turns out it's important to\n'slidetext': ''\n'from': 'index'\n'score': 12.458775520324707\n'normalizedScore': 0.12458775520324707\n'beforeDatePenaltyScore': 0.12458775520324707\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd6f'\n'access': 'granted'\n'part_number': 7\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': affect your other, like your other Python installation because we install some libraries and you don't want to interfere with your other libraries. Okay, can I continue Or somebody still, uh, doing something. Okay, so if you use pamp, you can install the, all the packages with this command Pamp install and the rest of the packages for code spaces. 'cause it's already an isolated environment. You don't need to create a virtual environment for Python, you can just do peep install. So I typed install two times. So what we need is a library for, uh, TQ DM is a library is a progress bar. Then Notebook is a Jupyter Notebook. OpenAI is a library for communicating with OpenAI, API and then Elasticsearch is a library for communicating with Elasticsearch and coming back to you. What I, This little picture here. So this will be Elasticsearch where we keep our, like as a knowledge base. This will be Elasticsearch and this one will be open ai. We'll use GPT for O. Um, but also later, uh, in the second part of the workshop I will show you how to replace it with a local LLM. It's something we can run locally. And uh, also we will do something, uh, we will use other open source lamps. Uh, for that we'll need a machine with A GPU. We'll use Google call app for finding that. But for now we start with OpenAI and as knowledge base we'll use Elasticsearch. And I should have pressed this. Okay, so it's installing everything. Uh, that was fast. Uh, Sorry, yes. Just Just so I'm losing a little bit here. Um, uh, so we managed to create GI account right now. That took for til an Hour okay. Code. And uh, I would like to step into where you currently Yes, Dependencies. Yeah, so the question was, uh, I have just created a GitHub account and GitHub repo and all that. What do I do now? So we use this uh, repo, right? So right now we are in preparing, preparing the environment. And right now what I did is I took this command and I ran, I ran PIP install and then this list of libraries. So if you do it locally, not in code spaces, you can do, you can prefix all these commands with ppen instead of pip. But because we do it on already on isolated environment in code spaces, there's nothing else on that environment. I just use pip. So PIP is the package manager from Python. And when you run PIP install it installs packages to your system. Python. It's a embarra but maybe\n'slidetext': ''\n'from': 'index'\n'score': 12.458775520324707\n'normalizedScore': 0.12458775520324707\n'beforeDatePenaltyScore': 0.12458775520324707\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd6e'\n'access': 'granted'\n'part_number': 6\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': And then the repository name, we can call it LLM Rock Workshop. Um, yeah, I already have it. We can just call it um, n for example. But like for you, you don't have to put this n thing there 'cause I already have this, This rabbit here. And like everything else else is optional. No, we are just creating the usual GitHub and I click create repository. This will create an repository. And once you have that, I'll probably wait a little bit. So I don't think there are any questions. I'll be like, there is live chat and there are questions there. And from time to time I'll be monitoring these questions. So I hope everyone has created an account, uh, a report. And then we go to the code button, click click Code Spaces, and click create code space on Main. And now we will have an environment like it's now creating a, an environment, a Linux machine. This environment has Python, it has Docker. That's basically what we need. So we can see that we, so we have this environment in our browser. We have Python 3.1, we have Docker and we'll need to install a bunch of libraries here. What I typically do is I click here and open in Visual Studio Code Desktop. Um, if you don't have Visual Studio Code, you will need to install it. Um, yeah. And once you open it with Visual Studio Code, it'll also ask you to install an ex uh, extension for, um, code spaces. If it doesn't ask you to install an extension, you can click on extensions and type code spaces and then install it. But here I have the environment, the remote environment in my Visual Studio code. Uh, again, I can do dock run, um, world And it works. Then you, you don't have to use this environment. Like this is just a convenient way of doing this. If you have Docker, if you have Python, you're good to go. And maybe I should wait a little bit. It, especially if you need to install a visual Studio code or extensions for those who want to do it locally and without code Spaces, without um, GitHub, you can install Ppen if you don't have it. Ppen is a special tool for Python for dependency management. So this way you can create an isolated environment. So in such a way that when you do something, it doesn't affect your other, like your other Python installation because we install some libraries and you don't want to interfere with your other libraries. Okay, can I continue Or somebody still, uh, doing something. Okay, so if you use pamp, you can\n'slidetext': ''\n'from': 'index'\n'score': 12.458775520324707\n'normalizedScore': 0.12458775520324707\n'beforeDatePenaltyScore': 0.12458775520324707\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd6d'\n'access': 'granted'\n'part_number': 5\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': LNLM. And this is what I did here with charge GPD. So I took a question, I took some context, I created a prompt, I sent it to NLM and then LLM gave the answer. So we have two steps here, two elements. So this is the first element, this is our knowledge base where we keep our knowledge, our context. And this is the second part. An LLM where we use the context from here, we use the question from the student, from the user, we build a prompt and we send it to an LLM to get an answer. So there are two steps, two components, and this thing is called rock. So it's retrieval augmented generation. So usual generation is when you don't have the knowledge base, you just send a question, a prompt and get an answer. So this is just g the G park, this is generation, but we augment this generation with retrieval. So this is the first step. So we retrieve something from our knowledge base, we put it in the prompt, and then we send it to an LLM and get back a better answer. So this is rock and this is what we're going to do today. Okay, cool. I think it's not as cold as in the morning, I How to put it back. Okay, so let's see what we have in our plan. So we'll start with preparing, preparing the environment where we'll all do that. Um, yeah, by the way, you do not need to have a charge GPT account for this workshop, don't worry. So this is just an example and um, we will need, I hope everyone has an um, GitHub account. If you don't then it's fine. From GitHub, what we need is a thing called called spaces. It's a very easy way to get an environment with um, Linux machine with Docker, basically with everything we need for this workshop. If you don't have this but you have Docker installed on your computer, it's okay. So what you just need to have is Python and Docker. We'll use uh, code spaces to get this environment, but you can use your local machine. So let's start. I'll show you how to do this in, uh, code spaces. So we, we'll in GitHub, so what we'll do is we'll create a new repository. So I just typed github.com and then clicked new repository. And then the repository name, we can call it LLM Rock Workshop. Um, yeah, I already have it. We can just call it um, n for example. But like for you, you don't have to put this n thing there 'cause I\n'slidetext': ''\n'from': 'index'\n'score': 12.458775520324707\n'normalizedScore': 0.12458775520324707\n'beforeDatePenaltyScore': 0.12458775520324707\n\n\nTop Vectors:\n'documentId': 'a5cde8d9fe4aa89417ad4e15'\n'chunk_id': '6876d8ff225e92fc1ac7bb43'\n'access': 'granted'\n'part_number': 11\n'total_parts': 11\n'contentType': 'COURSE'\n'title': Tools & Processes for MLOps\n'parentName': AI Systems Done Right: MLOps and Security Strategies\n'language': 'EN'\n'date': '2025-04-07T12:02:11.000Z'\n'abstract': Training a machine learning model is getting easier. But building and training the model is also the easy part. The real challenge is getting a machine learning system into production and running it reliably. In the field of software development, we have gained a significant insight in this regard: DevOps is no longer just nice to have, but absolutely necessary. So why not use DevOps tools and processes for machine learning projects as well?\n'parentId': 'dee36fcd279fe788093b3808'\n'parentDescription': Ready to unlock the secrets of building secure and scalable AI systems? Dive into the essentials of MLOps and MLSecOps, giving you the skills to keep your system safe, efficient, and bulletproof. From the latest in generative AI to mastering scalable workflows, you’ll walk away with the tools and tricks to turn AI into your safest, smartest asset!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Hauke Brammer'\n'chunkSource': 'text'\n'text': [20] https://www.terraform.io [21] https://www.mlflow.org/docs/latest/model-registry.html [22] https://docs.docker.com/registry/deploying/ [23] https://www.kubeflow.org [24] https://www.tensorflow.org/tfx/guide/serving [25] https://www.elastic.co/de/logstash [26] https://prometheus.io [27] https://queue.acm.org/detail.cfm?id=1142065\n'slidetext': ''\n'from': 'vector'\n'score': 0.888577938079834\n'normalizedScore': 0.8766811233309756\n'beforeDatePenaltyScore': 0.888577938079834\n\n'documentId': 'e166d6c9e06d2f15464f870c'\n'chunk_id': '6876df707899910dd58c1056'\n'access': 'granted'\n'part_number': 4\n'total_parts': 16\n'contentType': 'COURSE'\n'title': In the shadows of GenAI: Key Machine Learning Trends Beyond Generative AI\n'parentName': AI Systems Done Right: MLOps and Security Strategies\n'language': 'EN'\n'date': '2025-04-07T12:02:11.000Z'\n'abstract': While we were sitting in the office getting caught by the LLM rise in the last couple of months, in the shadows, many areas of ML made progress, gained traction, and are now ready for broad adoption - but what are these trends? This talk explores critical and industry-agnostic trends in machine learning beyond the GenAI hype.\n'parentId': 'dee36fcd279fe788093b3808'\n'parentDescription': Ready to unlock the secrets of building secure and scalable AI systems? Dive into the essentials of MLOps and MLSecOps, giving you the skills to keep your system safe, efficient, and bulletproof. From the latest in generative AI to mastering scalable workflows, you’ll walk away with the tools and tricks to turn AI into your safest, smartest asset!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Martin Stypinski,Leeanna Seidler'\n'chunkSource': 'text'\n'text': overcome to get it working. Um, and that's I guess why, why most of you are here to really learn and to go beyond the hype and learn what's deeper under the hood. So if you don't get the answers that you're looking for, definitely uh, just ask the speakers after the talk or talk to your peers. You'll be amazed how many of your peers here have had the same struggles. Don't feel stupid. I make so many of those mistakes as well. Feel free to talk to me as well. Uh, you'll see that by talking to people that you learn a lot. The machine learning community is really known to be a verbal community where we share a lot of information verbally. And it's not like in the blog post or in the YouTube videos where everything seems so simple. The reality is just quite different. So talk to everyone. If you didn't get the answer, we introduced another session tomorrow at one 30, the a MA session, which I will host with a few experts, uh, in the panel where you can ask the questions that haven't been answered yet. So come to that session with all the questions that you have. You can really ask anything and we'll answer them for you. Alright, so, um, I want to reiterate one thing. As a program chair, what's really important for me is that we get good feedback from you guys. So yes, please use the app, rate the talks and if you can write a small comment, tell us what you liked. So we make sure that we keep that for next time around. Tell us what you didn't like and what you missed. So we can also use that information. And also the speakers, you can ask them, they also like to get some feedback 'cause they want to do better as well. They don't mind if you write what you think 'cause we all learn from that. And then next time around, we make sure that we have an even better program program than we have today. Alright, enough introductions. So now I want to introduce our keynote speaker where we go beyond the hype of, uh, gen AI and go further than what you think and what you read in the blogs. Please give a warm welcome to Martin Stabinski. Thank you very much for having me here. Um, yeah, so welcome to Berlin and uh, we start with my keynote in the Shadows of Genai. And we all have to admit that in the last couple of years in terms of like language understanding, language retrieval, communication with machines, it's been a long and\n'slidetext': ''\n'from': 'vector'\n'score': 0.8858014345169067\n'normalizedScore': 0.8739046197674141\n'beforeDatePenaltyScore': 0.8858014345169067\n\n'documentId': 'ffe23ecef08c3e1c0b51ef2b'\n'chunk_id': '6876e1ae7899910dd58c1c2b'\n'access': 'granted'\n'part_number': 27\n'total_parts': 27\n'contentType': 'COURSE'\n'title': Intelligent Forms: Spice up your Angular Forms With AI & LLMs\n'parentName': Advanced Angular: Signals, NgRx & AI Forms\n'language': 'EN'\n'date': '2025-05-02T10:49:04.000Z'\n'abstract': Whether it's address data, bank details, or job application information - filling out extensive forms can be a time-consuming and tedious task for users. What if users could simply copy text to the clipboard and the form would fill itself in automatically? A Smart Form Filler makes exactly that possible: with the help of a Large Language Model (LLM), which recognises both the content of the clipboard and the structure of the form, your Angular forms can be filled out fully automatically - even completely locally and offline-capable, if desired. Christian Liebel from Thinktecture shows you how to implement this generative AI feature in Angular.\n'parentId': '5e78d57fd77e42835f9996dc'\n'parentDescription': Level up your Angular skills with this advanced course, highlighting the framework’s evolving features. We will cover cutting-edge topics, from mastering state management with the NgRx Signal store and unlocking AI-enhanced forms’ potential, to revisiting the performance benefits of server-side rendering. You will learn how to effectively use Signals through professional tips and real-world insights, and explore emerging features like httpResource in Angular 19.2. Whether you are scaling enterprise apps or upgrading legacy codebases, this course equips you with the practices to build smarter, faster Angular applications and stay ahead in today’s Angular ecosystem.\n'indexBrandName': 'InternationalJavaScriptCon'\n'indexSeriesName': ''\n'author': 'Christian Liebel'\n'chunkSource': 'text'\n'text': of LMS that you may hit. So, uh, yeah, basically feeding like database, right? Um, uh, a vector database, uh, with the concept of PDF, uh, would probably make sense. At least that's what my colleagues do. Okay. We have no online question. Um, and as we are already over time, uh, I wanna thank you for your attention. Uh, thanks for joining. Uh, see you tomorrow or later for the second session, which will be similar, but more focused on the new APIs that we have and a little bit more in depth in that regard. So thanks for joining and yeah.\n'slidetext': ''\n'from': 'vector'\n'score': 0.8820798397064209\n'normalizedScore': 0.8715501057534782\n'beforeDatePenaltyScore': 0.8820798397064209\n\n'documentId': 'e15ac0b644cd3afbe8bc9675'\n'chunk_id': '687a3152ca798166d3ba4557'\n'access': 'granted'\n'part_number': 20\n'total_parts': 21\n'contentType': 'COURSE'\n'title': Lessons in delivery through hyper growth at youtube and dropbox\n'parentName': Mastering CI/CD Best Practices\n'language': 'EN'\n'date': '2024-07-26T13:32:27.000Z'\n'abstract': Andrew will discuss his experience building continuous delivery systems through the hypergrowth phase of engineering organizations. He will focus on the lessons and principles he has learned at YouTube and Dropbox where he saw systems scale exponentially in headcount and system scale. He will discuss how those principles shaped his opinions of building a great developer experience that 'defaults to fast.'\n'parentId': '4497b71b66108d36f402c74e'\n'parentDescription': Unlock the secrets of seamless software deployment and agile development! This dynamic course dives into CI/CD and DevOps, offering expert insights and practical tips. Discover the essentials of release strategies, continuous delivery, and leveraging AWS for large-scale DevOps transformations. Perfect for leaders and developers alike, elevate your deployment game and drive innovation with this comprehensive guide. Learn how to: Master CI/CD practices: Understand the fundamentals of continuous integration and continuous delivery, and learn how to implement them effectively in your organization. Scale DevOps across your enterprise: Discover strategies for expanding DevOps practices beyond small teams, ensuring consistency and efficiency. Prioritize security in your pipeline: Learn how to build secure software by integrating security into your CI/CD pipeline. Align DevOps with business objectives: Explore the role of DevOps in achieving organizational goals and delivering value to customers. Whether you're a developer, operations engineer, or business leader, this course provides valuable insights and practical guidance to help you excel in the DevOps landscape. Join us to unlock the full potential of DevOps and drive your organization's success!\n'indexBrandName': 'DevOpsCon'\n'indexSeriesName': ''\n'author': 'Andrew Fong'\n'chunkSource': 'text'\n'text': spending, like we've built this, um, we've actually been deploying this in production. We have some pretty large users on it. I think Netflix has also done some really good work in this space. My, I I think at this point the um, what we're starting to see is like Argo has done like a great job of V one in the, uh, in the cobe space and that there is an extension that needs to happen as you think about developer experience and the rest. Um, because you probably also wanna give Sam ux ui, the more complicated your delivery systems get, probably the less command line tooling you're gonna be able to use for it as well. People need to be able to reason through it and see like what states are. Um, and so we think there's like a pretty big jump here that's gonna happen in the next 24 months around how delivery is managed. Um, and we also completely separate talk, but like I'm in the camp at CD is very different than CI as well. Um, very different use case and also very different technology stack that needs to implement at I see. Smile on this one. Um, so that's, uh, I'll conclude there. You can take a look at some resources here. Um, some of this stuff, uh, is kind of interesting. So my co-founder did a 4 million line Python migration, um, monolith into like a Lambda service. It takes a lot of these principles. Um, Netflix has our managed delivery project, and then we kind of take this entire talk is unbundled as a, uh, as a series of a series of architecture, um, architecture, uh, overviews. Um, so feel free to take a picture or whatnot. And then I'm gonna put the last slide up here, which is like ask for feedback. Cool. Can you go back to one? Yep. I can go back to one side. Yes. And I'm checking over here. Check nothing on this side, but happy to take questions. Happy to like open the floor up discussion. This connect. So this idea of convergence, does this connect at all to s from ideas? Does this connect Or, Um, so the question is, is how does this connect to Mark Burgess's, uh, promise theory stuff? Um, oddly there's probably a couple things. If you look at the actual design of this, it's very similar to what Mark has done. I've actually talked to him a bunch about it. Um, and then I think the other part is, uh, if you were to dig into our actual product, there's a lot of pieces that are influenced\n'slidetext': ''\n'from': 'vector'\n'score': 0.8924815654754639\n'normalizedScore': 0.866615582791547\n'beforeDatePenaltyScore': 0.8924815654754639\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36ceef5cff79d6f5cd6b'\n'access': 'granted'\n'part_number': 3\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': a data set. I'll quickly show you this data set. So I said, I told you already that this was a part, this is a part of the course. And in Datadog Club, this is the com uh, a community that I'm running. We have many free courses. And usually when there is a free course, there are many participants of this course. And there are many, many, many questions. And in our community we have, for our courses, we have these documents with frequently asked questions. So we usually ask people to check this document. So when they have a question, they first check the document. And if they don't see an answer to their question, then they ask this question in Slack. And this document, uh, this is for our data engineering course. This document is 326 pages long, right? So this is how it looks like. So there are all sorts of errors or all sorts of, um, issues people have. And um, yeah, there are answers to these questions too. So this is community maintained. So students, when they come across a problem, they go there. If they don't see the answer, they figure this out and put it there. But you can imagine that this is very difficult to actually find anything in this document, like the more than 300 pages. And what we're going to do today is we're going to take these documents, there are three of them, Oops. So for our data engineering course, for our machine learning course, and for our ML lops course, all documents look similar. And we want to build a bot, a q and a system that given a question from a student will answer based on the content of these documents. And, and if we talk about a usual LLM, so we know that l lms, they are good at answering questions. And let's say this is a student, so a student wants to ask. Um, so a very common question is, I just discovered the course. Is it too late for me to join? So the course for example, our LLM course started last week. Somebody like, maybe somebody of you does not know about the course, they find out about this and they ask this question, okay, I just found the course. This is very interesting. I want to join. And if the we ask this to an LM, maybe I can quickly show you what happens. So I will be using charge GPD. So I just discovered the course. How many does anyone else not have an account in charge G pt, why you're missing out. Uh, can I still join? Right? So we ask alem question\n'slidetext': ''\n'from': 'vector'\n'score': 0.8919334411621094\n'normalizedScore': 0.8660122415832154\n'beforeDatePenaltyScore': 0.8919334411621094\n\n'documentId': '31a17545468e0020a46cfb6e'\n'chunk_id': '687a291c96f7bf47ed452a3b'\n'access': 'granted'\n'part_number': 17\n'total_parts': 39\n'contentType': 'COURSE'\n'title': Workshop: AI-Powered Node.js Development with OpenAI\n'parentName': Unleash the Power of AI in Web Development\n'language': 'EN'\n'date': '2024-12-19T14:10:00.000Z'\n'abstract': In this immersive full-day workshop, you'll discover how to harness the power of OpenAI's advanced language models and seamlessly integrate them into your Node.js applications. Through hands-on projects, you'll learn to build intelligent chat interfaces, create advanced search systems, develop question-answering applications, and explore cutting-edge techniques like AI-driven function calling. Gain practical experience in leveraging AI to revolutionize your Node.js development workflow.\n'parentId': '33b5c3a3690ff71983ad2b73'\n'parentDescription': In this course, you’ll explore the intersection of AI and JavaScript, learning how to integrate machine learning models, enhance user experiences, and build smarter, more intuitive web applications. Drawing on insights from industry experts, this course equips you with the tools and techniques to harness AI’s potential in JavaScript-based development. You’ll start with a solid introduction to AI and its role in web development, diving into expert articles and insightful sessions from our iJS Conference. Then, you’ll engage in practical hands-on workshops focused on real-world applications, such as AI-powered Node.js development and building AI applications with JavaScript. By the end of the course, you’ll have a better understanding of how to integrate AI into your own web projects and create innovative web experiences. Join us and take your JavaScript skills to the next level!\n'indexBrandName': 'InternationalJavaScriptCon'\n'indexSeriesName': ''\n'author': 'Nir Kaufman'\n'chunkSource': 'text'\n'text': you explain And let me let, let's continue I'm gonna create a new file called search I'm gonna pay some mock data MOOC data and Prisma, And this is for the people here who don't want to mess semantic search, uh, feature that we are gonna build. create a, a util called uh, db. We already got it here. this is gonna answer your question, already know about my candidate. of my candidates from the database. and you need it, it's gonna generate type three And if I want to, uh, to uh, use, uh, uh, You see candidate, import candidate and give me very, very nice experience. Period. Cool. We want to take this data what is semantic search? of bass. No. Alright, What, let's talk about search. Do I need AI and LN for this? Why not? about this old recruiting process. Do I need AI technologies to do this? The answer is if I like to, to use something because similarity search mean, with strong, uh, uh, background in what, If you look through this entire, uh, this entire candidate, It's not exist here in similarity search. and say, well, LA's got a strong background in Python, I'm go ahead and give you Alice as a result It's similar enough to what you're looking for. because it's not something specific. There is a process, a process that's called embedding, save it as a rep, as a vector representation and then you're gonna play with it. now the first example, we introduced a simple, of a machine learning model called embedding model. So you get an unstructured data, I think that this is the most visualized way that I, Here. You see like, kind of, well, I tried Actually, I need to fix it a little bit. but if like a hotel is, uh, 1.9 Think about in this example, three dimensional graph, but basically we're talking about groups. And then I want to look, as a minus two and two. I mean, literally, it's close to this vector, right? minus 1.9 and one. but this is the vector representation. well, they're similar. How many of you seen the movie? Uh, interstellar. with where is inside the cataract, When we're talking about vectors, we've got like, layers on layers of multidimensional, uh, um, around the graph dependence on the vector representation. shows two. but I want you to think about groups. and we use the embedding model in order sits on a structure that we are going to use in this context. I can get results back the term that you searched for if you\n'slidetext': ''\n'from': 'vector'\n'score': 0.8825041055679321\n'normalizedScore': 0.8646395517266079\n'beforeDatePenaltyScore': 0.8825041055679321\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d3ef5cff79d6f5cd9b'\n'access': 'granted'\n'part_number': 51\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': the old query. Now we can try with something else. Um, do you remember what the query was? Yeah. Um, can I attend the course that started the days ago? Can I attend the course? That started a few days ago, so hopefully it works. Yeah, well, I don't know, maybe there is still another error, but it seems to work. Let me check the comments in the chat. As far as I recall, any API access to open AI requires prepaid credit free is only interactive usage. Okay. Maybe has anyone tried Grok or something else? How is it going with Grok? Yeah. Okay. How difficult was it to Very Easy. Very easy. Uhhuh. So like, uh, with Gro, do you, can you use OpenAI client or you need to install their own library? Uh, we have their own library. So you did keep install Grok? Right. Okay. And just Changed the import, change the client and then surprisingly their a p is almost the same as Open. Mm-Hmm. So don't Have to change much. Okay. I'm unsurprised. 'cause like, I guess they set the standard and everyone else copies that. Um, any more questions or We go have lunch now. So when do we come back? Like in one hour or like, I guess one hour is enough. Can you say it again? The program Is one 30. One 30. So one hour. One hour. Okay. So I'll just, I see that some of you still have questions, so if you don't, you can leave. We'll come back to in one hour, one 30 and if you have questions, I can answer them. Yeah. Is it something, a particular Question on, On Uhhuh? Okay. Does anyone have like generic general questions? Yeah. If you Have any insight as to why this works, uh, with you. Rephrase the Question, Uhhuh. 'cause Uh, is there cement similarity actually? Mm-Hmm. So the question is, uh, if I have any ideas why when I rephrased it still works. And I think like if we look at the documents that, um, we get, and let me check here, um, look out query, because I think it, uh, Elasticsearch can still retrieve relevant documents. Do I still have now here? I don't remember if I fix it because like it still has, uh, things like course and started. I think this is, uh, um, even though like you see that this is not the most relevant one, um, yeah. So this is like fourth most relevant one. It's still there. So for an LLM, it's uh, still enough?\n'slidetext': ''\n'from': 'vector'\n'score': 0.8905099034309387\n'normalizedScore': 0.8645887038520448\n'beforeDatePenaltyScore': 0.8905099034309387\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d9ef5cff79d6f5cdc4'\n'access': 'granted'\n'part_number': 92\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': part in installation from the rest and see what happens. And I, I think that's all for me. So thanks a lot for attention. I know it was fast sometimes, uh, but hopefully you can come back to the notes. Also. Let me um, mention it one more time. So right now there is a course about that. So this thing that I described here that I explained. So this was module one of the course and module two. So the course is called LLM Zoom camp. And this course just started. So if you want to learn more about vector search or how you orchestrate track or how you do monitoring, what are the best practices? The course just started. You can join it. And also if you later have questions about this workshop, this particular workshop, you can just join the course and ask these questions in the course channels. 'cause the content we have here in the first and second one and what they presented here are the same. So there are students who are doing this. There are um, instructors. I also monitor, uh, this channel. So if you have any questions about this workshop, maybe you will have later this week or this month or next year. I dunno. You can always come to our slack and ask questions about this. And in order to do that, uh, you, yeah, you can sign up to the slack and then you join the course channel. And then if you have any questions, this is the best place to get answers. Yep. Light lectures or It's prerecorded. So for example, everything I showed here is actually split into module. Yeah. This, this talk here is recorded, right? That we can Have access to. Uh, that's not a question for me. I don't know. So maybe you know if this is recorded or not. Sorry, the question is if this content is recorded? Yes. Is recorded Website. Okay, that's good that I deleted my hugging face. Um, talking. Yeah. 'cause I de deactivated it. So if you watch this in recording, it no longer works. Okay, well I guess that's it. Thanks for attention or more. Yeah, so we still have time to um, it's supposed to finish at five, right? So we still have roughly one hour. So I can stick around if you need help. Yeah. Yes. Do You have some agents? Agents not in the course and not here. So an agent is a thing that can do actions. So for example, you can, so in addition to that, mm. And so let's say you want to build an agent that traits on\n'slidetext': ''\n'from': 'vector'\n'score': 0.8902672529220581\n'normalizedScore': 0.8643460533431642\n'beforeDatePenaltyScore': 0.8902672529220581\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d2ef5cff79d6f5cd90'\n'access': 'granted'\n'part_number': 40\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': yeah, put them. So for this LLM I'll create a function, Uh, here. So, and then instead of printing, I just return it. So this is our LLM and this will be our build prompt function that I'll put here, build prompt, and then we have query and um, context documents, right? So we have context, we have prompt, and we return it. So, and then I'll remove this one. Okay? Just everything. I want to have everything in one place. So everything is related to building pros here. Then we have the l, m and the rock. So what should I use here? Can I attend the course that started? Can I attend the course that started a few days ago? No, you can, any other queries? Uh, let's try, how can I install Kafka? How can I install Kafka? I don't know actually if we have an answer to that question. Um, let me check. Kafka. Yeah, we'll have we have some, well we have 74 questions with Kafka. Yeah, so probably we do not have that, the answer to that question in the context. Let me print it so it's easier to read, right? So it says that it looks like there's no answer to that question, but here's how you can do it, right? So it's very nice of, uh, this LLM to tell us about that, but it might not tell us and just strictly go to to that part. And then we have no idea if this is coming from the context or is just coming up with answer yourself. So in this case, it looks correct. Well, maybe it's not, but like it looks legit. Uh, but we can also tweak out prompt to say like, um, let's try, if the provided context doesn't contain the answer, say I don't know, right? So we can try that and see what happens. Yeah, it's good. Is there any other way much confidence I cannot hear. Can you please? Confidence level Is, were there any way to add confidence level? Not that I know of here, right? So when you use open source cell lamps, usually somehow you can get the confidence levels. Um, but yeah, I dunno, they Put confidence on the query itself. Yeah, probably. Do you know how to do that? I guess it's the elastic search feature, Right? Ah, okay. So for those who are listening online, um, the comment was that you can put the confidence to the prompt, right? In indeed elastic search returns to the query, uh, score. So here we don't do this. Um, but you can say also you can\n'slidetext': ''\n'from': 'vector'\n'score': 0.8896417021751404\n'normalizedScore': 0.8637205025962464\n'beforeDatePenaltyScore': 0.8896417021751404\n\n'documentId': 'fb78cf2215e99d64eba7d65c'\n'chunk_id': '687a3263ca798166d3ba5078'\n'access': 'granted'\n'part_number': 14\n'total_parts': 26\n'contentType': 'COURSE'\n'title': Expertenwissen trifft AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'DE'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': Wir stellen unsere neue AI Lösung vor und zeigen wie du unsere RAG Technologie nutzt, um klare Antworten auf deine Software-Entwicklungs Probleme bekommst. Zusätzlich lernst du von Dr. Pieter Buteneers was RAG Modelle so besonders macht und wie du sie in deine Projekte einbinden kannst.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Dr. Pieter Buteneers'\n'chunkSource': 'text'\n'text': weil wir jetzt sehr viel über Machine Learning geredet haben, kann ich, wie gesagt, direkt daran teilnehmen Und Empfehlen. Deine Stimme versagt gerade, deswegen gehe ich jetzt gerade nochmal in den Chat. Und da haben wir nochmal eine weitere Frage. Wie sieht es mit abgelaufenen Inhalten aus? Kurse, Tutorials, die über die Plattform nicht mehr verfügbar sind. Sind diese über AI noch zu finden? Zu finden, ja. Allerdings musste man auch auf der Konferenz sein, um Zugriff darauf haben. Man wird dann in der Regel darauf verwiesen, was der neue Inhalt ist und kann dann auf der nächsten Buster, der nächsten MLCon, der nächsten Jax teilnehmen. Also es ist kein Loophole, um diesen Content leider zur Verfügung zu stellen. Allerdings wird sie in die Suche und damit in die Generierung der Antwort berücksichtigt. Also man freundet sich vielleicht auch nochmal so vorstellen, genau. Also nochmal erstmal als Abonnent und Abonnentin von Entwickler.de habt ihr unfassbar viele Inhalte direkt zur Verfügung. kommt ihr sehr, schnell ran, zum Beispiel über diese intuitive Mechanismus des Fragestellens von Entwickler.de. Entwickler Intelligence, verzeihung, das ist das eine. Und dann auf der anderen Seite, genau, gibt es bei uns eben diese Add-on-Produkte. Das heißt eben auch Seminare, Konferenzen teilnehmen, Themen teilnehmen, die ich natürlich nochmal separat buchen kann. Und ich glaube auch eine Stärke liegt halt auch daran, wenn ich sage, ok, jetzt bin ich in meinem Team und wir sind in der Situation, dass wir sagen, hey, wir machen schon, mit Kubernetes haben wir einen gewissen guten Grad an Erfahrung aufgebohrt. Und jetzt sind wir aber in der Lage, dass wir sagen, Security ist es nochmal eine spezielle Challenge, die wir nochmal weiter irgendwie aufbauen wollen. Dann kann ich auch nicht sagen, hey, zeig mir mal, was gibt es eigentlich auch für Verschulungen, für Trainings etc. Wie komme ich weiter mit dem Thema Kubernetes Security, einfach nur als Beispiel hier ausgewählt. Und in dem Moment genau, kann ich dann auch eben wiederum Entwickler Intelligence verwenden, um mich da hinzuführen, was meine nächsten Steps sein\n'slidetext': ''\n'from': 'vector'\n'score': 0.8868052363395691\n'normalizedScore': 0.862021678819909\n'beforeDatePenaltyScore': 0.8868052363395691\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a3152ca798166d3ba4550'\n'access': 'granted'\n'part_number': 27\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': course, not about all the courses, right? So this is like kind of, um, use case, specific filter, let's say. 'cause um, like what happens is in practice we have a Slack channel, and for each course there is, um, a separate Slack channel, right? So Slack workplace, and then like for each course there is a channel, and then there is a bot that answers questions from the students. And if we know that if a question is coming from this channel, we want to filter only by this course and so on, right? So it's not in the query. Uh, this is, we preset it. There are other questions. Only works on any equal conditions. So There's no Yeah, matching. There is no matching. Exactly. So this is exactly like the work causing in sql. Yes. Say case insensitive. Case insensitive. Yeah. So I think by default it's case insensitive. Um, but yeah, you can, like in Elasticsearch there is this concept of analyzer. So you have this, uh, query, right? So this is a string, and now you need to break it into words, into tokens, right? So when you do this, you get something like, like it's a list. I just, uh, we had, I just discovered. Then Elasticsearch would probably turn it into discover. Um, then it would drop the 'cause like it's, it doesn't really care carry any, like it doesn't have any meaning, right? So we drop the course, um, then, I don't know, maybe it'll also drop. Can, uh, I still join, right? So this is, um, and this process of turning the text into tokens is called, um, so now it's called tokenization in Elasticsearch, it's done by a thing called analyzer. So analyzer kind of analyzes the text and turns it into tokens, and then you can use different analyzers. So the default analyzer, um, is doing this thing. And the default analyzer actually, like here, it'll turn I into a lowercase I, right? So it'll lowercase all the words. Uh, and if you need a specific analyzer, you can check the analyzers that already exist. And if in your case you need to be case sensitive, then you can use a different analyzer that doesn't do that. So this is instead of using a, Yeah, so like I showed here, so this is an embedding what I call vector. So this is embedding, but we follow this path. Follow this path today. Yes. Well, in my experience, this actually works better. Um, vector database, they want to sell you their products.\n'slidetext': ''\n'from': 'vector'\n'score': 0.8867948055267334\n'normalizedScore': 0.8620112480070733\n'beforeDatePenaltyScore': 0.8867948055267334\n\n'documentId': 'fd1e5ad8ac352c46d9f10793'\n'chunk_id': '687a32e1ca798166d3ba559c'\n'access': 'granted'\n'part_number': 18\n'total_parts': 19\n'contentType': 'COURSE'\n'title': Does size matter? The Joys and Pains of K8S (Auto)Scaling\n'parentName': Set Sail and Build Fast With Kubernetes\n'language': 'EN'\n'date': '2024-07-26T13:14:14.000Z'\n'abstract': Autoscaling is one of K8S 'magic' features that folks get especially excited about. But beyond he technical details - autoscaling is a business concern. In the end it all boils down to balancing efficiency with availability. How much resources do our containers really need? When do we add more pods? How many pods is too many? Do node sizes matter? Let's see how to scale Kubernetes the scientific way.\n'parentId': '8c36f1c96a126627b0fcae02'\n'parentDescription': Kubernetes has revolutionized the way we deploy and manage containerized applications. This course delves deep into the core concepts of Kubernetes, focusing on critical areas: scaling, security, and architecture. Learn how to: Optimize resource utilization: Master the art of scaling Kubernetes applications to meet varying workloads. Discover advanced techniques beyond basic horizontal pod autoscaling. Fortify your Kubernetes environment: Understand the security challenges posed by cloud-native environments. Learn best practices to protect your clusters and data. Design scalable and resilient architectures: Gain insights into building robust Kubernetes-based systems. Explore architectural patterns and best practices. Whether you're a seasoned Kubernetes engineer or new to the platform, this course will equip you with the knowledge and skills to create high-performing and secure applications. Join us to explore the world of Kubernetes and become a true master of the platform!\n'indexBrandName': 'DevOpsCon'\n'indexSeriesName': ''\n'author': 'Anton Weiss'\n'chunkSource': 'text'\n'text': become a cluster whisperer, just like me, here is a link to a preliminary signup form. We'll be working on certification this summer, and it's going to be released in the next month or two. Uh, of course it's not called a cluster whisper certification, but instead it's called Kubernetes Optimization Essentials. So if you want to really become a Kubernetes optimization expert, definitely sign up for that and we'll be happy to, uh, notify you once the certification becomes official. We're going to launch it free at least initially. So definitely sign up for that, scan the code, and now, okay. Ignore this QR code. It doesn't belong here. Okay, so the, the QR code on this slide don't scan that it belongs to another talk I did forgot to remove it, but, uh, know that once you get the auto scaling and resizing correctly, you can do this trick. Okay? So you get the right size guitar that's correctly adapted to your needs. Sometimes you want ukulele for a folk song and sometimes you want an actual acoustic guitar to bang out some rock anthems. Okay? And with that, I'll be happy to take your questions. Hello? Hello. Alright, anybody have any questions? I will come to you with the microphone. You can ask him directly, no questions. Oh, we got a question. One second. So for, uh, VPA mm-Hmm, we, most of us know the, the native feature is pretty dumb. So do you have any alternative, like maybe, uh, like some third party tool or something that would be a bit more smart than the current VPA? Well, as I said, again, uh, you know, without doing a vendor pitch, but doing a vendor pitch, uh, I represent perfect scale here and that's exactly what perfect scale does, right? We provide also the recommendations and the automation. So, and we, we do this much smarter than BPA does it, and thanks for that question. Thank you. Uh, in general, we, we do, uh, things we do better than VPA, you know, in a nutshell, we do, uh, uh, relate to historical trends. We do, uh, identify memory leaks. We are revision aware and we are also, uh, aware of the amount of resources the nodes have. 'cause that's, for example, something also VPA doesn't know how to do it. And also you don't need to define, uh, VPA resource for each new workload. We are smart enough to just watch your workloads once we're installed, Right? Yeah, I think that answered the question. Anybody\n'slidetext': ''\n'from': 'vector'\n'score': 0.8875333070755005\n'normalizedScore': 0.8616666312148967\n'beforeDatePenaltyScore': 0.8875333070755005\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a3169ca798166d3ba4636'\n'access': 'granted'\n'part_number': 51\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': documents to put them in the context right then. Um, yeah, we build a prompt based on these documents and get back the answer. And actually when you were asking to try a different query, uh, because I had a problem, um, in my code, it was actually using the old query. Now we can try with something else. Um, do you remember what the query was? Yeah. Um, can I attend the course that started the days ago? Can I attend the course? That started a few days ago, so hopefully it works. Yeah, well, I don't know, maybe there is still another error, but it seems to work. Let me check the comments in the chat. As far as I recall, any API access to open AI requires prepaid credit free is only interactive usage. Okay. Maybe has anyone tried Grok or something else? How is it going with Grok? Yeah. Okay. How difficult was it to Very Easy. Very easy. Uhhuh. So like, uh, with Gro, do you, can you use OpenAI client or you need to install their own library? Uh, we have their own library. So you did keep install Grok? Right. Okay. And just Changed the import, change the client and then surprisingly their a p is almost the same as Open. Mm-Hmm. So don't Have to change much. Okay. I'm unsurprised. 'cause like, I guess they set the standard and everyone else copies that. Um, any more questions or We go have lunch now. So when do we come back? Like in one hour or like, I guess one hour is enough. Can you say it again? The program Is one 30. One 30. So one hour. One hour. Okay. So I'll just, I see that some of you still have questions, so if you don't, you can leave. We'll come back to in one hour, one 30 and if you have questions, I can answer them. Yeah. Is it something, a particular Question on, On Uhhuh? Okay. Does anyone have like generic general questions? Yeah. If you Have any insight as to why this works, uh, with you. Rephrase the Question, Uhhuh. 'cause Uh, is there cement similarity actually? Mm-Hmm. So the question is, uh, if I have any ideas why when I rephrased it still works. And I think like if we look at the documents that, um, we get, and let me check here, um, look out query, because I think it, uh, Elasticsearch can still retrieve relevant documents. Do I still have now here? I don't remember if I fix it because like it still has, uh,\n'slidetext': ''\n'from': 'vector'\n'score': 0.8862370252609253\n'normalizedScore': 0.8614534677412652\n'beforeDatePenaltyScore': 0.8862370252609253\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a313dca798166d3ba4494'\n'access': 'granted'\n'part_number': 3\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': LLM, you send a prompt and it gives back the answer. And uh, yeah, usually like, what's the day the Germany was united, right? And then it can answer when was it? And for the day workshop, um, uh, we prepared a data set. I'll quickly show you this data set. So I said, I told you already that this was a part, this is a part of the course. And in Datadog Club, this is the com uh, a community that I'm running. We have many free courses. And usually when there is a free course, there are many participants of this course. And there are many, many, many questions. And in our community we have, for our courses, we have these documents with frequently asked questions. So we usually ask people to check this document. So when they have a question, they first check the document. And if they don't see an answer to their question, then they ask this question in Slack. And this document, uh, this is for our data engineering course. This document is 326 pages long, right? So this is how it looks like. So there are all sorts of errors or all sorts of, um, issues people have. And um, yeah, there are answers to these questions too. So this is community maintained. So students, when they come across a problem, they go there. If they don't see the answer, they figure this out and put it there. But you can imagine that this is very difficult to actually find anything in this document, like the more than 300 pages. And what we're going to do today is we're going to take these documents, there are three of them, Oops. So for our data engineering course, for our machine learning course, and for our ML lops course, all documents look similar. And we want to build a bot, a q and a system that given a question from a student will answer based on the content of these documents. And, and if we talk about a usual LLM, so we know that l lms, they are good at answering questions. And let's say this is a student, so a student wants to ask. Um, so a very common question is, I just discovered the course. Is it too late for me to join? So the course for example, our LLM course started last week. Somebody like, maybe somebody of you does not know about the course, they find out about this and they ask this question, okay, I just found the course. This is very interesting. I want to join. And if the we ask this to an LM, maybe I can quickly show you what\n'slidetext': ''\n'from': 'vector'\n'score': 0.885270357131958\n'normalizedScore': 0.8604867996122979\n'beforeDatePenaltyScore': 0.885270357131958\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd83'\n'access': 'granted'\n'part_number': 27\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': So Slack workplace, and then like for each course there is a channel, and then there is a bot that answers questions from the students. And if we know that if a question is coming from this channel, we want to filter only by this course and so on, right? So it's not in the query. Uh, this is, we preset it. There are other questions. Only works on any equal conditions. So There's no Yeah, matching. There is no matching. Exactly. So this is exactly like the work causing in sql. Yes. Say case insensitive. Case insensitive. Yeah. So I think by default it's case insensitive. Um, but yeah, you can, like in Elasticsearch there is this concept of analyzer. So you have this, uh, query, right? So this is a string, and now you need to break it into words, into tokens, right? So when you do this, you get something like, like it's a list. I just, uh, we had, I just discovered. Then Elasticsearch would probably turn it into discover. Um, then it would drop the 'cause like it's, it doesn't really care carry any, like it doesn't have any meaning, right? So we drop the course, um, then, I don't know, maybe it'll also drop. Can, uh, I still join, right? So this is, um, and this process of turning the text into tokens is called, um, so now it's called tokenization in Elasticsearch, it's done by a thing called analyzer. So analyzer kind of analyzes the text and turns it into tokens, and then you can use different analyzers. So the default analyzer, um, is doing this thing. And the default analyzer actually, like here, it'll turn I into a lowercase I, right? So it'll lowercase all the words. Uh, and if you need a specific analyzer, you can check the analyzers that already exist. And if in your case you need to be case sensitive, then you can use a different analyzer that doesn't do that. So this is instead of using a, Yeah, so like I showed here, so this is an embedding what I call vector. So this is embedding, but we follow this path. Follow this path today. Yes. Well, in my experience, this actually works better. Um, vector database, they want to sell you their products. That's why most of the content you see on the internet these days by the vendors, they're about vector databases. Better If it's very structured document. Yeah, yeah. Pretext. Mm-Hmm. Would be hard to No, no, no. Like Elasticsearch is a, a search engine, right?\n'slidetext': ''\n'from': 'vector'\n'score': 0.8857781291007996\n'normalizedScore': 0.8598569295212714\n'beforeDatePenaltyScore': 0.8857781291007996\n\n'documentId': 'fd1e5ad8ac352c46d9f10793'\n'chunk_id': '687a32daca798166d3ba552a'\n'access': 'granted'\n'part_number': 2\n'total_parts': 19\n'contentType': 'COURSE'\n'title': Does size matter? The Joys and Pains of K8S (Auto)Scaling\n'parentName': Set Sail and Build Fast With Kubernetes\n'language': 'EN'\n'date': '2024-07-26T13:14:14.000Z'\n'abstract': Autoscaling is one of K8S 'magic' features that folks get especially excited about. But beyond he technical details - autoscaling is a business concern. In the end it all boils down to balancing efficiency with availability. How much resources do our containers really need? When do we add more pods? How many pods is too many? Do node sizes matter? Let's see how to scale Kubernetes the scientific way.\n'parentId': '8c36f1c96a126627b0fcae02'\n'parentDescription': Kubernetes has revolutionized the way we deploy and manage containerized applications. This course delves deep into the core concepts of Kubernetes, focusing on critical areas: scaling, security, and architecture. Learn how to: Optimize resource utilization: Master the art of scaling Kubernetes applications to meet varying workloads. Discover advanced techniques beyond basic horizontal pod autoscaling. Fortify your Kubernetes environment: Understand the security challenges posed by cloud-native environments. Learn best practices to protect your clusters and data. Design scalable and resilient architectures: Gain insights into building robust Kubernetes-based systems. Explore architectural patterns and best practices. Whether you're a seasoned Kubernetes engineer or new to the platform, this course will equip you with the knowledge and skills to create high-performing and secure applications. Join us to explore the world of Kubernetes and become a true master of the platform!\n'indexBrandName': 'DevOpsCon'\n'indexSeriesName': ''\n'author': 'Anton Weiss'\n'chunkSource': 'text'\n'text': bigger and faster. Discs, memory, CPU, you name it, network. And, um, um, we started, uh, being, uh, less frugal, right? By the way, if you're actually actually interested in how it has evolved during the ages and how all that changes our perception of space and time, make sure to watch, uh, a documentary series by none other than Mark Burgess, the creator of CF Engine, and the person who actually defined desired state configuration as we know it today much before, uh, Kubernetes came to life. Uh, it's called Bigger, fast, faster, smarter. Look it up on YouTube. I'll have a slide, uh, that refers to it later on. Uh, but back to our story. So information systems kept growing. Our apps became huge and heavy. We needed more servers. We needed bigger servers. But then, you know, the cloud came along, right? And, uh, with it, the promise of infinite elasticity, right? So, uh, for the cloud providers, the size does matter because for them, the bigger is better, right? You need more servers, we'll provision them for you. We'll just put them on your bill, right? You have more traffic, no problem. We'll put it on your bill. And this easiness of access to resources has made us carefree. When I ask a developer, how much memory does your application need in production, they usually tell me, I don't know, give it two jigs, three jigs. And when I tell them, well, that's quite a lot. They say, so what provision larger machines, uh, just a shrug, but travel is, uh, size does matter, and bigger isn't necessarily better. And also smaller isn't necessarily better. And the question is, how do we find the right size for our application? And then the question we'll be looking into today, specifically from the angle of Kubernetes. But first, uh, let me introduce myself, and with that, I'll go back to my screen sharing and share my slides. So let's do that, hopefully. Okay, no, that's not the slides that my slack. Okay, so now you should be able to see my slides, and I'll put myself in the small corner here. Okay? So, uh, this is the documentary series that I was referring to. Definitely watch that to better understand how it is evolving. And this is me. So I am an, people usually call me an, but, uh, my full name is Anton Vice. You can find me on the x, uh, social, uh, network, previously known as Twitter.\n'slidetext': ''\n'from': 'vector'\n'score': 0.8851634860038757\n'normalizedScore': 0.859296810143272\n'beforeDatePenaltyScore': 0.8851634860038757\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36cfef5cff79d6f5cd7c'\n'access': 'granted'\n'part_number': 20\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': uh, a cloud version of Elasticsearch, then you can just put the cloud version UL the UL of your cloud version. And here we can do Elasticsearch info to check that it can actually connect to, to the server. So what I'm doing right now is this part. Yeah. So now we can connect to Elasticsearch and we will create now an index in elastic search. So an index in elastic search is similar to a table in a usual relational database. So in a, let's say in a usual database, if we use Postgres or MySQL, we would have a table documents here we would have, we will create an index called documents or like, I don't remember how we'll call it, but we can do it. So there is uh, uh, this indices and we create can create an index and index name, I think I call it course questions, right? Index name would be course questions. And then we need to describe what exactly is there in this table. Like when we create a schema in the relational database, we say what type, each column is here, we need to do something similar here. We need to tell, like let me show. So we have four fields here, like four in relational database. It would be four columns, right? And for this think what we want to do for the course field is we want to filter on this. So it would be similar to uh, select where, uh, where course equals to data engineers on camp. I think it's select from, right, from blah, blah blah where course equals to data engineering course. So we want to be able to filter on this field. So for example, if a student wants to ask questions about this data engineer zoom camp, we don't want to show them answers from a different course, right? So we want to only stick ourselves to this particular course. So for that, in Elasticsearch we have a special type called keyword. Usually we want to have an exact match there, which is similar to doing this kind of query. And for these three fields, we want to have, um, we want to perform text search over these fields, which means that we want to see, like as I explained there when I was talking about the difference between text search and vector search, we want to, um, break this text into words and then do kind of word mark more match than when I have a question. Can I still join the course after the start date? I want to match the words, join course start date, these things, right? So for that I use the text, uh, uh, type. And then this is, uh,\n'slidetext': ''\n'from': 'vector'\n'score': 0.8851331472396851\n'normalizedScore': 0.8592119476601568\n'beforeDatePenaltyScore': 0.8851331472396851\n\n'documentId': 'ecef77510b81668321ea4048'\n'chunk_id': '687a3144ca798166d3ba4516'\n'access': 'granted'\n'part_number': 20\n'total_parts': 94\n'contentType': 'COURSE'\n'title': A Beginner-Friendly Workshop with Python and AI\n'parentName': RAG verstehen: Retrieval-Augmented Generation in der Praxis\n'language': 'EN'\n'date': '2024-08-15T07:38:35.000Z'\n'abstract': As a Fullstack subscriber of entwickler.de and devm.io you have exclusive access to this workshop held as part of MLcon Munich 2024 - The Event for Machine Learning Technologies & Innovations. This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. In this machine learning project we will focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks and club courses as our source of information. What We'll Do: - Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. - Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. - Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. - Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': 'b247419d6ef3db3aafbbb29f'\n'parentDescription': Generative AI ermöglicht es die bisher gekannte Sucherfahrung für Nutzer:innen durch die Implementierung von Retrieval-augmented Generation deutlich zu erleichtern und zu verbessern. Dieser Kurs gibt dir mit Hilfe spannender sowie lehrreicher Artikel, Live Events und Sessions eine Einführung zum Thema RAG und zeigt, wie auch du die Suchergebnisse in deinen Anwendungen optimieren kannst. Lerne außerdem, wie du deine bestehenden Anwendungen mit KI-Funktionalitäten ausstatten kannst, was du beachten musst, um deinen eigenen Chatbot zu erstellen und vieles mehr. Tauche ein in die Welt der Generative AI und Retrieval-augmented Generation, bleib mit uns auf dem neuesten Stand der KI-Entwicklung und biete deinen Nutzer:innen eine neue Form der User Experience. Worauf wartest du noch?\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': Elasticsearch client. I think I should do, uh, from Elasticsearch import. What do we have here? Elasticsearch and I call client Elasticsearch. And here hosts, uh, we have HTDP, local host 92 0 0. And um, of course if you, uh, use, uh, a cloud version of Elasticsearch, then you can just put the cloud version UL the UL of your cloud version. And here we can do Elasticsearch info to check that it can actually connect to, to the server. So what I'm doing right now is this part. Yeah. So now we can connect to Elasticsearch and we will create now an index in elastic search. So an index in elastic search is similar to a table in a usual relational database. So in a, let's say in a usual database, if we use Postgres or MySQL, we would have a table documents here we would have, we will create an index called documents or like, I don't remember how we'll call it, but we can do it. So there is uh, uh, this indices and we create can create an index and index name, I think I call it course questions, right? Index name would be course questions. And then we need to describe what exactly is there in this table. Like when we create a schema in the relational database, we say what type, each column is here, we need to do something similar here. We need to tell, like let me show. So we have four fields here, like four in relational database. It would be four columns, right? And for this think what we want to do for the course field is we want to filter on this. So it would be similar to uh, select where, uh, where course equals to data engineers on camp. I think it's select from, right, from blah, blah blah where course equals to data engineering course. So we want to be able to filter on this field. So for example, if a student wants to ask questions about this data engineer zoom camp, we don't want to show them answers from a different course, right? So we want to only stick ourselves to this particular course. So for that, in Elasticsearch we have a special type called keyword. Usually we want to have an exact match there, which is similar to doing this kind of query. And for these three fields, we want to have, um, we want to perform text search over these fields, which means that we want to see, like as I explained there when I was talking about the difference between text search and vector search, we want to, um, break this text into words and then do kind of word\n'slidetext': ''\n'from': 'vector'\n'score': 0.8834826946258545\n'normalizedScore': 0.8586991371061944\n'beforeDatePenaltyScore': 0.8834826946258545\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d1ef5cff79d6f5cd8b'\n'access': 'granted'\n'part_number': 35\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': back. How long break? Let's have uh, 10 minutes. Oh yeah. So 11, 18, 11, 12. Let's 1120. So we continue. And right now, so we, We finished this part. Now we'll do this part. So we'll now use OpenAI. I spoke with some of you, you still don't have, uh, 'cause you need to create, uh, an account in OpenAI and also put some money. I realized that it might be problematic for you to do this on the spot. Um, we'll in the second part of the workshop we'll replace this part with something else. But also if for you just sitting here and watching me doing with it, with open AI is boring. Could be, right? So there are some alternatives. I want to quickly, before we start, I want to talk about these alternatives. So then, uh, maybe like if for you it gets very boring, you can try to use something else while we do this workshop. But in the second part we'll run these things locally. It should be, okay. So I want to show you another, if you put in Google LLM Zoom camp or like, there is actually a link here at the beginning of this document. This LLM Zoom camp, this is the course I mentioned. And in this course we have a list of open AI alternatives. So these are services that you can use, uh, to send a prompt and get back the answer. So services, uh, LLM services and many of them have actually free credits. Some of them are totally free like grok. Um, some of them they give you free, free credits, um, like some of them free. But with rate limits, um, yeah, you can check. Like for example, uh, many students use grok and it's fine. So if you find yourself, um, if you think it's boring for you, 'cause I do, I use open AI and you cannot follow, you can maybe create an account in grok and use that. Right? So this is, uh, let me show you one more time how to find this link. So at the beginning of the workshop document, there is a link to LLM Zoom camp. You click on that, you go to the first module. And in the first module there is these open AI alternatives. So I see that there is a question. Yes. Uh, do you also go into details on uh, setup? Yeah, so the question is if I go to into details for local alarms, yes. In the second part of the workshop we will use all LAMA locally and then we'll use some other\n'slidetext': ''\n'from': 'vector'\n'score': 0.8846113681793213\n'normalizedScore': 0.8586901685997931\n'beforeDatePenaltyScore': 0.8846113681793213\n\n'documentId': '647ccd09adebbec3f1c2ad84'\n'chunk_id': '687a36d0ef5cff79d6f5cd7d'\n'access': 'granted'\n'part_number': 21\n'total_parts': 93\n'contentType': 'COURSE'\n'title': Workshop: Build a Chatbot with LLM\n'parentName': Get Ready, Get Set, AI + Chatbots\n'language': 'EN'\n'date': '2024-07-25T13:21:21.000Z'\n'abstract': This workshop is designed for people with no prior knowledge in generative AI, ML, or NLP. We'll focus on building a chatbot using Python and Large Language Models (LLMs), utilizing the FAQs from DataTalks.Club courses as our source of information.  What We'll Do: Extracting FAQs for the Knowledge Base: We'll begin by extracting all questions and answers from Google Docs to form our knowledge base. This step involves organizing the data in a format suitable for the chatbot. Indexing the Knowledge Base: After extraction, we'll index this information. Indexing allows the chatbot to efficiently search through the knowledge base to find relevant answers. Setting Up the Chatbot in a Jupyter Notebook: Our main activity will be setting up and programming the chatbot in a Jupyter notebook. This will include basic Python coding and understanding how to interface with LLMs. Integrating LLMs with RAG: We will integrate LLMs using Retrieval-Augmented Generation (RAG) to enable the chatbot to provide accurate answers. RAG will enhance the chatbot's ability to interpret questions and retrieve the correct information from the knowledge base.\n'parentId': '2d4578757c1771ea8a22dbaa'\n'parentDescription': Feeling overwhelmed by the options for building an AI application? This course cuts through the confusion and equips you with the skills to get started. Learn from Real-World Examples: We'll explore several successful AI applications to inspire your own ideas. Live coding sessions will walk you through the steps of building a chatbot or application, step-by-step. Building AI Applications from Scratch: A comprehensive written series guides you through the entire process, from initial considerations to ongoing maintenance. Master the fundamentals: Understand vector data, its implementation, search functionality, and the crucial role of vector databases. Crafting a Powerful Chatbot: Build a working chatbot proof-of-concept alongside us! We'll leverage OpenAI's API, PHP, and Pinecone to create a solution that empowers customers to easily find answers. Ready to build your first AI application or perhaps refine your latest chatbot? This course is your roadmap to success!\n'indexBrandName': 'MLCon'\n'indexSeriesName': ''\n'author': 'Alexey Grigorev'\n'chunkSource': 'text'\n'text': I have a question. Can I still join the course after the start date? I want to match the words, join course start date, these things, right? So for that I use the text, uh, uh, type. And then this is, uh, a bit too much to type. So we'll just copy it from here And put it here. So I just explained this that we have multiple types. So for text, for section for question, the type is text because we want to perform text search and then which means we want to break the text into distinct words. And then when we do, uh, when we perform search, we want to do keyword matches, right? And of course is we want to match the exact we want to filter to be able to filter on this. And yeah, so this is not important for us. This is important when you have um, a cluster, Elasticsearch cluster with multiple instances. So you want to say like how many, like how exactly it's distributed across multiple instances. So in our case, we have only one instance of Elasticsearch. So we just use these settings. And of course when you use it in production and you have let's say three instances, you want to read more about this setting before you create an index. And I think I named it as client, right? So now what happened is we created this index and this is the name of this index, and we can now index or go over all our documents and put them in this index. Um, so save them to our Elasticsearch in the database. So I'll do this for documents, uh, in documents, Then Elasticsearch client index. And then we need to specify the name of the index, which is these course questions. And I think we do, what do I have here? Document? Yeah, so this is this field. The second document equals to document. So we simply go over all the documents we have, and here I'll use a progress bar that we installed, TQDM, um, and then I'll just do something like this. So we then we can monitor the progress of, uh, indexing. And now our documents are indexed. I see a question in chat. Uh, can you please show again what you entered in Jupiter from the beginning? So I want, I will show you where you can find it. So everything I do, well of course you just can follow this, um, this document and everything I type in Jupiter is in this document, but also like if you go to notebooks, in notebooks, you have this elastic search i, this elastic search notebook and it contains everything a like this is\n'slidetext': ''\n'from': 'vector'\n'score': 0.8843786120414734\n'normalizedScore': 0.8584574124619452\n'beforeDatePenaltyScore': 0.8843786120414734\n\n\n Dieser Content von  ist in deiner Sprache nicht verfügbar. Zurück zur Antwort"
}